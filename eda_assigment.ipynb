{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *1.  What is a parameter?*\n",
    "\n",
    "A parameter in machine learning is a variable used by the model during training to learn from the data. Parameters are adjusted during the training process, allowing the model to fit the data and make accurate predictions. They are critical to the model’s performance and directly impact the output. \n",
    "\n",
    "Here are a few common examples of parameters in different machine learning algorithms:\n",
    "- Weights in neural networks: Determine the importance of inputs in predicting the output.\n",
    "- Coefficients in linear regression: Define the relationship between input features and the target variable.\n",
    "- Support vectors in SVMs (Support Vector Machines): Used to define the boundary that separates different classes.\n",
    "\n",
    "In contrast to parameters, hyperparameters are set before the training process begins and are not learned from the data. Examples include learning rate, number of hidden layers in a neural network, and the number of nearest neighbors in KNN (K-Nearest Neighbors).\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *2. What is correlation? What does negative correlation mean?*\n",
    "\n",
    "Correlation is a statistical measure that describes the extent to which two variables change together. In simpler terms, it tells us how much one variable is related to another. The value of correlation ranges from -1 to 1:\n",
    "- A correlation of 1 means a perfect positive correlation, where both variables increase together.\n",
    "- A correlation of 0 indicates no correlation, meaning there's no apparent relationship between the variables.\n",
    "- A correlation of -1 signifies a perfect negative correlation, where one variable increases as the other decreases.\n",
    "\n",
    "Negative correlation specifically means that as one variable goes up, the other tends to go down. For example, if we look at the relationship between temperature and the number of cups of hot chocolate sold, we might find a negative correlation: as the temperature decreases, the number of cups of hot chocolate sold increases.\n",
    "\n",
    "Here's a quick illustration:\n",
    "\n",
    "| Correlation Value | Relationship Type        | Example                              |\n",
    "|-------------------|--------------------------|--------------------------------------|\n",
    "| 1.0               | Perfect Positive         | Height vs. Shoe Size (generally)     |\n",
    "| 0.0               | No Correlation           | Shoe Size vs. Intelligence           |\n",
    "| -1.0              | Perfect Negative         | Temperature vs. Hot Chocolate Sales  |\n",
    "\n",
    "Understanding correlation is essential in many fields, like finance, biology, and social sciences, as it helps to identify and quantify relationships between different variables. Just remember, correlation does not imply causation—two variables can be correlated without one causing the other to change. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *3. Define Machine Learning. What are the main components in Machine Learning?*\n",
    "\n",
    "Machine Learning (ML) is a field of artificial intelligence (AI) that enables systems to learn and improve from experience without being explicitly programmed. Essentially, it involves using data to train models, which can then make predictions or decisions based on new data. \n",
    "\n",
    "Here are the main components in machine learning:\n",
    "\n",
    "1. Data: The foundation of any ML model. Data can be structured (like spreadsheets) or unstructured (like images, text, etc.). Good quality and quantity of data are crucial for effective learning.\n",
    "\n",
    "2. Features: Also known as variables or attributes, these are individual measurable properties or characteristics of the data. Feature selection and engineering are critical steps in building effective ML models.\n",
    "\n",
    "3. Model: A mathematical representation that learns from the data. There are various types of models such as linear regression, decision trees, neural networks, etc. The choice of model depends on the problem and data.\n",
    "\n",
    "4. Training: The process of feeding data into the model to enable it to learn. During training, the model adjusts its parameters to minimize errors and improve predictions.\n",
    "\n",
    "5. Evaluation: Assessing the performance of the model using metrics like accuracy, precision, recall, etc. Evaluation helps to understand how well the model generalizes to new, unseen data.\n",
    "\n",
    "6. Hyperparameters: Settings or configurations that are set before the training process begins. Examples include learning rate, number of epochs, and the number of layers in a neural network. Hyperparameter tuning is essential for optimizing model performance.\n",
    "\n",
    "7. Prediction: Once trained and evaluated, the model can be used to make predictions on new data. This is the ultimate goal of machine learning – to make informed decisions based on patterns and insights learned from historical data.\n",
    "\n",
    "8. Deployment: Integrating the model into a real-world environment where it can be used to make predictions on live data. Deployment involves ensuring the model operates efficiently and reliably in a production setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *4. How does loss value help in determining whether the model is good or not?*\n",
    "\n",
    "Loss value, often referred to as the \"cost\" or \"error,\" is a critical metric used to assess how well a machine learning model is performing during training. It quantifies the difference between the model’s predictions and the actual target values. Here's how it helps:\n",
    "\n",
    "1. Indicator of Model Performance: \n",
    "   - A low loss value indicates that the model’s predictions are close to the actual values, suggesting that the model is performing well.\n",
    "   - A high loss value indicates a large disparity between the predictions and actual values, suggesting that the model may not be performing well.\n",
    "\n",
    "2. Guiding the Training Process: \n",
    "   - During training, the goal is to minimize the loss value. This process, called optimization, involves adjusting the model’s parameters to reduce the error.\n",
    "   - Techniques like gradient descent use the loss value to determine the direction and magnitude of parameter adjustments.\n",
    "\n",
    "3. Overfitting and Underfitting:\n",
    "   - Overfitting: If the loss value on the training data is low, but high on validation or test data, the model might be overfitting, meaning it has learned the training data too well, including the noise, and may not generalize well to new data.\n",
    "   - Underfitting: If the loss value is high for both training and validation data, the model is underfitting, meaning it’s too simple to capture the underlying patterns in the data.\n",
    "\n",
    "4. Comparing Models:\n",
    "   - Loss value provides a quantitative measure to compare different models or different configurations of the same model. A model with a lower loss value on validation data is generally considered better.\n",
    "\n",
    "Here’s a simplified example:\n",
    "\n",
    "Imagine you’re training a model to predict house prices. After each round of training, you check the loss value:\n",
    "- Epoch 1: Loss = 1000 (high error, model is just starting to learn)\n",
    "- Epoch 10: Loss = 200 (error is decreasing, model is learning)\n",
    "- Epoch 50: Loss = 50 (low error, model predictions are getting closer to actual values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *5.What are continuous and categorical variables?*\n",
    " Continuous variables  and  categorical variables  are two types of data used in statistical analysis and machine learning:\n",
    "\n",
    "### Continuous Variables\n",
    "- Definition : These are variables that can take an infinite number of values within a given range. They are typically numerical and can be measured.\n",
    "-  Examples : Height, weight, temperature, time, age, and distance.\n",
    "-  Characteristics :\n",
    "  - They can be broken down into smaller increments (e.g., 1.5, 1.51, 1.511, etc.).\n",
    "  - Often represented in real numbers.\n",
    "  - Suitable for mathematical operations like addition, subtraction, multiplication, and division.\n",
    "\n",
    "### Categorical Variables\n",
    "-  Definition : These variables represent discrete categories or groups and are typically non-numeric. They can be divided into two subtypes: nominal and ordinal.\n",
    "  -  Nominal : Categories without a natural order (e.g., colors, types of animals, brands).\n",
    "  -  Ordinal : Categories with a natural order (e.g., education levels, rankings, satisfaction ratings).\n",
    "-  Examples : Gender, blood type, country of origin, and yes/no responses.\n",
    "-  Characteristics :\n",
    "  - They can't be broken down into smaller increments.\n",
    "  - Often represented as labels or names.\n",
    "  - Suitable for counting and frequency analysis but not for mathematical operations.\n",
    "\n",
    "Here's a quick comparison:\n",
    "\n",
    "| Aspect                 | Continuous Variables                      | Categorical Variables                      |\n",
    "|------------------------|------------------------------------------|-------------------------------------------|\n",
    "| *Nature*             | Numerical                                 | Non-numerical                             |\n",
    "| *Examples*           | Height, weight, temperature               | Gender, blood type, country of origin     |\n",
    "| *Subtypes*           | Not applicable                            | Nominal and ordinal                       |\n",
    "| *Mathematical Ops*   | Addition, subtraction, multiplication     | Counting, frequency analysis              |\n",
    "\n",
    "Both types of variables play crucial roles in data analysis and machine learning. Continuous variables are used for tasks that require precise measurements and calculations, while categorical variables help in grouping and classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *6. How do we handle categorical variables in Machine Learning? What are the common techniques?*\n",
    "\n",
    "Handling categorical variables effectively is crucial in machine learning to ensure the model can interpret and utilize the information they contain. Here are some common techniques used to handle categorical variables:\n",
    "\n",
    "### 1. Label Encoding\n",
    "This technique involves converting each category to a numerical label. For example, \"red,\" \"green,\" and \"blue\" could be converted to 0, 1, and 2. While simple, this method can sometimes imply an ordinal relationship between categories that doesn't exist.\n",
    "\n",
    "### 2. One-Hot Encoding\n",
    "In one-hot encoding, each category is transformed into a binary vector. For example, \"red,\" \"green,\" and \"blue\" would be represented as:\n",
    "- Red: [1, 0, 0]\n",
    "- Green: [0, 1, 0]\n",
    "- Blue: [0, 0, 1]\n",
    "\n",
    "This technique avoids ordinal relationships and works well with machine learning algorithms, but it can create a high-dimensional feature space with many categories.\n",
    "\n",
    "### 3. Ordinal Encoding\n",
    "This is used when the categorical variable has a natural order. Categories are mapped to integers that reflect their order. For example, \"low,\" \"medium,\" and \"high\" might be encoded as 1, 2, and 3, respectively.\n",
    "\n",
    "### 4. Frequency Encoding\n",
    "This method replaces each category with the frequency of its occurrence in the data. For example, if \"blue\" appears 20 times, \"red\" appears 30 times, and \"green\" appears 10 times, they would be replaced with 20, 30, and 10, respectively.\n",
    "\n",
    "### 5. Target Encoding (Mean Encoding)\n",
    "This technique involves replacing categories with the mean of the target variable for that category. For example, if predicting house prices, \"neighborhood\" categories could be replaced with the average house price in each neighborhood.\n",
    "\n",
    "### 6. Binary Encoding\n",
    "Binary encoding is a combination of label encoding and one-hot encoding. Categories are first label encoded, and then those integer values are converted to binary. Each binary digit becomes a new feature.\n",
    "\n",
    "### 7. Leave-One-Out Encoding\n",
    "A variant of target encoding, this method replaces a category value with the mean of the target variable, calculated by excluding the current row. This helps to reduce overfitting in the training data.\n",
    "\n",
    "### 8. Hashing Encoding\n",
    "This method uses a hash function to assign categories to different buckets. This can be useful when dealing with high-cardinality categorical variables where one-hot encoding would create too many features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *7. What do you mean by training and testing a dataset?*\n",
    " Training and testing datasets are essential concepts in machine learning that help in building and evaluating models effectively.\n",
    "\n",
    "### Training Dataset\n",
    "- Purpose: Used to train the machine learning model.\n",
    "- Process: The model learns from this data by identifying patterns and adjusting its parameters to minimize errors.\n",
    "- Size: Usually, the majority of the available data is allocated for training to ensure the model has enough examples to learn from.\n",
    "\n",
    "### Testing Dataset\n",
    "- Purpose: Used to evaluate the performance of the trained model.\n",
    "- Process: After the model is trained, it is tested on this separate dataset to assess how well it generalizes to new, unseen data. This helps in checking for issues like overfitting or underfitting.\n",
    "- Size: A smaller portion of the available data is set aside for testing to provide a fair evaluation without the model having seen these examples during training.\n",
    "\n",
    "### Why Split the Data?\n",
    "1. Generalization: Ensures the model performs well on new, unseen data rather than just memorizing the training data.\n",
    "2. Evaluation: Provides an unbiased assessment of the model’s performance.\n",
    "\n",
    "### Example Workflow:\n",
    "1. Data Collection: Gather all your data.\n",
    "2. Data Splitting: Split the data into training and testing datasets (commonly 70-80% for training and 20-30% for testing).\n",
    "3. Model Training: Train the model using the training dataset.\n",
    "4. Model Testing: Evaluate the model using the testing dataset.\n",
    "\n",
    "This split helps to create a robust model that can effectively handle real-world data.\n",
    "\n",
    "Is there a particular dataset or project you're working on? I'm happy to dive deeper!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *8.  What is sklearn.preprocessing?*\n",
    "\n",
    "`sklearn.preprocessing` is a module in the `scikit-learn` library that provides various utilities for preprocessing data. Preprocessing is a crucial step in the machine learning pipeline because it transforms raw data into a suitable format for modeling. This includes scaling, encoding, imputing missing values, and more.\n",
    "\n",
    "Here are some key functionalities provided by `sklearn.preprocessing`:\n",
    "\n",
    "### 1. Scaling Features\n",
    "- StandardScaler: Standardizes features by removing the mean and scaling to unit variance.\n",
    "- MinMaxScaler: Transforms features by scaling each feature to a given range, typically between zero and one.\n",
    "- MaxAbsScaler: Scales each feature by its maximum absolute value.\n",
    "\n",
    "### 2. Encoding Categorical Features\n",
    "- LabelEncoder: Encodes target labels with value between 0 and n_classes-1.\n",
    "- OneHotEncoder: Encodes categorical features as a one-hot numeric array.\n",
    "\n",
    "### 3. Imputing Missing Values\n",
    "- SimpleImputer: Imputes missing values using mean, median, most frequent, or constant value.\n",
    "- KNNImputer: Imputes missing values using k-nearest neighbors.\n",
    "\n",
    "### 4. Generating Polynomial Features\n",
    "- PolynomialFeatures: Generates a new feature matrix consisting of all polynomial combinations of the features with a specified degree.\n",
    "\n",
    "### 5. Normalizing Data\n",
    "- Normalizer: Normalizes samples individually to unit norm.\n",
    "\n",
    "### 6. Binarizing Data\n",
    "- Binarizer: Thresholds numerical features to binary values.\n",
    "\n",
    "Sklearn.preprocessing offers many tools to prepare data effectively, ensuring the best possible performance from your machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.18321596 -1.18321596]\n",
      " [-0.50709255 -0.50709255]\n",
      " [ 0.16903085  0.16903085]\n",
      " [ 1.52127766  1.52127766]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Sample data\n",
    "data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "print(scaled_data)\n",
    "# StandardScaler will standardize the input data by removing the mean and scaling to unit variance, making the data ready for machine learning algorithms that perform better with standardized input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *9.  What is a Test set?*\n",
    "A test set is a subset of your data that is used to evaluate the performance of a machine learning model after it has been trained. It serves as a final checkpoint to ensure that the model generalizes well to new, unseen data, which is crucial for its effectiveness in real-world applications.\n",
    "\n",
    "### Key Characteristics of a Test Set:\n",
    "- Unseen Data: The test set should consist of data that the model has never seen during the training process.\n",
    "- Evaluation: It is used exclusively for model evaluation and not for training or model selection.\n",
    "- Generalization: Helps in assessing how well the model performs on new data, indicating its ability to generalize from the training data to real-world scenarios.\n",
    "- Performance Metrics: Common metrics evaluated using the test set include accuracy, precision, recall, F1-score, and others, depending on the specific problem and the chosen model.\n",
    "\n",
    "### Example Workflow:\n",
    "1. Data Splitting: The complete dataset is divided into a training set and a test set. Typically, 70-80% of the data is used for training, and 20-30% is reserved for testing.\n",
    "2. Training: The model is trained on the training set.\n",
    "3. Validation (optional): Sometimes, a validation set is used to tune hyperparameters and make decisions about the model architecture.\n",
    "4. Testing: Once the model is finalized, its performance is evaluated on the test set to see how well it performs on data it hasn’t encountered before.\n",
    "\n",
    "By ensuring the test set is used only for final evaluation, you can get an unbiased estimate of the model’s performance and make informed decisions about its deployment in real-world scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number Of Data Points : \n",
      "X Train : 75 \n",
      "y Train : 75 \n",
      "X Test : 25\n",
      "y test : 25\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Sample data\n",
    "X = [i for i in range(1,101)]  # Features\n",
    "y = [i for i in range (101,201)]  # Target\n",
    "\n",
    "# Splitting the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "print(f\"Number Of Data Points : \\nX Train : {len(X_train)} \\ny Train : {len(y_train)} \\nX Test : {len(X_test)}\\ny test : {len(y_test)}\")\n",
    "# X_train and y_train are used for training the model\n",
    "# X_test and y_test are used for testing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. How do we split data for model fitting (training and testing) in Python?How do you approach a Machine Learning problem?\n",
    "#### Approaching a Machine Learning Problem\n",
    "\n",
    "1. Define the Problem:\n",
    "   - Clearly understand the problem you’re trying to solve. Define the objective, whether it’s classification, regression, clustering, etc.\n",
    "\n",
    "2.  Gather Data:\n",
    "   - Collect relevant data. Ensure it is representative of the problem you’re trying to solve.\n",
    "\n",
    "3. Exploratory Data Analysis (EDA):\n",
    "   - Analyze the data to understand its structure, detect patterns, and identify potential issues such as missing values or outliers.\n",
    "   - Use visualizations (e.g., histograms, scatter plots) to gain insights.\n",
    "\n",
    "4. Preprocess Data:\n",
    "   - Clean and preprocess the data. Handle missing values, encode categorical variables, and scale numerical features if necessary.\n",
    "\n",
    "5. Feature Engineering:\n",
    "   - Create new features from existing data if it improves the model's performance.\n",
    "   - Select the most relevant features.\n",
    "\n",
    "6. Split Data:\n",
    "   - Split the data into training, validation (optional), and test sets. Ensure the splits are representative of the overall dataset.\n",
    "\n",
    "7. Choose a Model:\n",
    "   - Select a suitable machine learning algorithm based on the problem and data characteristics.\n",
    "\n",
    "8. Train the Model:\n",
    "   - Train the model using the training dataset. Optimize hyperparameters using cross-validation if needed.\n",
    "\n",
    "9. Evaluate the Model:\n",
    "   - Assess the model’s performance using the test set. Use appropriate metrics (e.g., accuracy, precision, recall, RMSE) to evaluate the model.\n",
    "\n",
    "10. Refine the Model:\n",
    "    - If the model’s performance is not satisfactory, refine it by adjusting features, tuning hyperparameters, or trying different algorithms.\n",
    "\n",
    "11. Deploy the Model:\n",
    "    - Once satisfied with the model’s performance, deploy it to make predictions on new data. Ensure it operates efficiently in a production environment.\n",
    "\n",
    "12. Monitor and Maintain the Model:\n",
    "    - Continuously monitor the model’s performance in the real world. Update the model as necessary to maintain its effectiveness.\n",
    "\n",
    "This structured approach helps ensure that each step is methodically addressed, leading to more robust and reliable machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Splitting Data for Training and Testing\n",
    "\n",
    "# Install scikit-learn:  \n",
    "#  pip install scikit-learn\n",
    "\n",
    "\n",
    "# Import Necessary Libraries:\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "   \n",
    "\n",
    "# Prepare Your Data:\n",
    "# Example data\n",
    "X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])\n",
    "y = np.array([0, 1, 0, 1, 0])\n",
    "   \n",
    "# Split the Data:   \n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# X_train, y_train will be used for training\n",
    "# X_test, y_test will be used for testing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Why do we have to perform EDA before fitting a model to the data?*\n",
    "Performing Exploratory Data Analysis (EDA) before fitting a model is essential for several reasons:\n",
    "\n",
    "1. Understanding the Data:\n",
    "   - EDA helps you get a good grasp of the data's structure, distribution, and inherent patterns. This understanding is crucial for selecting appropriate models and preprocessing techniques.\n",
    "\n",
    "2. Detecting Anomalies and Outliers:\n",
    "   - By visualizing data, you can identify anomalies and outliers that could skew the model's performance if not addressed properly.\n",
    "\n",
    "3. Handling Missing Values:\n",
    "   - EDA helps to identify missing values and decide on strategies to handle them, such as imputation or removal.\n",
    "\n",
    "4. Feature Selection and Engineering:\n",
    "   - Understanding the relationships between features can guide you in selecting the most relevant ones or creating new features to improve model performance.\n",
    "\n",
    "5. Detecting Data Leakage:\n",
    "   - EDA can reveal any instances of data leakage where information from outside the training dataset is used to create the model, leading to overfitting.\n",
    "\n",
    "6. Checking Assumptions:\n",
    "   - Many machine learning algorithms have underlying assumptions (e.g., normality, homoscedasticity). EDA allows you to check these assumptions and transform data if necessary.\n",
    "\n",
    "7. Informing Model Choice:\n",
    "   - Insights gained from EDA can inform the choice of models. For example, if relationships between features and target variables are non-linear, you might opt for non-linear models.\n",
    "\n",
    "8. Improving Data Quality:\n",
    "   - EDA helps identify and correct data quality issues, such as incorrect data entries, inconsistent formats, or duplicate records.\n",
    "\n",
    "### Example EDA Steps\n",
    "1. Summarize Data:\n",
    "   - Use summary statistics (mean, median, mode, standard deviation) to understand the central tendencies and dispersion.\n",
    "   \n",
    "2. Visualize Data:\n",
    "   - Create visualizations like histograms, scatter plots, box plots, and heatmaps to uncover patterns and relationships.\n",
    "\n",
    "3. Check Correlations:\n",
    "   - Use correlation matrices to understand relationships between features.\n",
    "\n",
    "4. Identify Missing Values:\n",
    "   - Detect and visualize missing data to decide on appropriate handling methods.\n",
    "\n",
    "5. Analyze Distributions:\n",
    "   - Plot distributions of continuous variables to identify skewness, kurtosis, and potential transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *12. What is correlation?*\n",
    "Correlation is a statistical measure that describes the extent to which two variables change together. In simpler terms, it tells us how much one variable is related to another. The value of correlation ranges from -1 to 1:\n",
    "- A correlation of 1  means a perfect positive correlation, where both variables increase together.\n",
    "- A correlation of 0  indicates no correlation, meaning there's no apparent relationship between the variables.\n",
    "- A correlation of -1  signifies a perfect negative correlation, where one variable increases as the other decreases.\n",
    "\n",
    "Understanding correlation is essential in many fields, like finance, biology, and social sciences, as it helps to identify and quantify relationships between different variables. Just remember, correlation does not imply causation—two variables can be correlated without one causing the other to change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. What does negative correlation mean?*\n",
    "Negative correlation specifically means that as one variable goes up, the other tends to go down. For example, if we look at the relationship between temperature and the number of cups of hot chocolate sold, we might find a negative correlation: as the temperature decreases, the number of cups of hot chocolate sold increases.\n",
    "\n",
    "Here's a quick illustration:\n",
    "\n",
    "| Correlation Value | Relationship Type        | Example                              |\n",
    "|-------------------|--------------------------|--------------------------------------|\n",
    "| 1.0               | Perfect Positive         | Height vs. Shoe Size (generally)     |\n",
    "| 0.0               | No Correlation           | Shoe Size vs. Intelligence           |\n",
    "| -1.0              | Perfect Negative         | Temperature vs. Hot Chocolate Sales  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *14. How can you find correlation between variables in Python?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             temperature     sales\n",
      "temperature     1.000000 -0.997054\n",
      "sales          -0.997054  1.000000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdAAAAFpCAYAAAAsmHm9AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAd1ElEQVR4nO3de/RldV3/8edrxkEFNUSEhotKronEEryE11IUDEgaMU34GbEwmzSpn10sXJWXWistNMsyxoEQvLIsBxl1FBH9ef2RjIgDqAgBxjCTJMRF8ReM3/fvj7MHz3z5Xs53n32+3zmc52Otvc6+7/eBA+/v+7M/+7NTVUiSpIVZttQBSJI0jkygkiS1YAKVJKkFE6gkSS2YQCVJasEEKklSCyZQSdJYSHJ2kpuTXDnL9iR5R5Jrk2xO8qS+bUcnubrZdloX8ZhAJUnj4hzg6Dm2HwOsaqY1wBkASZYD72y2HwKcmOSQYYMxgUqSxkJVfR64dY5dVgPvqZ5LgD2TrAQOB66tquuq6m7gvGbfoZhAJUn3F/sDN/Ytb2nWzbZ+KA8Y9gTz+fiKgx0rUPcLbz563VKHIA3tix99dkZ17mH/f/+C7d/+bXpNrzusq6qF/Ic303erOdYPZeQJVJI0GbJiuNxc99Q6YJi/VLcAB/YtHwBsBXabZf1QTKCSpE4se8DIittBbQBOTXIe8FTg9qraluS/gFVJDgJuAk4A/tewFzOBSpI6kRWj7VaT5IPAc4C9k2wB3gCsAKiqtcBG4FjgWuAu4JRm2/YkpwIXAsuBs6vqqmHjMYFKksZCVZ04z/YCXj3Lto30EmxnTKCSpE7sAk24i8oEKknqxLCdiMaNCVSS1IlJq0AdSEGSpBasQCVJnbAJV5KkFiatCdcEKknqRJabQCVJWrBlE5ZA7UQkSVILVqCSpE5k2WRVoCZQSVInsnyyGjVNoJKkTkzaPVATqCSpE5PWhDtZ9bYkSR2xApUkdcImXEmSWnAgBUmSWsiyyborOFnfVpKkjliBSpI6MWm9cE2gkqRO2IlIkqQWrEAlSWrBTkSSJGleVqCSpE7YhCtJUgt2IpIkqQUrUEmSWrATkSRJmpcVqCSpEzbhSpLUgglUkqQWJi2Beg9UkqQWrEAlSZ1YjF64SY4G/h5YDpxVVW+Ztv21wMuaxQcAjwMeWVW3JrkBuBP4EbC9qp4yTCwmUElSJ0Y9kEKS5cA7gaOALcClSTZU1Td27FNVpwOnN/sfB/x+Vd3ad5ojqup7XcRjApUkdWIR7oEeDlxbVdcBJDkPWA18Y5b9TwQ+OKpgvAcqSepEli0bbkrWJNnUN62Zdon9gRv7lrc06+4bS7I7cDTw4b7VBXwqyVdnOPeCWYFKknYJVbUOWDfHLjOVuDXLvscBX5rWfPvMqtqaZB/goiTfqqrPtwzXClSS1I0sy1DTALYAB/YtHwBsnWXfE5jWfFtVW5vPm4Hz6TUJt2YClSR1YhES6KXAqiQHJdmNXpLccJ84kp8Ang1c0LdujyQP3TEPPB+4cpjvaxOuJKkTo36Mpaq2JzkVuJDeYyxnV9VVSV7ZbF/b7Ho88Kmq+kHf4fsC5yeBXu77QFV9cph4TKCSpLFRVRuBjdPWrZ22fA5wzrR11wGHdhmLCVSS1IlJG8rPBCpJ6sSkvQ/UBCpJ6kasQCVJWrBJa8KdrHpbkqSOWIFKkjrhPVBJklqwCXeaJD+d5OIkVzbLT0jyZ6MPTZI0ToYdTH7cDBLxmcDrgHsAqmozveGTJEm61yIM5bdLGSSB7l5VX5m2bvsogpEkaVwMcg/0e0keS/PKmCQvBraNNCpJ0tgZxypyGIMk0FfTez/bzyS5CbgeeNlIo5IkjZ8xvI85jDkTaJLlwKuq6sjm9S/LqurOxQlNkjRO4khEP1ZVP0ry5Gb+B3PtK0nSJBmkCfdrSTYA/wLcm0Srav3IopIkjZ1xfBRlGIMk0L2AW4Dn9q0rwAQqSbqXnYimqapTFiMQSdKYswLdWZJ30zzC0q+qXj6SiCRJY8kK9L4+1jf/IOB4YOtowpEkaTwM0oT74f7lJB8EPj2yiCRJYymxCXc+q4BHdR2IJGnM2YS7syR3svM90P8E/mRkEUmSxpKPsUxTVQ9djEAkSeNt0joRDfI+0IsHWSdJ0iSZtQJN8iBgd2DvJA8Hdvxp8TBgv0WITZI0TuxEdK/fBl5DL1l+lR8n0DuAd442LEnSuJm0JtxZE2hV/T3w90l+t6r+YRFjkiSNIzsR7ayq/iHJzwKH0BtIYcf694wyMEmSdmWDPMbyBuA59BLoRuAY4IuACVSSdK9Jex/oIPX2i4HnAf/ZDCx/KPDAkUYlSRo/y5YNN42ZQUYi+mFVTSXZnuRhwM3AT404LknSmJm0TkSDpPxNSfYEzqTXG/cy4CujDEqDecKZf8WRN32ZX/zaR5c6FGlojzrgwaw9/Yl8Zv0vcOLxByx1OGojy4abxsycEafXoP3mqrqtqtYCRwEn+47QXcOWc9fzlRe8YqnDkDpxx53b+bt113Le+TcudSjahSU5OsnVSa5NctoM25+T5PYklzfT6wc9dqHmbMKtqkryEeDJzfINw15Q3bn1i5t48KP3X+owpE7cdvs93Hb7PTzjKXstdShqa8RNuEmW0xuH4ChgC3Bpkg1V9Y1pu36hql7Q8tiBDVIzX5Lk59teQJI0GZJlQ00DOBy4tqquq6q7gfOA1QOGN8yxMxok4iPoJdF/T7I5yRVJNs91QJI1STYl2fTJqduGiU+SNC6WZaipP3c005ppV9gf6G/j39Ksm+7pSb6e5BNJHr/AYwc2SC/cYxZ60qpaB6wD+PiKg2ue3SVNqBcdux/H/dJKAP7oTVdwy613L3FEGsawrzPrzx2zXWKmw6YtXwY8uqq+n+RY4CP03mM9yLELMshIRN9J8ixgVVW9O8kjgYcMc1FJAli/cSvrN25d6jA0PrYAB/YtHwDs9AOqqjv65jcm+ackew9y7EIN8jqzN9B7gfbrmlUrgPcNc1F147D3vo1nfOE89jj4IJ57/ec48JQXL3VIUmt77bmC9e9+Gi994QH8xq89mvXvfhq7P3j5UoelhUiGm+Z3KbAqyUFJdgNOADbsHEJ+snmChCSH08tztwxy7EIN0oR7PPBEemUxVbU1iS/Z3gVcftIfLnUIUmduve0eXnTKJUsdhoYx4tGEqmp7klOBC4HlwNlVdVWSVzbb19IbPe9VSbYDPwROqKoCZjx2mHgGSaB3N4+zFECSPYa5oCTpfmoRxsKtqo30xmXvX7e2b/4fgX8c9NhhDPLnwoeSvAvYM8lvAZ+mNyqRJEkTa5BORG9NchS9F2n/NPD6qrpo5JFJksbKsL1wx80gTbgAVwAPptfl94rRhSNJGltjOJ7tMAbphfsKeoPHv4jezdlLkrx81IFJksbMkAMpjJtBKtDXAk+sqlsAkjwC+DJw9igDkySNlwGH47vfGOTbbgHu7Fu+k52HQ5IkaeIMUoHeBPxbkgvo3QNdDXwlyR8AVNXfjjA+SdK4GMNm2GEMkkD/vZl2uKD5dDAFSdKPTVgT7iCPsbxpMQKRJI25RRhIYVcybwJN8hTgT4FH9+9fVU8YYVySpHHjc6D38X56PXGvAKZGG44kSeNhkAT6X1U11Ij1kqQJ4D3Q+3hDkrOAi4H/2bGyqtaPLCpJ0vixF+59nAL8DL33gO5owi3ABCpJ+jEr0Ps4tKp+buSRSJI0Rgb5c+GSJIeMPBJJ0nhLhpvGzCAV6LOAk5NcT+8eaIDyMRZJ0k58jOU+jh55FJKk8TeGVeQw5v1zoaq+AxwIPLeZv2uQ4yRJEybLhpvGzCDvA30D8CfA65pVK4D3jTIoSZJ2dYM04R4PPBG4DKCqtiZxIHlJ0s68B3ofd1dVJSmAJHuMOCZJ0jiasHuggyTQDyV5F7Bnkt8CXg6cOdqwJEljZwzvYw5jkAT6SOBfgTuAg4HXA0eOMihJ0hiyAr2Po6rqT4CLdqxI8jZ6HYskSZpIsybQJK8Cfgf4qSSb+zY9FPjSqAOTJI0ZOxHd6wPAJ4A3A6f1rb+zqm4daVSSpLFTNuH2VNXtwO3AiYsXjiRpbE1YJ6LJ+raSJHVkkE5EkiTNb8IqUBOoJKkT3gOVJKmNCatAJ+vbSpJGZxFeqJ3k6CRXJ7k2yWkzbH9Zks3N9OUkh/ZtuyHJFUkuT7Jp2K9rBSpJGgtJlgPvBI4CtgCXJtlQVd/o2+164NlV9d9JjgHWAU/t235EVX2vi3hMoJKkbox+IIXDgWur6jqAJOcBq4F7E2hVfblv/0uAA0YVjE24kqROVDLUlGRNkk1905ppl9gfuLFveUuzbja/SW9AoHtDBD6V5KsznHvBrEAlSd0YshNRVa2j1+Q66xVmOmzGHZMj6CXQZ/WtfmbzTut9gIuSfKuqPt82XhOoJKkTNfpeuFuAA/uWDwC2Tt8pyROAs4BjquqWe+Or2tp83pzkfHpNwq0TqE24kqRxcSmwKslBSXYDTgA29O+Q5FHAeuCkqvp23/o9kjx0xzzwfODKYYKxApUkdWPEAylU1fYkpwIXAsuBs6vqqiSvbLavpffO6kcA/5RePNur6inAvsD5zboHAB+oqk8OE48JVJLUiUVowqWqNgIbp61b2zf/CuAVMxx3HXDo9PXDMIFKkroxYUP5eQ9UkqQWrEAlSd2YsLFwTaCSpE74NhZJktqwApUkaeFqxoGC7r8m688FSZI6YgUqSerEYjwHuisxgUqSumEClSRp4SatF+5k/bkgSVJHrEAlSZ3wHqgkSW1MWBOuCVSS1AkrUEmSWnAgBUmSNC8rUElSJ2zClSSpDTsRSZK0cDVhdwVNoJKkTjgSkSRJmpcVqCSpE3YikiSphUl7DtQEKknqxKRVoJP1bSVJ6ogVqCSpE5PWC9cEKknqhPdAJUlqYdLugZpAJUmdmLQKdLL+XJAkqSNWoJKkTtiEK0lSCzbhSpLUQmXZUNMgkhyd5Ook1yY5bYbtSfKOZvvmJE8a9NiFMoFKkjpRZKhpPkmWA+8EjgEOAU5Mcsi03Y4BVjXTGuCMBRy7ICZQSdK4OBy4tqquq6q7gfOA1dP2WQ28p3ouAfZMsnLAYxdk5PdA33z0ulFfQloUr/vkmqUOQerA1SM787AjESVZQ69q3GFdVfUnkf2BG/uWtwBPnXaamfbZf8BjF8RORJKkTlQNl0CbZDlX1TXTBWrAfQY5dkFMoJKkTtTo7wpuAQ7sWz4A2DrgPrsNcOyCeA9UkjQuLgVWJTkoyW7ACcCGaftsAH6j6Y37NOD2qto24LELYgUqSerEqJ8DrartSU4FLgSWA2dX1VVJXtlsXwtsBI4FrgXuAk6Z69hh4jGBSpI6sRgDKVTVRnpJsn/d2r75Al496LHDMIFKkjoxaSMRmUAlSZ2YtARqJyJJklqwApUkdWLY50DHjQlUktSJSWvCNYFKkjphApUkqYVJS6B2IpIkqQUrUElSJ+xEJElSC1MT1oRrApUkdcJ7oJIkaV5WoJKkTngPVJKkFiatCdcEKknqhBWoJEktTFoFaiciSZJasAKVJHXCJlxJklqYWuoAFpkJVJLUCStQSZJasBORJEmalxWoJKkTNuFKktTCpDXhmkAlSZ2YqqWOYHF5D1SSpBasQCVJnbAJV5KkFuxEJElSCzVh90BNoJKkTkxNWBOunYgkSWrBBCpJ6kRVhpqGkWSvJBcluab5fPgM+xyY5LNJvpnkqiT/u2/bG5PclOTyZjp2vmuaQCVJnagabhrSacDFVbUKuLhZnm478IdV9TjgacCrkxzSt/3tVXVYM22c74ImUElSJ4oMNQ1pNXBuM38u8ML7xFe1raoua+bvBL4J7N/2giZQSVInpmq4aUj7VtU26CVKYJ+5dk7yGOCJwL/1rT41yeYkZ8/UBDydCVSStEtIsibJpr5pzbTtn05y5QzT6gVe5yHAh4HXVNUdzeozgMcChwHbgLfNdx4fY5EkdWLYjkBVtQ5YN8f2I2fbluS7SVZW1bYkK4GbZ9lvBb3k+f6qWt937u/27XMm8LH54rUClSR1Yok7EW0ATm7mTwYumL5DkgD/DHyzqv522raVfYvHA1fOd0ETqCSpE1NkqGlIbwGOSnINcFSzTJL9kuzoUftM4CTguTM8rvI3Sa5Ishk4Avj9+S5oE64kaexV1S3A82ZYvxU4tpn/IsycqavqpIVe0wQqSeqEY+FKktSCb2ORJKmFDp7lHCsmUElSJyatCddeuJIktWAFKknqRAfj2Y4VE6gkqRPeA5UkqQXvgUqSpHlZgUqSOjFpFagJVJLUiSkHUpAkaeGsQCVJamHSEqidiCRJasEKVJLUCZ8DlSSpBd/GIklSC5N2D9QEKknqxKQ14dqJSJKkFqxAJUmdsAlXkqQWTKCSJLXgPVBJkjQvK1BJUidswpUkqYWpqaWOYHEtOIEmWQY8pKruGEE8kqQxNWkV6ED3QJN8IMnDkuwBfAO4OslrRxuaJGmcVA03jZtBOxEd0lScLwQ2Ao8CThpVUJIk7eoGbcJdkWQFvQT6j1V1T5Ix/HtBkjQqk/YYy6AJ9F3ADcDXgc8neTTgPVBJ0r1q6HbY8Xqby0AJtKreAbyjb9V3khwxmpAkSeNoHO9jDmPQTkT7JvnnJJ9olg8BTh5pZBrIow54MGtPfyKfWf8LnHj8AUsdjtTKE878K4686cv84tc+utShaAhTU8NN42bQTkTnABcC+zXL3wZeM4J4tEB33Lmdv1t3Leedf+NShyK1tuXc9XzlBa9Y6jA0xpLsleSiJNc0nw+fZb8bklyR5PIkmxZ6fL9BE+jeVfUhYAqgqrYDPxrwWI3Qbbffw7euuZPt2yes7UT3K7d+cRP33Hr7UoehIS3xYyynARdX1Srg4mZ5NkdU1WFV9ZSWxwODJ9AfJHkEUABJngb4a5ck3WuqhpuGtBo4t5k/l95TIyM9ftAE+gfABuCxSb4EvAf43dl2TrImyaYkm/7zO97TkKRJsMQV6L5Vta0XR20D9pktTOBTSb6aZE2L4+81aC/cy5I8GziYXj/jq6vqnjn2XwesA3jWcZ+zbbFjLzp2P477pZUA/NGbruCWW+9e4ogkaXhNQutPauuafLJj+6eBn5zh0D9dwGWeWVVbk+wDXJTkW1X1+TbxzplAk7xolk0/nYSqWt/mohrO+o1bWb9x61KHIUk7qSHbYfuLr1m2HznbtiTfTbKyqrYlWQncPMs5tjafNyc5Hzgc+Dww0PH95qtAj5tjWwEm0CW2154rOOvtT2aP3ZczNQUv+ZUD+PXfuZS7fmgfL42Pw977Nh7x7MPZbe+H89zrP8c1f/EP3Pjuf13qsLRASzwS0QZ6j1e+pfm8YPoOzXjuy6rqzmb++cBfDHr8dHMm0Ko6ZSHRa/Hdets9vOiUS5Y6DGkol5/0h0sdgjqwxAMpvAX4UJLfBP4DeAlAkv2As6rqWGBf4Pwk0Mt/H6iqT851/FwGfp1Zkl8GHg88aMe6qvqL2Y+QJE2SqSUsQavqFuB5M6zfChzbzF8HHLqQ4+cy6EhEa4GX0ut5G3qZ+dELuZAkSfcngz7G8oyq+g3gv6vqTcDTgQNHF5YkadxM2vtAB23C/X/N511Ne/KtwEGjCUmSNI7GMQkOY9AE+tEkewKnA5fR64F75qiCkiSNn6kJy6CDJtBvAT+qqg83b2J5EvCRkUUlSRo7NYZvVBnGoPdA/7x5buZZwFH03s5yxsiikiRpFzdoAt3xVP4vA2ur6gJgt9GEJEkaR1U11DRuBm3CvSnJu4Ajgb9O8kAGT76SpAkwji/FHsagSfDX6L1Q++iqug3YC3jtqIKSJI0fK9AZVNVd9I1727zqZduogpIkaVc38FB+kiTNZYkHk190JlBJUieGfZ3ZuDGBSpI6MYa3MYdiApUkdWIp38ayFHwURZKkFqxAJUmdGMdHUYZhApUkdWLSxsI1gUqSOuHbWCRJamHSmnDtRCRJUgtWoJKkTkzaYywmUElSJyasBdcEKknqxqQN5ec9UEmSWrAClSR1wsdYJElqYdKacE2gkqROmEAlSWphwvKnnYgkSWrDClSS1AmbcCVJamHSxsI1gUqSOjFpQ/l5D1SS1ImqGmoaRpK9klyU5Jrm8+Ez7HNwksv7pjuSvKbZ9sYkN/VtO3a+a5pAJUn3B6cBF1fVKuDiZnknVXV1VR1WVYcBTwbuAs7v2+XtO7ZX1cb5LmgClSR1oqZqqGlIq4Fzm/lzgRfOs//zgH+vqu+0vaAJVJLUiWETaJI1STb1TWsWcPl9q2obQPO5zzz7nwB8cNq6U5NsTnL2TE3A09mJSJLUiWHHwq2qdcC62bYn+TTwkzNs+tOFXCfJbsCvAK/rW30G8JdANZ9vA14+13lMoJKksVBVR862Lcl3k6ysqm1JVgI3z3GqY4DLquq7fee+dz7JmcDH5ovHJlxJUieW+B7oBuDkZv5k4II59j2Rac23TdLd4XjgyvkuaAUqSerEEg+k8BbgQ0l+E/gP4CUASfYDzqqqY5vl3YGjgN+edvzfJDmMXhPuDTNsvw8TqCSpE0s5kEJV3UKvZ+309VuBY/uW7wIeMcN+Jy30miZQSVInJm0sXO+BSpLUghWoJKkTDiYvSVILNTW11CEsKhOoJKkTk/Y2FhOoJKkTk9aEayciSZJasAKVJHVi0h5jMYFKkjphApUkqYWpmqxeuN4DlSSpBStQSVInbMKVJKkFE6gkSS1M2nOgJlBJUiemJmwoPzsRSZLUghWoJKkT3gOVJKmFmrDnQE2gkqROTFoF6j1QSZJasAKVJHVi0ipQE6gkqROTNhauCVSS1AkrUEmSWigHUpAkSfOxApUkdcImXEmSWnAgBUmSWpiyApUkaeHsRCRJkuZlBSpJ6oSdiCRJasFORJIktTBpFaj3QCVJasEKVJLUiUnrhZuqySq574+SrKmqdUsdhzQsf8saJzbh3j+sWeoApI74W9bYMIFKktSCCVSSpBZMoPcP3jPS/YW/ZY0NOxFJktSCFagkSS2YQDuSZM8kv7PUccwnyWuS7L7UcUjTJTknyYuXOg5pUCbQ7uwJLHkCTc9c/15fAywogSZxwA1JmsYE2p23AI9NcnmS05O8NsmlSTYneRNAksck+VaSs5JcmeT9SY5M8qUk1yQ5vNnvjUnem+Qzzfrf2nGROc77zST/BFwGHJjkjCSbklzVt9/vAfsBn03y2Wbd9/vO/eIk5zTz5yT522a/v07y2CSfTPLVJF9I8jOL8M9UYy7JHkk+nuTrzW/+pUle3/yGr0yyLklmOO7JST7X/N4uTLKyWf97Sb7R/P7PW/xvJPWpKqcOJuAxwJXN/PPp9SYMvT9SPgb8YrPPduDnmvVfBc5u9lsNfKQ5/o3A14EHA3sDN9JLfHOddwp4Wl88ezWfy4H/AzyhWb4B2Ltvv+/3zb8YOKeZP6c5//Jm+WJgVTP/VOAzS/3P3GnXn4BfBc7sW/6JHb/NZvm9wHHN/DnNb3AF8GXgkc36lwJnN/NbgQc283su9fdzmuzJprnReH4zfa1ZfgiwCvgP4PqqugIgyVXAxVVVSa6glwh3uKCqfgj8sKkCDweeNcd5v1NVl/Qd/2tJ1tAb73glcAiweYHf41+q6kdJHgI8A/iXvmLhgQs8lybTFcBbk/w18LGq+kKSX03yx/RuJewFXAV8tO+Yg4GfBS5qfm/LgW3Nts3A+5N8BPjIonwDaRYm0NEI8OaqetdOK5PHAP/Tt2qqb3mKnf99TH++qOY57w/6lg8C/gj4+ar676ZZ9kGzxNp/nen77DjnMuC2qjpslnNIM6qqbyd5MnAs8OYknwJeDTylqm5M8kbu+7sLcFVVPX2GU/4yvVaXXwH+PMnjq2r76L6BNDvvgXbnTuChzfyFwMubyo0k+yfZZ4HnW53kQUkeATwHuHQB530YveR3e5J9gWNmiRPgu0ke13Q8On6mQKrqDuD6JC9prpskhy7w+2gCJdkPuKuq3ge8FXhSs+l7ze94pl63VwOPTPL05hwrkjy++Y0eWFWfBf6YXse9h4z6O0izsQLtSFXd0nQGuhL4BPAB4P82TVDfB34d+NECTvkV4OPAo4C/rKqtwNYkj5vvvFX19SRfo9c0dh3wpb7N64BPJNlWVUcAp9G713kjcCWz/w/pZcAZSf6M3j2q8+jdp5Xm8nPA6UmmgHuAVwEvpNe0ewO9Pwx3UlV3N4+zvCPJT9D7/9TfAd8G3tesC/D2qrpt9F9BmpkjEe2Cmmat71fVW5c6FknSzGzClSSpBStQSZJasAKVJKkFE6gkSS2YQCVJasEEKklSCyZQSZJaMIFKktTC/wejuovv0yKKYQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 576x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# To find the correlation between variables in Python, you can use several libraries, but one of the most commonly used ones is `pandas`. Here's how you can do it:\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example data\n",
    "data = {\n",
    "    'temperature': [30, 35, 40, 45, 50],\n",
    "    'sales': [200, 180, 150, 120, 100]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "print(correlation_matrix)\n",
    "\n",
    "#  You can visualize the correlation matrix using a heatmap for better insights.\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "plt.show()\n",
    "\n",
    "# This code will generate a heatmap showing the correlation coefficients between variables, with colors representing the strength and direction of the correlations.a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *15. What is causation? Explain difference between correlation and causation with an example.*\n",
    "Causation refers to a relationship where one event or variable directly influences another. In other words, causation implies that changes in one variable cause changes in another variable. This is different from correlation, which only indicates a relationship or association between two variables without implying cause and effect.\n",
    "\n",
    "#### Correlation:\n",
    "- Definition: Measures the strength and direction of the relationship between two variables.\n",
    "- Implication: Indicates that two variables are related but does not prove that one variable causes the other to change.\n",
    "- Example: There is a correlation between ice cream sales and drowning incidents. As ice cream sales increase, so do drowning incidents.\n",
    "\n",
    "#### Causation:\n",
    "- Definition: Indicates that changes in one variable directly result in changes in another variable.\n",
    "- Implication: Establishes a cause-and-effect relationship.\n",
    "- Example: Drinking contaminated water causes waterborne diseases. Here, consuming contaminated water directly leads to illness.\n",
    "\n",
    "### Example to Illustrate the Difference:\n",
    "- Correlation Example: Imagine you observe that people who wear sunglasses also tend to buy more sunscreen. While there is a correlation between wearing sunglasses and buying sunscreen, it does not mean that wearing sunglasses causes people to buy sunscreen. Both behaviors are likely driven by a third factor: sunny weather.\n",
    "  \n",
    "- Causation Example: Consider a study where researchers find that increasing the amount of exercise leads to weight loss in participants. Here, the act of exercising causes the change in weight, establishing a causal relationship.\n",
    "\n",
    "To determine causation, more rigorous methods such as controlled experiments or longitudinal studies are needed, where variables can be manipulated and observed for their direct effects.\n",
    "Understanding the difference between correlation and causation is crucial in data analysis and scientific research to avoid drawing incorrect conclusions from data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *16. What is an Optimizer? What are different types of optimizers? Explain each with an example.*\n",
    "An optimizer in machine learning is an algorithm used to adjust the parameters of a model in order to minimize the loss function. The goal of an optimizer is to find the best set of parameters that will result in the lowest possible loss, thereby improving the model's performance. Optimizers are crucial in the training process as they determine how the model learns from the data.\n",
    "\n",
    "### Common Types of Optimizers and Their Examples:\n",
    "\n",
    "1. **Gradient Descent**\n",
    "   - Description: The most basic optimization algorithm. It iteratively adjusts the parameters in the opposite direction of the gradient of the loss function with respect to the parameters.\n",
    "   - Example:\n",
    "     ```python\n",
    "     # Gradient Descent update rule\n",
    "     theta = theta - learning_rate * gradient\n",
    "     ```\n",
    "\n",
    "2. **Stochastic Gradient Descent (SGD)**\n",
    "   - Description: A variant of gradient descent where the model updates parameters for each training example individually. This introduces more noise but can lead to faster convergence.\n",
    "   - Example:\n",
    "     ```python\n",
    "     for i in range(num_samples):\n",
    "         theta = theta - learning_rate * gradient_i\n",
    "     ```\n",
    "\n",
    "3. **Mini-Batch Gradient Descent**\n",
    "   - Description: A compromise between batch gradient descent and stochastic gradient descent. It updates parameters based on a small random subset (mini-batch) of the training data.\n",
    "   - Example:\n",
    "     ```python\n",
    "     for mini_batch in generate_mini_batches(data, batch_size):\n",
    "         theta = theta - learning_rate * gradient_mini_batch\n",
    "     ```\n",
    "\n",
    "4. **Momentum**\n",
    "   - Description: An extension of gradient descent that adds a momentum term to accelerate convergence and reduce oscillations. It accumulates a velocity vector that influences the direction of parameter updates.\n",
    "   - Example:\n",
    "     ```python\n",
    "     v = momentum * v - learning_rate * gradient\n",
    "     theta = theta + v\n",
    "     ```\n",
    "\n",
    "5. **Nesterov Accelerated Gradient (NAG)**\n",
    "   - Description: A variant of momentum that anticipates the future position of parameters by looking ahead, which can lead to faster convergence.\n",
    "   - Example:\n",
    "     ```python\n",
    "     v_prev = v\n",
    "     v = momentum * v - learning_rate * gradient(theta + momentum * v_prev)\n",
    "     theta = theta + v\n",
    "     ```\n",
    "\n",
    "6. **Adagrad**\n",
    "   - Description: An optimizer that adapts the learning rate for each parameter based on the accumulated squared gradients. This allows for more significant updates for infrequent features.\n",
    "   - Example:\n",
    "     ```python\n",
    "     G = G + gradient**2\n",
    "     theta = theta - (learning_rate / (sqrt(G) + epsilon)) * gradient\n",
    "     ```\n",
    "\n",
    "7. **RMSprop**\n",
    "   - Description: An improvement over Adagrad that deals with its diminishing learning rate problem by using an exponentially decaying average of squared gradients.\n",
    "   - Example:\n",
    "     ```python\n",
    "     G = decay_rate * G + (1 - decay_rate) * gradient**2\n",
    "     theta = theta - (learning_rate / (sqrt(G) + epsilon)) * gradient\n",
    "     ```\n",
    "\n",
    "8. **Adam (Adaptive Moment Estimation)**\n",
    "   - Description: Combines the ideas of momentum and RMSprop. It computes adaptive learning rates for each parameter using estimates of the first and second moments of the gradients.\n",
    "   - Example:\n",
    "     ```python\n",
    "     m = beta1 * m + (1 - beta1) * gradient\n",
    "     v = beta2 * v + (1 - beta2) * (gradient**2)\n",
    "     m_hat = m / (1 - beta1**t)\n",
    "     v_hat = v / (1 - beta2**t)\n",
    "     theta = theta - learning_rate * m_hat / (sqrt(v_hat) + epsilon)\n",
    "     ```\n",
    "\n",
    "### Summary of Optimizers\n",
    "\n",
    "| Optimizer                     | Characteristics                                                                            |\n",
    "|-------------------------------|--------------------------------------------------------------------------------------------|\n",
    "| Gradient Descent              | Updates parameters using the full dataset.                                                  |\n",
    "| Stochastic Gradient Descent   | Updates parameters for each training example individually.                                  |\n",
    "| Mini-Batch Gradient Descent   | Updates parameters based on small random subsets of the training data.                      |\n",
    "| Momentum                      | Accelerates convergence by adding a velocity term.                                          |\n",
    "| Nesterov Accelerated Gradient | Improves momentum by looking ahead.                                                        |\n",
    "| Adagrad                       | Adapts learning rates based on the accumulated squared gradients.                           |\n",
    "| RMSprop                       | Uses an exponentially decaying average of squared gradients to adjust learning rates.       |\n",
    "| Adam                          | Combines momentum and RMSprop, adjusting learning rates based on first and second moments.  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *17. What is sklearn.linear_model ?*\n",
    "`sklearn.linear_model` is a module within the `scikit-learn` library in Python that provides various linear models for regression and classification. Linear models are foundational in machine learning and statistics because they assume a linear relationship between input variables (features) and output variables (targets).\n",
    "\n",
    "### Key Linear Models Provided by `sklearn.linear_model`:\n",
    "\n",
    "1. **Linear Regression**:\n",
    "   - Usage: For simple and multiple linear regression.\n",
    "   - Example:\n",
    "     ```python\n",
    "     from sklearn.linear_model import LinearRegression\n",
    "     model = LinearRegression()\n",
    "     model.fit(X_train, y_train)\n",
    "     predictions = model.predict(X_test)\n",
    "     ```\n",
    "\n",
    "2. **Ridge Regression**:\n",
    "   - Usage: A type of linear regression that includes L2 regularization to prevent overfitting.\n",
    "   - Example:\n",
    "     ```python\n",
    "     from sklearn.linear_model import Ridge\n",
    "     model = Ridge(alpha=1.0)\n",
    "     model.fit(X_train, y_train)\n",
    "     predictions = model.predict(X_test)\n",
    "     ```\n",
    "\n",
    "3. **Lasso Regression**:\n",
    "   - Usage: A type of linear regression that includes L1 regularization, which can lead to sparse models with fewer coefficients.\n",
    "   - Example:\n",
    "     ```python\n",
    "     from sklearn.linear_model import Lasso\n",
    "     model = Lasso(alpha=0.1)\n",
    "     model.fit(X_train, y_train)\n",
    "     predictions = model.predict(X_test)\n",
    "     ```\n",
    "\n",
    "4. **Elastic Net**:\n",
    "   - Usage: Combines both L1 and L2 regularization.\n",
    "   - Example:\n",
    "     ```python\n",
    "     from sklearn.linear_model import ElasticNet\n",
    "     model = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
    "     model.fit(X_train, y_train)\n",
    "     predictions = model.predict(X_test)\n",
    "     ```\n",
    "\n",
    "5. **Logistic Regression**:\n",
    "   - Usage: For binary and multi-class classification problems.\n",
    "   - Example:\n",
    "     ```python\n",
    "     from sklearn.linear_model import LogisticRegression\n",
    "     model = LogisticRegression()\n",
    "     model.fit(X_train, y_train)\n",
    "     predictions = model.predict(X_test)\n",
    "     ```\n",
    "\n",
    "6. **Bayesian Ridge Regression**:\n",
    "   - Usage: A probabilistic model that incorporates prior distributions for the coefficients.\n",
    "   - Example:\n",
    "     ```python\n",
    "     from sklearn.linear_model import BayesianRidge\n",
    "     model = BayesianRidge()\n",
    "     model.fit(X_train, y_train)\n",
    "     predictions = model.predict(X_test)\n",
    "     ```\n",
    "\n",
    "7. **Perceptron**:\n",
    "   - Usage: A simple linear classifier (binary classifier).\n",
    "   - Example:\n",
    "     ```python\n",
    "     from sklearn.linear_model import Perceptron\n",
    "     model = Perceptron()\n",
    "     model.fit(X_train, y_train)\n",
    "     predictions = model.predict(X_test)\n",
    "     ```\n",
    "\n",
    "### Summary Table\n",
    "\n",
    "| Model                   | Description                                      | Regularization     |\n",
    "|-------------------------|--------------------------------------------------|--------------------|\n",
    "| Linear Regression       | Basic linear regression                          | None               |\n",
    "| Ridge Regression        | Linear regression with L2 regularization         | L2 (Ridge)         |\n",
    "| Lasso Regression        | Linear regression with L1 regularization         | L1 (Lasso)         |\n",
    "| Elastic Net             | Combines L1 and L2 regularization                | L1 and L2          |\n",
    "| Logistic Regression     | Classification model                             | L1 and/or L2       |\n",
    "| Bayesian Ridge          | Probabilistic linear regression                  | Bayesian prior     |\n",
    "| Perceptron              | Simple linear classifier                         | None               |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.09385382291155293\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Sample data\n",
    "X = np.random.rand(100, 3)\n",
    "y = np.random.rand(100)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the model\n",
    "model = Ridge(alpha=1.0)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(f'Mean Squared Error: {mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *18. What does model.fit() do? What arguments must be given?*\n",
    "`model.fit()` is a crucial method used in machine learning to train a model. This method essentially tells the model to learn from the given data. Here’s what happens and what arguments are required:\n",
    "\n",
    "### Purpose of `model.fit()`\n",
    "- Training: The `fit()` method trains the model on the provided dataset, adjusting its parameters based on the input features and target values to minimize the error.\n",
    "- Learning Patterns: It enables the model to learn patterns, relationships, and trends from the data, which are then used to make predictions on new, unseen data.\n",
    "\n",
    "### Required Arguments\n",
    "The arguments required by `model.fit()` can vary depending on the specific model being used, but generally include:\n",
    "\n",
    "1. **X (Input Features)**:\n",
    "   - Description: The feature matrix containing the input data.\n",
    "   - Type: Typically a 2D array or DataFrame where each row represents a sample and each column represents a feature.\n",
    "   - Example:\n",
    "     ```python\n",
    "     X = [[1, 2], [3, 4], [5, 6]]\n",
    "     ```\n",
    "\n",
    "2. **y (Target Values)**:\n",
    "   - Description: The target vector containing the true output values corresponding to the input features.\n",
    "   - Type: Typically a 1D array or Series where each entry represents the target value for the corresponding sample in `X`.\n",
    "   - Example:\n",
    "     ```python\n",
    "     y = [0, 1, 0]\n",
    "     ```\n",
    "\n",
    "### Optional Arguments\n",
    "Some models may also accept optional arguments, such as:\n",
    "\n",
    "- Sample Weights: Array of weights that assigns different importance to different samples.\n",
    "- Validation Data: A tuple `(X_val, y_val)` used for validating the model during training.\n",
    "- Epochs: Number of iterations over the entire dataset (common in neural networks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.33333333 0.33333333]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Sample data\n",
    "X = [[1, 2], [3, 4], [5, 6]]\n",
    "y = [0, 1, 0]\n",
    "\n",
    "# Initialize the model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict([[2, 3], [4, 5]])\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *19. What does model.predict() do? What arguments must be given?*\n",
    "The `model.predict()` method is used to make predictions using a trained machine learning model. After the model has been trained using the `fit()` method, `predict()` is called to predict the target values for new, unseen data.\n",
    "\n",
    "### Purpose of `model.predict()`\n",
    "- Prediction: The primary purpose of `predict()` is to generate predicted values based on the input features provided. These predictions can be used for various tasks, such as classification, regression, etc.\n",
    "- Inference: Allows the model to infer outcomes on new data samples, which were not part of the training set.\n",
    "\n",
    "### Required Arguments\n",
    "- **X (Input Features)**:\n",
    "  - Description: The feature matrix containing the input data for which predictions are to be made.\n",
    "  - Type: Typically a 2D array or DataFrame where each row represents a new sample and each column represents a feature.\n",
    "  - Example:\n",
    "    ```python\n",
    "    X_new = [[2, 3], [4, 5]]\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.33333333 0.33333333]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Sample data for training\n",
    "X_train = [[1, 2], [3, 4], [5, 6]]\n",
    "y_train = [0, 1, 0]\n",
    "\n",
    "# Sample data for prediction\n",
    "X_new = [[2, 3], [4, 5]]\n",
    "\n",
    "# Initialize the model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_new)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *20. What are continuous and categorical variables?*\n",
    "Continuous variables and categorical variables are two types of data used in statistical analysis and machine learning:\n",
    "\n",
    "### Continuous Variables\n",
    "- Definition: These are variables that can take an infinite number of values within a given range. They are typically numerical and can be measured.\n",
    "- Examples: Height, weight, temperature, time, age, and distance.\n",
    "- Characteristics:\n",
    "  - They can be broken down into smaller increments (e.g., 1.5, 1.51, 1.511, etc.).\n",
    "  - Often represented in real numbers.\n",
    "  - Suitable for mathematical operations like addition, subtraction, multiplication, and division.\n",
    "\n",
    "### Categorical Variables\n",
    "- Definition: These variables represent discrete categories or groups and are typically non-numeric. They can be divided into two subtypes: nominal and ordinal.\n",
    "  - Nominal: Categories without a natural order (e.g., colors, types of animals, brands).\n",
    "  - Ordinal: Categories with a natural order (e.g., education levels, rankings, satisfaction ratings).\n",
    "- Examples: Gender, blood type, country of origin, and yes/no responses.\n",
    "- Characteristics:\n",
    "  - They can't be broken down into smaller increments.\n",
    "  - Often represented as labels or names.\n",
    "  - Suitable for counting and frequency analysis but not for mathematical operations.\n",
    "\n",
    "Here's a quick comparison:\n",
    "\n",
    "| Aspect                 | Continuous Variables                      | Categorical Variables                      |\n",
    "|------------------------|------------------------------------------|-------------------------------------------|\n",
    "| Nature             | Numerical                                 | Non-numerical                             |\n",
    "| Examples           | Height, weight, temperature               | Gender, blood type, country of origin     |\n",
    "| Subtypes           | Not applicable                            | Nominal and ordinal                       |\n",
    "| Mathematical Ops   | Addition, subtraction, multiplication     | Counting, frequency analysis              |\n",
    "\n",
    "Both types of variables play crucial roles in data analysis and machine learning. Continuous variables are used for tasks that require precise measurements and calculations, while categorical variables help in grouping and classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *21. What is feature scaling? How does it help in Machine Learning?*\n",
    "\n",
    "Feature scaling is the process of normalizing or standardizing the range of independent variables or features of data. In machine learning, this process ensures that all features contribute equally to the model and helps improve the performance and training stability of the model.\n",
    "\n",
    "### Why Feature Scaling is Important:\n",
    "1. Equality Among Features:\n",
    "   - Some machine learning algorithms, such as gradient descent-based methods, are sensitive to the scale of the input features. If one feature varies in a wider range than others, it might dominate the learning process, skewing the results.\n",
    "\n",
    "2. Improves Convergence Speed:\n",
    "   - Feature scaling can significantly speed up the convergence of gradient descent by ensuring the gradient steps are made in similar magnitudes for all features.\n",
    "\n",
    "3. Prevents Numerical Instability:\n",
    "   - Algorithms like Support Vector Machines, K-Nearest Neighbors, and Principal Component Analysis (PCA) can suffer from numerical instability if the features are on different scales.\n",
    "\n",
    "4. Consistency:\n",
    "   - Ensures that each feature contributes proportionately to the final model, making the model more interpretable.\n",
    "\n",
    "### Common Techniques for Feature Scaling:\n",
    "\n",
    "1. Standardization (Z-score normalization):\n",
    "   - Formula: \\( X_{std} = \\frac{X - \\mu}{\\sigma} \\)\n",
    "   - Explanation: Standardizes the data to have a mean of 0 and a standard deviation of 1.\n",
    "   - Example:\n",
    "     ```python\n",
    "     from sklearn.preprocessing import StandardScaler\n",
    "     scaler = StandardScaler()\n",
    "     X_scaled = scaler.fit_transform(X)\n",
    "     ```\n",
    "\n",
    "2. Min-Max Scaling (Normalization):\n",
    "   - Formula: \\( X_{norm} = \\frac{X - X_{min}}{X_{max} - X_{min}} \\)\n",
    "   - Explanation: Scales the data to a fixed range, typically [0, 1].\n",
    "   - Example:\n",
    "     ```python\n",
    "     from sklearn.preprocessing import MinMaxScaler\n",
    "     scaler = MinMaxScaler()\n",
    "     X_scaled = scaler.fit_transform(X)\n",
    "     ```\n",
    "\n",
    "3. MaxAbs Scaling:\n",
    "   - Formula: \\( X_{maxabs} = \\frac{X}{|X_{max}|} \\)\n",
    "   - Explanation: Scales data by dividing by the maximum absolute value of each feature.\n",
    "   - Example\n",
    "     ```python\n",
    "     from sklearn.preprocessing import MaxAbsScaler\n",
    "     scaler = MaxAbsScaler()\n",
    "     X_scaled = scaler.fit_transform(X)\n",
    "     ```\n",
    "\n",
    "4. Robust Scaling:\n",
    "   - Formula: \\( X_{robust} = \\frac{X - Q1}{Q3 - Q1} \\)\n",
    "   - Explanation: Scales the data according to the interquartile range (IQR), making it robust to outliers.\n",
    "   - Example:\n",
    "     ```python\n",
    "     from sklearn.preprocessing import RobustScaler\n",
    "     scaler = RobustScaler()\n",
    "     X_scaled = scaler.fit_transform(X)\n",
    "     ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      " [[1. 2.]\n",
      " [2. 3.]\n",
      " [3. 4.]]\n",
      "Scaled Data:\n",
      " [[-1.22474487 -1.22474487]\n",
      " [ 0.          0.        ]\n",
      " [ 1.22474487  1.22474487]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Sample data\n",
    "X = np.array([[1.0, 2.0], [2.0, 3.0], [3.0, 4.0]])\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(\"Original Data:\\n\", X)\n",
    "print(\"Scaled Data:\\n\", X_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *22. How do we perform scaling in Python?*\n",
    "Scaling is a crucial step in preprocessing your data for machine learning models. Here’s how you can perform scaling in Python using the `scikit-learn` library.\n",
    "\n",
    "### Steps to Perform Scaling:\n",
    "\n",
    "1. **Install `scikit-learn`**:\n",
    "   - If you haven’t already, you can install it using pip:\n",
    "     ```bash\n",
    "     pip install scikit-learn\n",
    "     ```\n",
    "\n",
    "2. **Import Necessary Libraries**:\n",
    "   ```python\n",
    "   import numpy as np\n",
    "   from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, MaxAbsScaler\n",
    "   ```\n",
    "\n",
    "3. **Prepare Your Data**:\n",
    "   - Example data:\n",
    "     ```python\n",
    "     X = np.array([[1.0, 2.0], [2.0, 3.0], [3.0, 4.0]])\n",
    "     ```\n",
    "\n",
    "4. **Choose a Scaler**:\n",
    "   - You can choose from various scalers depending on your needs:\n",
    "     - StandardScaler: Standardizes features by removing the mean and scaling to unit variance.\n",
    "     - MinMaxScaler: Scales features to a specified range, usually [0, 1].\n",
    "     - RobustScaler: Scales features using statistics that are robust to outliers.\n",
    "     - MaxAbsScaler: Scales each feature by its maximum absolute value.\n",
    "\n",
    "5. **Fit and Transform the Data**:\n",
    "   - Here’s how to use each scaler:\n",
    "\n",
    "   **StandardScaler**:\n",
    "   ```python\n",
    "   scaler = StandardScaler()\n",
    "   X_scaled = scaler.fit_transform(X)\n",
    "   print(\"StandardScaler:\\n\", X_scaled)\n",
    "   ```\n",
    "\n",
    "   **MinMaxScaler**:\n",
    "   ```python\n",
    "   scaler = MinMaxScaler()\n",
    "   X_scaled = scaler.fit_transform(X)\n",
    "   print(\"MinMaxScaler:\\n\", X_scaled)\n",
    "   ```\n",
    "\n",
    "   **RobustScaler**:\n",
    "   ```python\n",
    "   scaler = RobustScaler()\n",
    "   X_scaled = scaler.fit_transform(X)\n",
    "   print(\"RobustScaler:\\n\", X_scaled)\n",
    "   ```\n",
    "\n",
    "   **MaxAbsScaler**:\n",
    "   ```python\n",
    "   scaler = MaxAbsScaler()\n",
    "   X_scaled = scaler.fit_transform(X)\n",
    "   print(\"MaxAbsScaler:\\n\", X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      " [[1. 2.]\n",
      " [2. 3.]\n",
      " [3. 4.]]\n",
      "Scaled Data:\n",
      " [[-1.22474487 -1.22474487]\n",
      " [ 0.          0.        ]\n",
      " [ 1.22474487  1.22474487]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Sample data\n",
    "X = np.array([[1.0, 2.0], [2.0, 3.0], [3.0, 4.0]])\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(\"Original Data:\\n\", X)\n",
    "print(\"Scaled Data:\\n\", X_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *23. What is sklearn.preprocessing?*\n",
    "`sklearn.preprocessing` is a module in the `scikit-learn` library that provides various utilities for preprocessing data. Preprocessing is a crucial step in the machine learning pipeline because it transforms raw data into a suitable format for modeling. This includes scaling, encoding, imputing missing values, and more.\n",
    "\n",
    "Here are some key functionalities provided by `sklearn.preprocessing`:\n",
    "\n",
    "### 1. **Scaling Features**\n",
    "- StandardScaler: Standardizes features by removing the mean and scaling to unit variance.\n",
    "- MinMaxScaler: Transforms features by scaling each feature to a given range, typically between zero and one.\n",
    "- MaxAbsScaler: Scales each feature by its maximum absolute value.\n",
    "\n",
    "### 2. **Encoding Categorical Features**\n",
    "- LabelEncoder: Encodes target labels with value between 0 and n_classes-1.\n",
    "- OneHotEncoder: Encodes categorical features as a one-hot numeric array.\n",
    "\n",
    "### 3. **Imputing Missing Values**\n",
    "- SimpleImputer: Imputes missing values using mean, median, most frequent, or constant value.\n",
    "- KNNImputer: Imputes missing values using k-nearest neighbors.\n",
    "\n",
    "### 4. **Generating Polynomial Features**\n",
    "- PolynomialFeatures: Generates a new feature matrix consisting of all polynomial combinations of the features with a specified degree.\n",
    "\n",
    "### 5. **Normalizing Data**\n",
    "- Normalizer: Normalizes samples individually to unit norm.\n",
    "\n",
    "### 6. **Binarizing Data**\n",
    "- Binarizer: Thresholds numerical features to binary values.\n",
    "\n",
    " \n",
    "`sklearn.preprocessing` offers many tools to prepare data effectively, ensuring the best possible performance from your machine learning models.\n",
    "\n",
    "Do you want to explore a specific preprocessing technique or need an example on how to apply it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.18321596 -1.18321596]\n",
      " [-0.50709255 -0.50709255]\n",
      " [ 0.16903085  0.16903085]\n",
      " [ 1.52127766  1.52127766]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Sample data\n",
    "data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "print(scaled_data)\n",
    "\n",
    "# In this example, `StandardScaler` will standardize the input data by removing the mean and scaling to unit variance, making the data ready for machine learning algorithms that perform better with standardized input.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *24. How do we split data for model fitting (training and testing) in Python?*\n",
    "To split data for model fitting in Python, we commonly use the `train_test_split` function from the `scikit-learn` library. Here’s a step-by-step guide:\n",
    "\n",
    "##### Steps to Split Data for Training and Testing:\n",
    "\n",
    "1. **Install `scikit-learn`**:\n",
    "   - If you haven’t already, install it using pip:\n",
    "     ```bash\n",
    "     pip install scikit-learn\n",
    "     ```\n",
    "\n",
    "2. **Import Necessary Libraries**:\n",
    "   ```python\n",
    "   from sklearn.model_selection import train_test_split\n",
    "   import numpy as np\n",
    "   ```\n",
    "\n",
    "3. **Prepare Your Data**:\n",
    "   - Example data:\n",
    "     ```python\n",
    "     X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])\n",
    "     y = np.array([0, 1, 0, 1, 0])\n",
    "     ```\n",
    "\n",
    "4. **Split the Data**:\n",
    "   - Use `train_test_split` to create training and testing datasets:\n",
    "     ```python\n",
    "     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "     # X_train and y_train are used for training the model\n",
    "     # X_test and y_test are used for testing the model\n",
    "     ```\n",
    "\n",
    "The `test_size` parameter specifies the proportion of the dataset to include in the test split (e.g., 0.2 means 20% of the data will be used for testing). The `random_state` parameter ensures reproducibility of the results.\n",
    " \n",
    "### How to Approach a Machine Learning Problem\n",
    "\n",
    "1. Define the Problem: Clearly understand the problem you're trying to solve and define the objective.\n",
    "2. Gather Data: Collect relevant data that is representative of the problem.\n",
    "3. Exploratory Data Analysis (EDA): Analyze the data to understand its structure, detect patterns, and identify potential issues.\n",
    "4. Preprocess Data: Clean and preprocess the data. Handle missing values, encode categorical variables, and scale numerical features.\n",
    "5. Feature Engineering: Create new features from existing data to improve model performance. Select the most relevant features.\n",
    "6. Split Data: Split the data into training, validation (optional), and test sets.\n",
    "7. Choose a Model: Select a suitable machine learning algorithm based on the problem and data characteristics.\n",
    "8. Train the Model: Train the model using the training dataset.\n",
    "9. Evaluate the Model: Assess the model’s performance using the test set and appropriate metrics.\n",
    "10. Refine the Model: Adjust features, tune hyperparameters, or try different algorithms if the model's performance is not satisfactory.\n",
    "11. Deploy the Model: Integrate the model into a real-world environment for making predictions on live data.\n",
    "12. Monitor and Maintain the Model: Continuously monitor the model's performance and update it as necessary to maintain its effectiveness.\n",
    "\n",
    "This structured approach ensures that each step is methodically addressed, leading to more robust and reliable machine learning models.\n",
    "\n",
    "Would you like to dive deeper into any specific stage of this process or need help with a particular aspect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (4, 2)\n",
      "X_test shape: (1, 2)\n",
      "y_train shape: (4,)\n",
      "y_test shape: (1,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Sample data\n",
    "X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])\n",
    "y = np.array([0, 1, 0, 1, 0])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the resulting shapes\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *25. Explain data encoding?*\n",
    "Data encoding is a crucial step in preparing data for machine learning models, especially when dealing with categorical variables. It involves converting categorical data into a numerical format that can be easily processed by machine learning algorithms. Here are some common techniques for data encoding:\n",
    "\n",
    "### Common Encoding Techniques\n",
    "\n",
    "1. **Label Encoding**:\n",
    "   - Description: Converts each category to a numerical label. This method is straightforward but can sometimes imply an ordinal relationship between categories that doesn't exist.\n",
    "  \n",
    "\n",
    "2. **One-Hot Encoding**:\n",
    "   - *Description*: Converts each category into a binary vector, where each unique category is represented as a one-hot encoded vector. This method avoids ordinal relationships and works well with machine learning algorithms.\n",
    "\n",
    "3. **Ordinal Encoding**:\n",
    "   - Description: Used when the categorical variable has a natural order. Categories are mapped to integers that reflect their order.\n",
    "\n",
    "4. **Frequency Encoding**:\n",
    "   - Description: Replaces each category with the frequency of its occurrence in the data.\n",
    "\n",
    "\n",
    "5. **Target Encoding (Mean Encoding)**:\n",
    "   - Description: Replaces categories with the mean of the target variable for that category. Useful for high-cardinality categorical features.\n",
    "   \n",
    " \n",
    "Data encoding transforms categorical data into numerical values, enabling machine learning algorithms to process the data effectively. The choice of encoding technique depends on the nature of the categorical variable and the specific requirements of the machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Sample data\n",
    "data = ['red', 'green', 'blue']\n",
    "\n",
    "# Initialize the encoder\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the data\n",
    "encoded_data = encoder.fit_transform(data)\n",
    "\n",
    "print(encoded_data)  # Output: [2, 1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "data = np.array(['red', 'green', 'blue']).reshape(-1, 1)\n",
    "\n",
    "# Initialize the encoder\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "# Fit and transform the data\n",
    "encoded_data = encoder.fit_transform(data)\n",
    "\n",
    "print(encoded_data)\n",
    "     # Output:\n",
    "     # [[0. 0. 1.]\n",
    "     #  [0. 1. 0.]\n",
    "     #  [1. 0. 0.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.]\n",
      " [2.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Sample data\n",
    "data = [['low'], ['medium'], ['high']]\n",
    "\n",
    "# Initialize the encoder\n",
    "encoder = OrdinalEncoder()\n",
    "\n",
    "# Fit and transform the data\n",
    "encoded_data = encoder.fit_transform(data)\n",
    "\n",
    "print(encoded_data)  # Output: [[0.], [1.], [2.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    2\n",
      "1    1\n",
      "2    3\n",
      "3    2\n",
      "4    3\n",
      "5    3\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "data = pd.Series(['red', 'green', 'blue', 'red', 'blue', 'blue'])\n",
    "\n",
    "# Frequency encoding\n",
    "frequency_encoded = data.map(data.value_counts())\n",
    "\n",
    "print(frequency_encoded)  # Output: [2, 1, 3, 2, 3, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1.0\n",
      "1    0.0\n",
      "2    1.0\n",
      "3    0.0\n",
      "4    0.5\n",
      "5    0.5\n",
      "Name: target, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "data = pd.DataFrame({\n",
    "         'category': ['A', 'B', 'A', 'B', 'C', 'C'],\n",
    "         'target': [1, 0, 1, 0, 1, 0]\n",
    "     })\n",
    "\n",
    "# Mean encoding\n",
    "mean_encoded = data.groupby('category')['target'].transform('mean')\n",
    "\n",
    "print(mean_encoded)\n",
    "     # Output:\n",
    "     # 0    1.0\n",
    "     # 1    0.0\n",
    "     # 2    1.0\n",
    "     # 3    0.0\n",
    "     # 4    0.5\n",
    "     # 5    0.5\n",
    "     # Name: target, dtype: float64"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
