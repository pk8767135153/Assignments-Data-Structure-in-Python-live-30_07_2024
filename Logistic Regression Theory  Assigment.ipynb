{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22b57caa-59a7-4844-8853-c7a6b5ab0233",
   "metadata": {},
   "source": [
    "**1. What is Logistic Regression, and how does it differ from Linear Regression?**\n",
    "\n",
    "* **Linear Regression:**\n",
    "    * Predicts a continuous output variable.\n",
    "    * Models the relationship between independent and dependent variables as a straight line.\n",
    "    * Output range: (-∞, +∞).\n",
    "* **Logistic Regression:**\n",
    "    * Predicts a categorical output variable (typically binary, 0 or 1).\n",
    "    * Models the probability of a certain outcome occurring.\n",
    "    * Output range: (0, 1).\n",
    "    * Uses the sigmoid function to map the linear combination of inputs to a probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bfc770-8349-4e14-aa9a-b3da7fb51d09",
   "metadata": {},
   "source": [
    "\n",
    "**2. What is the mathematical equation of Logistic Regression?**\n",
    "\n",
    "The equation is:\n",
    "\n",
    "* `p(y=1|x) = 1 / (1 + e^(-z))`\n",
    "    * Where:\n",
    "        * `p(y=1|x)` is the probability of the output being 1 given the input features `x`.\n",
    "        * `e` is Euler's number (approximately 2.71828).\n",
    "        * `z = b0 + b1*x1 + b2*x2 + ... + bn*xn` is the linear combination of input features, where `b0` is the intercept and `b1`, `b2`, ..., `bn` are the coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9734d85f-bc36-4211-8e72-04b799ca0a69",
   "metadata": {},
   "source": [
    "**3. Why do we use the Sigmoid function in Logistic Regression?**\n",
    "\n",
    "* The sigmoid function (also called the logistic function) squashes any real-valued number into a range between 0 and 1.\n",
    "* This makes it ideal for representing probabilities.\n",
    "* It provides a smooth, S-shaped curve that's differentiable, which is important for optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf6ebe5-9563-44e0-b9e3-5cd031403c42",
   "metadata": {},
   "source": [
    "\n",
    "**4. What is the cost function of Logistic Regression?**\n",
    "\n",
    "* The cost function is typically the **cross-entropy** (or log loss) function.\n",
    "* For binary classification:\n",
    "    * `Cost(h(x), y) = -y * log(h(x)) - (1 - y) * log(1 - h(x))`\n",
    "    * Where:\n",
    "        * `h(x)` is the predicted probability (output of the sigmoid function).\n",
    "        * `y` is the actual label (0 or 1).\n",
    "* The goal is to minimize this cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06358f5-aaf9-4860-8230-4f8689107ff4",
   "metadata": {},
   "source": [
    "**5. What is Regularization in Logistic Regression? Why is it needed?**\n",
    "\n",
    "* Regularization adds a penalty term to the cost function to prevent overfitting.\n",
    "* Overfitting occurs when a model learns the training data too well, including noise, and performs poorly on unseen data.\n",
    "* Regularization reduces the complexity of the model by shrinking the coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d7e540-365d-4034-a6ca-05609bf28078",
   "metadata": {},
   "source": [
    "**6. Explain the difference between Lasso, Ridge, and Elastic Net regression.**\n",
    "\n",
    "* **Ridge Regression (L2 Regularization):**\n",
    "    * Adds the squared magnitude of coefficients to the cost function.\n",
    "    * Shrinks coefficients towards zero but rarely makes them exactly zero.\n",
    "    * Reduces the impact of less important features.\n",
    "* **Lasso Regression (L1 Regularization):**\n",
    "    * Adds the absolute magnitude of coefficients to the cost function.\n",
    "    * Can shrink some coefficients to exactly zero, effectively performing feature selection.\n",
    "    * Useful when you suspect many features are irrelevant.\n",
    "* **Elastic Net Regression:**\n",
    "    * Combines L1 and L2 regularization.\n",
    "    * Provides a balance between feature selection and coefficient shrinkage.\n",
    "    * It helps when there are highly correlated features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e817f61f-72b5-4158-a280-c4620f3e01fa",
   "metadata": {},
   "source": [
    "**7. When should we use Elastic Net instead of Lasso or Ridge?**\n",
    "\n",
    "* When you have a dataset with many features, some of which are correlated.\n",
    "* It provides a balance between the strengths of Lasso and Ridge.\n",
    "* It is more stable than Lasso when dealing with highly correlated features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054e0bfe-3164-4a91-8771-b2e73c37a6a3",
   "metadata": {},
   "source": [
    "**8. What is the impact of the regularization parameter (λ) in Logistic Regression?**\n",
    "\n",
    "* λ (lambda) controls the strength of regularization.\n",
    "* A larger λ increases regularization, shrinking coefficients more aggressively.\n",
    "* A smaller λ reduces regularization, allowing coefficients to be larger.\n",
    "* When lambda is zero, then there is no regularization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3267a46d-736b-41d6-9c3f-d8dc38350651",
   "metadata": {},
   "source": [
    "**9. What are the key assumptions of Logistic Regression?**\n",
    "\n",
    "* Binary output variable.\n",
    "* Independence of observations.\n",
    "* Little to no multicollinearity among predictors.\n",
    "* Linearity between the log-odds of the outcome and the predictor variables.\n",
    "* Adequate sample size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342950e5-2821-4770-93f8-21cd52620ed0",
   "metadata": {},
   "source": [
    "**10. What are some alternatives to Logistic Regression for classification tasks?**\n",
    "\n",
    "* Support Vector Machines (SVMs)\n",
    "* Decision Trees\n",
    "* Random Forests\n",
    "* Gradient Boosting Machines (e.g., XGBoost, LightGBM)\n",
    "* Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc72ad6-54ab-4eb4-ae44-0ab382171c05",
   "metadata": {},
   "source": [
    "**11. What are Classification Evaluation Metrics?**\n",
    "\n",
    "* **Accuracy:** Overall correctness.\n",
    "* **Precision:** Correct positive predictions out of all positive predictions.\n",
    "* **Recall (Sensitivity):** Correct positive predictions out of all actual positives.\n",
    "* **F1-score:** Harmonic mean of precision and recall.\n",
    "* **AUC-ROC:** Area under the Receiver Operating Characteristic curve, measuring the model's ability to distinguish between classes.\n",
    "* **Confusion Matrix:** A table showing true positives, true negatives, false positives, and false negatives.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc441727-0b8d-405e-ae32-2720175c7e05",
   "metadata": {},
   "source": [
    "\n",
    "**12. How does class imbalance affect Logistic Regression?**\n",
    "\n",
    "* Class imbalance occurs when one class has significantly more samples than the other.\n",
    "* It can lead to biased models that favor the majority class.\n",
    "* Techniques to handle it include:\n",
    "    * Oversampling the minority class.\n",
    "    * Undersampling the majority class.\n",
    "    * Using class weights in the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277945c4-47a1-405a-92ce-23519e2dc91a",
   "metadata": {},
   "source": [
    "**13. What is Hyperparameter Tuning in Logistic Regression?**\n",
    "\n",
    "* Hyperparameter tuning involves finding the optimal values for parameters that are not learned from the data (e.g., λ, solver).\n",
    "* Methods include:\n",
    "    * Grid search.\n",
    "    * Random search.\n",
    "    * Cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4841b2f-e6f2-48cf-8365-270fb77d5c04",
   "metadata": {},
   "source": [
    "**14. What are different solvers in Logistic Regression? Which one should be used?**\n",
    "\n",
    "* Solvers optimize the cost function. Common ones include:\n",
    "    * `liblinear`: Suitable for small datasets.\n",
    "    * `lbfgs`: Good for small to medium datasets.\n",
    "    * `sag`: Suitable for large datasets.\n",
    "    * `saga`: Suitable for large datasets and handles L1 regularization well.\n",
    "* The choice depends on the dataset size and regularization type. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a60e5f-ecf7-4b34-8bc6-9110d4c5d735",
   "metadata": {},
   "source": [
    "**15. How is Logistic Regression extended for multiclass classification?**\n",
    "\n",
    "* **One-vs-Rest (OvR) or One-vs-All (OvA):** Train a binary logistic regression classifier for each class against all other classes.\n",
    "* **Softmax Regression (Multinomial Logistic Regression):** Directly models the probabilities of multiple classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99978906-1ce7-4579-9854-d00e0615b0de",
   "metadata": {},
   "source": [
    "**16. What are the advantages and disadvantages of Logistic Regression?**\n",
    "\n",
    "* **Advantages:**\n",
    "    * Easy to implement and interpret.\n",
    "    * Efficient for training.\n",
    "    * Provides probability estimates.\n",
    "* **Disadvantages:**\n",
    "    * Assumes linearity.\n",
    "    * Sensitive to outliers.\n",
    "    * May not perform well with complex relationships. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acdce0b-160f-477e-ab7d-0a155443ff2e",
   "metadata": {},
   "source": [
    "**17. What are some use cases of Logistic Regression?**\n",
    "\n",
    "* Medical diagnosis (e.g., predicting disease risk).\n",
    "* Credit risk assessment.\n",
    "* Spam detection.\n",
    "* Customer churn prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874b1271-2be3-4fe7-bcbc-1eeaa9253416",
   "metadata": {},
   "source": [
    "**18. What is the difference between Softmax Regression and Logistic Regression?**\n",
    "\n",
    "* **Logistic Regression:** Binary classification.\n",
    "* **Softmax Regression:** Multiclass classification.\n",
    "* Logistic regression uses the sigmoid function, and Softmax uses the Softmax function.\n",
    "* Softmax outputs a probability distribution over multiple classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a0494d-f1af-41ee-994c-fc74fae4b9d7",
   "metadata": {},
   "source": [
    "**19. How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?**\n",
    "\n",
    "* **Softmax:** Preferred when classes are mutually exclusive. More efficient in training.\n",
    "* **OvR:** Can be used when classes are not mutually exclusive. Simpler to implement.\n",
    "* For most cases, Softmax is generally the best option."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8175a8-fa00-453e-b023-bdcee639dab0",
   "metadata": {},
   "source": [
    "**20. How do we interpret coefficients in Logistic Regression?**\n",
    "\n",
    "* Coefficients represent the change in the log-odds of the outcome for a one-unit change in the predictor, holding other predictors constant.\n",
    "* A positive coefficient increases the log-odds (and thus the probability), while a negative coefficient decreases it.\n",
    "* To get the odds ratio, you can exponentiate the coefficient (exp(coefficient)). The odds ratio can be interpreted as the multiplicative change in the odds of the outcome for a one-unit increase in the predictor."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
