{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. What is Simple Linear Regression\n",
        "\n",
        "Simple Linear Regression is a statistical method used to understand the relationship between two continuous variables. Essentially, it helps us predict the value of one variable (dependent variable) based on the value of another variable (independent variable). Here's a brief rundown of how it works:\n",
        "\n",
        "1. **Assumption**: We assume there is a linear relationship between the two variables, which means when we plot them on a graph, the data points form a pattern that resembles a straight line.\n",
        "2. **Equation**: The relationship is represented by the linear equation:  \n",
        "   $$ y = mx + c $$\n",
        "   - **\\( y \\)** is the dependent variable we want to predict.\n",
        "   - **\\( x \\)** is the independent variable we use to make predictions.\n",
        "   - **\\( m \\)** is the slope of the line, which indicates the rate of change of the dependent variable with respect to the independent variable.\n",
        "   - **\\( c \\)** is the intercept, the value of \\( y \\) when \\( x \\) is zero.\n",
        "\n",
        "3. **Goal**: The main goal is to find the best-fitting line through the data points. This line minimizes the sum of the squared differences (residuals) between the observed values and the values predicted by the line."
      ],
      "metadata": {
        "id": "7nXWvT2fxY41"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. What are the key assumptions of Simple Linear Regression\n",
        "\n",
        "\n",
        "Simple Linear Regression relies on several key assumptions to produce reliable and meaningful results. Here are the main assumptions:\n",
        "\n",
        "1. **Linearity**: The relationship between the independent variable \\( x \\) and the dependent variable \\( y \\) is linear. This means that when plotted, the data points form a pattern that resembles a straight line.\n",
        "\n",
        "2. **Independence**: The observations are independent of each other. In other words, the value of the dependent variable for one observation does not influence the value for another observation.\n",
        "\n",
        "3. **Homoscedasticity**: The residuals (the differences between the observed and predicted values) have constant variance. This means that the spread of the residuals is similar across all levels of the independent variable \\( x \\).\n",
        "\n",
        "4. **Normality of Residuals**: The residuals are normally distributed. This assumption is important for making valid statistical inferences.\n",
        "\n",
        "5. **No Multicollinearity**: This assumption is more relevant for multiple linear regression, but it's good to be aware of it. It means that the independent variables should not be too highly correlated with each other.\n",
        "\n",
        "6. **No Autocorrelation**: This assumption means that the residuals should not be correlated with each other. Autocorrelation can be an issue in time series data."
      ],
      "metadata": {
        "id": "NliqFGwyxp6v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.   What does the coefficient m represent in the equation Y=mX+c\n",
        "\n",
        "In the equation \\( Y = mX + c \\), the coefficient \\( m \\) is known as the **slope** of the line. It represents the rate at which the dependent variable \\( Y \\) changes for every one-unit increase in the independent variable \\( X \\). In other words, the slope \\( m \\) tells us how steep the line is and the direction of the relationship between \\( X \\) and \\( Y \\).\n",
        "\n",
        "- If \\( m \\) is positive, it indicates a positive relationship between \\( X \\) and \\( Y \\), meaning that as \\( X \\) increases, \\( Y \\) also increases.\n",
        "- If \\( m \\) is negative, it indicates a negative relationship between \\( X \\) and \\( Y \\), meaning that as \\( X \\) increases, \\( Y \\) decreases.\n",
        "- If \\( m \\) is zero, it means there is no relationship between \\( X \\) and \\( Y \\); the line is horizontal.\n",
        "\n",
        "The slope is a crucial part of the linear equation because it quantifies the strength and direction of the relationship between the two variables. Essentially, it helps us understand how much of an impact changes in \\( X \\) have on \\( Y \\).\n",
        "\n"
      ],
      "metadata": {
        "id": "oeDCPhGIyA53"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.  What does the intercept c represent in the equation Y=mX+c\n",
        "\n",
        "In the equation \\( Y = mX + c \\), the intercept \\( c \\) is the point where the line crosses the \\( Y \\)-axis. This value represents the predicted value of \\( Y \\) when the independent variable \\( X \\) is zero. Essentially, it's the baseline value of \\( Y \\) when \\( X \\) has no influence.\n",
        "\n",
        "Think of the intercept as the starting point of your prediction. For instance, if you're predicting the cost of a meal based on the number of dishes, the intercept \\( c \\) might represent the base cost (e.g., service charge or basic setup) before any dishes are ordered.\n",
        "\n",
        "Here's a quick visualization:\n",
        "- When \\( X = 0 \\), the equation simplifies to \\( Y = c \\).\n",
        "- So, \\( c \\) is the value of \\( Y \\) that you get when \\( X \\) is zero.\n",
        ""
      ],
      "metadata": {
        "id": "4f9Q2ad7yO59"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5.  How do we calculate the slope m in Simple Linear Regression\n",
        "\n",
        "To calculate the slope \\( m \\) in Simple Linear Regression, you can use the formula derived from the least squares method. Here's how it's done:\n",
        "\n",
        "$$ m = \\frac{n(\\sum{XY}) - (\\sum{X})(\\sum{Y})}{n(\\sum{X^2}) - (\\sum{X})^2} $$\n",
        "\n",
        "Where:\n",
        "- \\( n \\) is the number of data points.\n",
        "- \\( \\sum{XY} \\) is the sum of the product of the independent variable (\\( X \\)) and the dependent variable (\\( Y \\)).\n",
        "- \\( \\sum{X} \\) is the sum of the independent variable (\\( X \\)).\n",
        "- \\( \\sum{Y} \\) is the sum of the dependent variable (\\( Y \\)).\n",
        "- \\( \\sum{X^2} \\) is the sum of the squares of the independent variable (\\( X \\)).\n",
        "\n",
        "Let’s break it down step-by-step:\n",
        "\n",
        "1. **Calculate the sums**:\n",
        "   - \\( \\sum{X} \\)\n",
        "   - \\( \\sum{Y} \\)\n",
        "   - \\( \\sum{XY} \\)\n",
        "   - \\( \\sum{X^2} \\)\n",
        "\n",
        "2. **Plug these sums into the formula**:\n",
        "   - Compute the numerator: \\( n(\\sum{XY}) - (\\sum{X})(\\sum{Y}) \\)\n",
        "   - Compute the denominator: \\( n(\\sum{X^2}) - (\\sum{X})^2 \\)\n",
        "\n",
        "3. **Divide the numerator by the denominator** to get the slope \\( m \\).\n",
        "\n",
        "Here’s an example:\n",
        "\n",
        "Imagine you have the following data points:\n",
        "\n",
        "| \\( X \\) | \\( Y \\) |\n",
        "|:------:|:------:|\n",
        "|   1    |   2    |\n",
        "|   2    |   3    |\n",
        "|   3    |   5    |\n",
        "|   4    |   4    |\n",
        "|   5    |   6    |\n",
        "\n",
        "1. **Calculate the sums**:\n",
        "   - \\( \\sum{X} = 1 + 2 + 3 + 4 + 5 = 15 \\)\n",
        "   - \\( \\sum{Y} = 2 + 3 + 5 + 4 + 6 = 20 \\)\n",
        "   - \\( \\sum{XY} = 1 \\cdot 2 + 2 \\cdot 3 + 3 \\cdot 5 + 4 \\cdot 4 + 5 \\cdot 6 = 2 + 6 + 15 + 16 + 30 = 69 \\)\n",
        "   - \\( \\sum{X^2} = 1^2 + 2^2 + 3^2 + 4^2 + 5^2 = 1 + 4 + 9 + 16 + 25 = 55 \\)\n",
        "\n",
        "2. **Plug these sums into the formula**:\n",
        "   - Numerator: \\( 5 \\cdot 69 - 15 \\cdot 20 = 345 - 300 = 45 \\)\n",
        "   - Denominator: \\( 5 \\cdot 55 - 15^2 = 275 - 225 = 50 \\)\n",
        "\n",
        "3. **Divide the numerator by the denominator**:\n",
        "   - \\( m = \\frac{45}{50} = 0.9 \\)\n",
        "\n",
        "So, the slope \\( m \\) is 0.9."
      ],
      "metadata": {
        "id": "sHkiDP2Mya24"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. What is the purpose of the least squares method in Simple Linear Regression\n",
        "\n",
        "The least squares method is a key technique in Simple Linear Regression, used to find the best-fitting line through a set of data points. The main purposes of the least squares method are:\n",
        "\n",
        "1. **Minimize Errors**: The goal is to minimize the sum of the squared differences (residuals) between the observed values (actual data points) and the predicted values (values on the regression line). By minimizing these squared errors, the line of best fit is found, providing the most accurate representation of the relationship between the independent and dependent variables.\n",
        "\n",
        "2. **Optimize Predictions**: By using the least squares method, we can optimize the parameters (slope \\( m \\) and intercept \\( c \\)) of the linear equation \\( Y = mX + c \\). This ensures that our predictions for the dependent variable \\( Y \\) are as close as possible to the actual observed values.\n",
        "\n",
        "3. **Quantify Relationships**: The least squares method helps quantify the strength and direction of the relationship between the independent variable \\( X \\) and the dependent variable \\( Y \\). The slope \\( m \\) indicates how much \\( Y \\) changes for a one-unit change in \\( X \\), while the intercept \\( c \\) represents the value of \\( Y \\) when \\( X \\) is zero.\n",
        "\n",
        "4. **Simplify Analysis**: The least squares method provides a straightforward and mathematically sound way to fit a linear model to data. This makes it easier to analyze and interpret the relationship between variables, identify trends, and make informed decisions based on the data."
      ],
      "metadata": {
        "id": "OOZp8fYfyxCx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. How is the coefficient of determination (R²) interpreted in Simple Linear Regression\n",
        "\n",
        "The coefficient of determination, denoted as \\( R^2 \\), is a key metric in Simple Linear Regression that helps us understand the goodness of fit of the regression model. Here's how to interpret it:\n",
        "\n",
        "1. **Explained Variance**: \\( R^2 \\) represents the proportion of the total variation in the dependent variable \\( Y \\) that is explained by the independent variable \\( X \\). It ranges from 0 to 1, with 0 indicating that the independent variable explains none of the variance in the dependent variable, and 1 indicating that it explains all of the variance.\n",
        "\n",
        "2. **Strength of the Relationship**: A higher \\( R^2 \\) value indicates a stronger relationship between the independent and dependent variables. For example, an \\( R^2 \\) value of 0.75 means that 75% of the variance in \\( Y \\) can be explained by \\( X \\), while the remaining 25% is due to other factors or random noise.\n",
        "\n",
        "3. **Model Performance**: \\( R^2 \\) provides a measure of how well the regression model fits the data. A higher \\( R^2 \\) value suggests that the model is better at predicting the dependent variable. However, it is important to note that a high \\( R^2 \\) does not necessarily mean the model is good. It is also essential to consider other factors, such as the validity of assumptions and potential overfitting.\n",
        "\n",
        "4. **Comparison Tool**: \\( R^2 \\) can be used to compare different regression models. When evaluating multiple models, the one with the higher \\( R^2 \\) value is generally considered to have a better fit to the data.\n",
        "\n",
        "Here's a quick summary of the \\( R^2 \\) interpretation:\n",
        "- \\( R^2 = 0 \\): The independent variable explains none of the variance in the dependent variable.\n",
        "- \\( R^2 = 1 \\): The independent variable explains all of the variance in the dependent variable.\n",
        "- \\( R^2 = 0.75 \\): 75% of the variance in the dependent variable is explained by the independent variable."
      ],
      "metadata": {
        "id": "-MgcLRbKzBgk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. What is Multiple Linear Regression\n",
        "\n",
        "Multiple Linear Regression is an extension of Simple Linear Regression that models the relationship between a dependent variable and two or more independent variables. This method is particularly useful when you believe that several factors influence the outcome you're interested in predicting. Here's a breakdown:\n",
        "\n",
        "### Basics\n",
        "The general form of the Multiple Linear Regression equation is:\n",
        "$$ Y = b_0 + b_1X_1 + b_2X_2 + \\ldots + b_nX_n $$\n",
        "Where:\n",
        "- \\( Y \\) is the dependent variable (the outcome we're trying to predict).\n",
        "- \\( b_0 \\) is the intercept (the value of \\( Y \\) when all \\( X_i \\) are zero).\n",
        "- \\( b_1, b_2, \\ldots, b_n \\) are the coefficients (slopes) for the independent variables.\n",
        "- \\( X_1, X_2, \\ldots, X_n \\) are the independent variables.\n",
        "\n",
        "### Purpose\n",
        "The purpose of Multiple Linear Regression is to understand the influence of multiple factors on a single outcome and to predict the dependent variable based on the values of multiple independent variables.\n",
        "\n",
        "### Assumptions\n",
        "Just like Simple Linear Regression, Multiple Linear Regression has several key assumptions:\n",
        "- **Linearity**: The relationship between each independent variable and the dependent variable is linear.\n",
        "- **Independence**: The observations are independent of each other.\n",
        "- **Homoscedasticity**: The residuals have constant variance.\n",
        "- **Normality of Residuals**: The residuals are normally distributed.\n",
        "- **No Multicollinearity**: The independent variables should not be too highly correlated with each other.\n",
        "\n",
        "### Example\n",
        "Imagine you want to predict a house's price based on its size (square footage), number of bedrooms, and age. Your model might look like this:\n",
        "$$ \\text{Price} = b_0 + b_1(\\text{Size}) + b_2(\\text{Bedrooms}) + b_3(\\text{Age}) $$\n",
        "\n",
        "Where:\n",
        "- \\( b_1 \\) might represent the effect of size on the price.\n",
        "- \\( b_2 \\) might represent the effect of the number of bedrooms on the price.\n",
        "- \\( b_3 \\) might represent the effect of age on the price."
      ],
      "metadata": {
        "id": "qvdTnKEXzMlc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. What is the main difference between Simple and Multiple Linear Regression\n",
        "\n",
        "\n",
        "The main difference between Simple and Multiple Linear Regression lies in the number of independent variables used to predict the dependent variable.\n",
        "\n",
        "**Simple Linear Regression**:\n",
        "- Uses a single independent variable to predict the dependent variable.\n",
        "- The relationship is modeled with the equation: \\( Y = mX + c \\)\n",
        "- Example: Predicting house prices based on square footage alone.\n",
        "\n",
        "**Multiple Linear Regression**:\n",
        "- Uses two or more independent variables to predict the dependent variable.\n",
        "- The relationship is modeled with the equation: \\( Y = b_0 + b_1X_1 + b_2X_2 + \\ldots + b_nX_n \\)\n",
        "- Example: Predicting house prices based on square footage, number of bedrooms, and age of the house.\n",
        "\n"
      ],
      "metadata": {
        "id": "hXwRyJ-MzPXM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10.   What are the key assumptions of Multiple Linear Regression\n",
        "\n",
        "Multiple Linear Regression shares several key assumptions with Simple Linear Regression, but with additional considerations given the complexity of the model. Here are the key assumptions:\n",
        "\n",
        "1. **Linearity**: The relationship between each independent variable and the dependent variable is linear. This means that the change in the dependent variable is proportional to the change in each independent variable.\n",
        "\n",
        "2. **Independence**: The observations are independent of each other. This means that the value of the dependent variable for one observation does not influence the value for another observation.\n",
        "\n",
        "3. **Homoscedasticity**: The residuals (the differences between the observed and predicted values) have constant variance across all levels of the independent variables. This means that the spread of the residuals is similar for all values of the independent variables.\n",
        "\n",
        "4. **Normality of Residuals**: The residuals are normally distributed. This assumption is important for making valid statistical inferences and hypothesis testing.\n",
        "\n",
        "5. **No Multicollinearity**: The independent variables should not be too highly correlated with each other. High multicollinearity can make it difficult to determine the individual effect of each independent variable on the dependent variable.\n",
        "\n",
        "6. **No Autocorrelation**: The residuals should not be correlated with each other. This is especially important in time series data, where autocorrelation can be an issue.\n",
        "\n",
        "7. **Model Specification**: The model is correctly specified, meaning that all relevant variables are included, and no irrelevant variables are included. Omitting relevant variables or including irrelevant ones can lead to biased and inconsistent estimates."
      ],
      "metadata": {
        "id": "BadSqlLuzTTb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model\n",
        "\n",
        "Heteroscedasticity refers to a condition in which the variance of the residuals (the differences between the observed and predicted values) is not constant across all levels of the independent variables. In simpler terms, it means that the spread or \"scatter\" of the residuals varies at different levels of the independent variables.\n",
        "\n",
        "### Effects of Heteroscedasticity on Multiple Linear Regression:\n",
        "1. **Inefficiency of Estimates**: When heteroscedasticity is present, the estimates of the regression coefficients may still be unbiased, but they are no longer the most efficient (minimum variance) estimates. This means that the standard errors of the coefficients can be incorrect, leading to unreliable hypothesis tests and confidence intervals.\n",
        "\n",
        "2. **Biased Standard Errors**: Heteroscedasticity can lead to biased standard errors of the regression coefficients. This, in turn, affects the results of hypothesis tests (such as t-tests) and the construction of confidence intervals, potentially leading to incorrect conclusions.\n",
        "\n",
        "3. **Invalid Inferences**: Due to biased standard errors, the p-values associated with the regression coefficients may be inaccurate, leading to invalid inferences about the significance of the independent variables.\n",
        "\n",
        "### Detecting Heteroscedasticity:\n",
        "There are several methods to detect heteroscedasticity, including:\n",
        "- **Residual Plots**: Plotting the residuals against the fitted values or an independent variable can help visualize heteroscedasticity. If the spread of residuals increases or decreases systematically with the fitted values, heteroscedasticity is present.\n",
        "- **Breusch-Pagan Test**: A formal statistical test that assesses the presence of heteroscedasticity by examining whether the residuals' variance is related to the independent variables.\n",
        "- **White Test**: Another formal test that checks for heteroscedasticity by examining the residuals and their squared values.\n",
        "\n",
        "### Remedies for Heteroscedasticity:\n",
        "If heteroscedasticity is detected, several approaches can be used to address it:\n",
        "- **Transforming Variables**: Applying transformations to the dependent or independent variables (e.g., log transformation) can help stabilize the variance of the residuals.\n",
        "- **Weighted Least Squares (WLS)**: This method assigns weights to the observations based on the inverse of the variance of the residuals, giving less weight to observations with higher variance.\n",
        "- **Robust Standard Errors**: Using robust standard errors can help mitigate the impact of heteroscedasticity on hypothesis tests and confidence intervals, providing more reliable inferences."
      ],
      "metadata": {
        "id": "nvgmPqJr0UxC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 12. How can you improve a Multiple Linear Regression model with high multicollinearity\n",
        "\n",
        "High multicollinearity in a Multiple Linear Regression model can make it difficult to determine the individual effect of each independent variable on the dependent variable. It can also lead to unstable estimates of the regression coefficients. Here are some methods to address and improve a model with high multicollinearity:\n",
        "\n",
        "1. **Remove Highly Correlated Variables**: Identify and remove one or more of the highly correlated independent variables. This can help reduce multicollinearity and make the model more interpretable. You can use correlation matrices or Variance Inflation Factor (VIF) to identify highly correlated variables.\n",
        "\n",
        "2. **Combine Variables**: If two or more variables are highly correlated, consider combining them into a single composite variable. For example, if you have two variables measuring similar aspects, you might take their average or create an index.\n",
        "\n",
        "3. **Principal Component Analysis (PCA)**: PCA is a dimensionality reduction technique that transforms the original correlated variables into a smaller set of uncorrelated variables (principal components). These components can then be used as predictors in the regression model.\n",
        "\n",
        "4. **Ridge Regression**: Ridge regression (L2 regularization) is a technique that adds a penalty term to the regression equation. This penalty term shrinks the regression coefficients towards zero, reducing the impact of multicollinearity. While it doesn't eliminate multicollinearity, it can help stabilize the coefficient estimates.\n",
        "\n",
        "5. **Lasso Regression**: Lasso regression (L1 regularization) is another regularization technique that adds a penalty term to the regression equation. It can shrink some coefficients to exactly zero, effectively performing variable selection and reducing multicollinearity.\n",
        "\n",
        "6. **Data Collection**: If possible, collect more data. Increasing the sample size can sometimes help mitigate the effects of multicollinearity, although it may not always be feasible.\n",
        "\n",
        "7. **Feature Selection**: Use statistical techniques such as stepwise regression, forward selection, or backward elimination to select a subset of variables that are most important for predicting the dependent variable. This can help reduce multicollinearity by excluding less important variables.\n",
        "\n",
        "8. **Standardize Variables**: Standardizing (scaling) the independent variables can sometimes help address multicollinearity by making the variables comparable and reducing the influence of outliers."
      ],
      "metadata": {
        "id": "lZaogyT60ckd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 13.  What are some common techniques for transforming categorical variables for use in regression models\n",
        "\n",
        "When dealing with categorical variables in regression models, it's important to transform them into a numerical format that the model can understand. Here are some common techniques for transforming categorical variables:\n",
        "\n",
        "1. **One-Hot Encoding**:\n",
        "   - Converts each category into a separate binary variable (dummy variable).\n",
        "   - For example, if you have a categorical variable \"Color\" with values \"Red,\" \"Blue,\" and \"Green,\" one-hot encoding will create three binary variables: \"Color_Red,\" \"Color_Blue,\" and \"Color_Green.\"\n",
        "   - This approach is suitable when there is no ordinal relationship between categories.\n",
        "\n",
        "2. **Label Encoding**:\n",
        "   - Assigns a unique integer value to each category.\n",
        "   - For example, \"Red\" = 1, \"Blue\" = 2, \"Green\" = 3.\n",
        "   - This method is simple and efficient but can introduce unintended ordinal relationships between categories.\n",
        "\n",
        "3. **Ordinal Encoding**:\n",
        "   - Similar to label encoding but specifically used when categories have a natural order.\n",
        "   - For example, if you have an \"Education Level\" variable with values \"High School,\" \"Bachelor's,\" and \"Master's,\" you can encode them as 1, 2, and 3, respectively.\n",
        "   - This approach maintains the ordinal relationship between categories.\n",
        "\n",
        "4. **Frequency Encoding**:\n",
        "   - Replaces each category with its frequency in the dataset.\n",
        "   - For example, if \"Red\" appears 50 times, \"Blue\" appears 30 times, and \"Green\" appears 20 times, the encoded values will be 50, 30, and 20, respectively.\n",
        "   - This method can be useful when the frequency of categories carries important information.\n",
        "\n",
        "5. **Target Encoding**:\n",
        "   - Replaces each category with the mean of the target variable for that category.\n",
        "   - For example, if the target variable is \"Price,\" you can replace each category with the average \"Price\" for that category.\n",
        "   - This method can be powerful but may require techniques like cross-validation to avoid overfitting.\n",
        "\n",
        "6. **Binary Encoding**:\n",
        "   - Combines the properties of label encoding and one-hot encoding.\n",
        "   - Converts the integer-encoded labels into binary numbers and splits the digits into separate columns.\n",
        "   - This approach can be more efficient for high-cardinality categorical variables.\n",
        "\n",
        "7. **Mean Encoding**:\n",
        "   - Similar to target encoding but uses the mean of the target variable for each category.\n",
        "   - For example, if the target variable is \"Sales,\" you can replace each category with the average \"Sales\" for that category."
      ],
      "metadata": {
        "id": "NisxA2W91gVs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 14. What is the role of interaction terms in Multiple Linear Regression\n",
        "\n",
        "Interaction terms in Multiple Linear Regression are used to capture the combined effect of two or more independent variables on the dependent variable. They help us understand how the relationship between one independent variable and the dependent variable changes depending on the level of another independent variable. This can be particularly useful when the effect of one variable is not consistent across all levels of another variable.\n",
        "\n",
        "### How Interaction Terms Work\n",
        "An interaction term is created by multiplying two or more independent variables together. The resulting product is then included as an additional independent variable in the regression model. The general form of a Multiple Linear Regression equation with interaction terms is:\n",
        "\n",
        "\\[ Y = b_0 + b_1X_1 + b_2X_2 + b_3(X_1 \\times X_2) + \\ldots + b_nX_n \\]\n",
        "\n",
        "Where:\n",
        "- \\( Y \\) is the dependent variable.\n",
        "- \\( b_0 \\) is the intercept.\n",
        "- \\( b_1, b_2, \\ldots, b_n \\) are the coefficients for the independent variables.\n",
        "- \\( X_1, X_2, \\ldots, X_n \\) are the independent variables.\n",
        "- \\( b_3 \\) is the coefficient for the interaction term \\( (X_1 \\times X_2) \\).\n",
        "\n",
        "### Purpose of Interaction Terms\n",
        "1. **Capture Combined Effects**: Interaction terms allow us to model the combined effects of independent variables on the dependent variable. This is important when the effect of one variable depends on the level of another variable.\n",
        "\n",
        "2. **Improve Model Fit**: By including interaction terms, we can often improve the fit of the regression model to the data, leading to more accurate predictions and better understanding of the relationships between variables.\n",
        "\n",
        "3. **Identify Synergistic Relationships**: Interaction terms help identify synergistic relationships, where the combined effect of two variables is greater (or smaller) than the sum of their individual effects.\n",
        "\n",
        "### Example\n",
        "Consider a scenario where you want to predict employee performance (\\( Y \\)) based on hours of training (\\( X_1 \\)) and years of experience (\\( X_2 \\)). If you suspect that the effect of training on performance might be different for employees with varying levels of experience, you can include an interaction term:\n",
        "\n",
        "\\[ \\text{Performance} = b_0 + b_1(\\text{Training}) + b_2(\\text{Experience}) + b_3(\\text{Training} \\times \\text{Experience}) \\]\n",
        "\n",
        "In this model:\n",
        "- \\( b_1 \\) represents the effect of training on performance when experience is zero.\n",
        "- \\( b_2 \\) represents the effect of experience on performance when training is zero.\n",
        "- \\( b_3 \\) represents the combined effect of training and experience on performance."
      ],
      "metadata": {
        "id": "O9oF6zGB1t_S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 15.  How can the interpretation of intercept differ between Simple and Multiple Linear Regression\n",
        "\n",
        "The interpretation of the intercept in regression models can differ significantly between Simple and Multiple Linear Regression due to the complexity and number of independent variables involved. Here’s how:\n",
        "\n",
        "### Simple Linear Regression:\n",
        "- **Intercept (\\(c\\))**: In a Simple Linear Regression model \\(Y = mX + c\\), the intercept \\(c\\) represents the predicted value of the dependent variable \\(Y\\) when the independent variable \\(X\\) is zero. Essentially, it’s the baseline value of \\(Y\\) in the absence of \\(X\\).\n",
        "- **Example**: If you're predicting house prices based on square footage, the intercept might represent the baseline price of a house with zero square footage (theoretically).\n",
        "\n",
        "### Multiple Linear Regression:\n",
        "- **Intercept (\\(b_0\\))**: In a Multiple Linear Regression model \\(Y = b_0 + b_1X_1 + b_2X_2 + \\ldots + b_nX_n\\), the intercept \\(b_0\\) represents the predicted value of the dependent variable \\(Y\\) when all independent variables (\\(X_1, X_2, \\ldots, X_n\\)) are zero. This means it’s the baseline value of \\(Y\\) in the absence of all independent variables.\n",
        "- **Example**: If you're predicting house prices based on square footage, number of bedrooms, and age of the house, the intercept \\(b_0\\) represents the baseline price of a house with zero square footage, zero bedrooms, and zero age (a theoretical scenario).\n",
        "\n",
        "### Key Differences:\n",
        "1. **Context of Zero Values**:\n",
        "   - In Simple Linear Regression, the zero value context is straightforward and usually more interpretable.\n",
        "   - In Multiple Linear Regression, the zero value context can be more complex and sometimes unrealistic (e.g., a house with zero square footage, zero bedrooms, and zero age).\n",
        "\n",
        "2. **Baseline Understanding**:\n",
        "   - In Simple Linear Regression, the intercept gives a direct baseline understanding of the dependent variable without the influence of the single independent variable.\n",
        "   - In Multiple Linear Regression, the intercept provides a baseline that considers the absence of multiple factors, which can make it harder to interpret.\n",
        "\n",
        "3. **Influence of Variables**:\n",
        "   - In Simple Linear Regression, the intercept is influenced by just one independent variable.\n",
        "   - In Multiple Linear Regression, the intercept is influenced by multiple independent variables, and their combined absence defines the intercept's value."
      ],
      "metadata": {
        "id": "FSAmEv5U14uT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  16.  What is the significance of the slope in regression analysis, and how does it affect predictions\n",
        "\n",
        "The slope in regression analysis is a critical component that tells us about the relationship between the independent variable(s) and the dependent variable. Its significance and impact on predictions are fundamental to understanding how changes in one variable influence the other. Here's a detailed look at its role:\n",
        "\n",
        "### Significance of the Slope:\n",
        "1. **Rate of Change**: The slope indicates the rate at which the dependent variable \\( Y \\) changes with respect to the independent variable \\( X \\). In a simple linear regression equation \\( Y = mX + c \\), \\( m \\) is the slope.\n",
        "   - A positive slope means that as \\( X \\) increases, \\( Y \\) also increases.\n",
        "   - A negative slope means that as \\( X \\) increases, \\( Y \\) decreases.\n",
        "   - A zero slope means that changes in \\( X \\) have no effect on \\( Y \\).\n",
        "\n",
        "2. **Direction of Relationship**: The sign of the slope (positive or negative) indicates the direction of the relationship between the variables.\n",
        "   - Positive slope: Direct relationship (both variables move in the same direction).\n",
        "   - Negative slope: Inverse relationship (variables move in opposite directions).\n",
        "\n",
        "3. **Strength of Relationship**: The magnitude of the slope (how steep it is) reflects the strength of the relationship.\n",
        "   - A steeper slope (higher magnitude) indicates a stronger relationship.\n",
        "   - A gentler slope (lower magnitude) indicates a weaker relationship.\n",
        "\n",
        "### Impact on Predictions:\n",
        "1. **Predictive Power**: The slope allows us to make predictions about the dependent variable based on the values of the independent variable(s). It quantifies the expected change in \\( Y \\) for a one-unit change in \\( X \\).\n",
        "   - For example, if the slope \\( m \\) is 2, then for every one-unit increase in \\( X \\), \\( Y \\) is expected to increase by 2 units.\n",
        "\n",
        "2. **Accuracy of the Model**: A more accurate slope leads to more reliable predictions. If the slope is calculated correctly and the model assumptions hold, the predictions will closely reflect the actual values.\n",
        "\n",
        "3. **Interpretation of Results**: Understanding the slope helps in interpreting the results of the regression analysis. It provides insights into the relationship between variables, which can inform decision-making and strategy.\n",
        "\n",
        "### Example:\n",
        "Imagine you're using regression analysis to predict the sales revenue (\\( Y \\)) based on advertising spend (\\( X \\)). If the slope \\( m \\) is 5, it means that for every additional unit of advertising spend, the sales revenue is expected to increase by 5 units."
      ],
      "metadata": {
        "id": "k1tpBV6m2KYT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 17.  What are the limitations of using R² as a sole measure of model performance\n",
        "\n",
        "While \\( R^2 \\) is a useful metric in regression analysis, it has several limitations when used as the sole measure of model performance. Here are some key limitations:\n",
        "\n",
        "1. **Doesn't Account for Overfitting**: \\( R^2 \\) can increase as more variables are added to the model, even if those variables have no real predictive power. This can lead to overfitting, where the model performs well on the training data but poorly on new, unseen data.\n",
        "\n",
        "2. **Doesn't Indicate Causation**: A high \\( R^2 \\) value indicates a strong association between the independent and dependent variables, but it doesn't imply causation. The relationship could be due to confounding factors or coincidence.\n",
        "\n",
        "3. **Not Suitable for Non-Linear Models**: \\( R^2 \\) assumes a linear relationship between the independent and dependent variables. For non-linear models, \\( R^2 \\) may not accurately reflect the model's performance.\n",
        "\n",
        "4. **Ignores Bias**: \\( R^2 \\) focuses on the proportion of variance explained by the model but doesn't account for bias in the model's predictions. A model with high \\( R^2 \\) could still be systematically biased.\n",
        "\n",
        "5. **Insensitive to Scale**: \\( R^2 \\) is a relative measure and doesn't provide information about the absolute accuracy of the model's predictions. For example, a model with a high \\( R^2 \\) could still have large residuals.\n",
        "\n",
        "6. **Doesn't Consider Complexity**: \\( R^2 \\) doesn't penalize model complexity. More complex models may fit the training data well (high \\( R^2 \\)) but may not generalize well to new data. Metrics like Adjusted \\( R^2 \\) and AIC (Akaike Information Criterion) can help address this issue.\n",
        "\n",
        "7. **Limited Interpretability**: In the context of Multiple Linear Regression, interpreting \\( R^2 \\) becomes more complicated as it reflects the combined explanatory power of all independent variables, making it harder to assess the contribution of individual variables.\n",
        "\n",
        "Given these limitations, it's essential to use \\( R^2 \\) in conjunction with other performance metrics and validation techniques to get a comprehensive understanding of the model's performance. Some alternative metrics include:\n",
        "- Adjusted \\( R^2 \\)\n",
        "- Root Mean Squared Error (RMSE)\n",
        "- Mean Absolute Error (MAE)\n",
        "- AIC (Akaike Information Criterion) and BIC (Bayesian Information Criterion)\n",
        "- Cross-validation scores\n"
      ],
      "metadata": {
        "id": "GQwYEGbe2Yz5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 18 .  How would you interpret a large standard error for a regression coefficient\n",
        "\n",
        "A large standard error for a regression coefficient indicates that there is considerable variability or uncertainty in the estimate of that coefficient. Here's how to interpret this:\n",
        "\n",
        "1. **High Variability**: A large standard error suggests that the regression coefficient is not estimated precisely. The estimated value of the coefficient may vary significantly from sample to sample.\n",
        "\n",
        "2. **Less Reliable**: The coefficient with a large standard error is less reliable. It means that the true relationship between the independent variable and the dependent variable is not clearly defined by the data.\n",
        "\n",
        "3. **Wider Confidence Interval**: A large standard error results in a wider confidence interval for the coefficient. This means that we are less certain about the true value of the coefficient.\n",
        "\n",
        "4. **Lower Statistical Significance**: A larger standard error usually leads to a higher p-value, making it harder to reject the null hypothesis that the coefficient is equal to zero. This means the independent variable may not be a significant predictor of the dependent variable.\n",
        "\n",
        "5. **Possible Multicollinearity**: In multiple regression models, a large standard error might indicate multicollinearity, where independent variables are highly correlated with each other, making it difficult to isolate the individual effect of each variable.\n",
        "\n",
        "### Example Interpretation:\n",
        "Suppose you have a regression model predicting house prices based on the number of bedrooms, square footage, and age of the house. If the standard error for the coefficient of square footage is large, it means that the estimated impact of square footage on house prices is uncertain. This might be due to high variability in the data or possible multicollinearity with other predictors like the number of bedrooms."
      ],
      "metadata": {
        "id": "ItSRYUMr2iJw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 19 .  How would you interpret a large standard error for a regression coefficient\n",
        "\n",
        "A large standard error for a regression coefficient indicates that there is considerable variability or uncertainty in the estimate of that coefficient. Here's how to interpret this:\n",
        "\n",
        "1. **High Variability**: A large standard error suggests that the regression coefficient is not estimated precisely. The estimated value of the coefficient may vary significantly from sample to sample.\n",
        "\n",
        "2. **Less Reliable**: The coefficient with a large standard error is less reliable. It means that the true relationship between the independent variable and the dependent variable is not clearly defined by the data.\n",
        "\n",
        "3. **Wider Confidence Interval**: A large standard error results in a wider confidence interval for the coefficient. This means that we are less certain about the true value of the coefficient.\n",
        "\n",
        "4. **Lower Statistical Significance**: A larger standard error usually leads to a higher p-value, making it harder to reject the null hypothesis that the coefficient is equal to zero. This means the independent variable may not be a significant predictor of the dependent variable.\n",
        "\n",
        "5. **Possible Multicollinearity**: In multiple regression models, a large standard error might indicate multicollinearity, where independent variables are highly correlated with each other, making it difficult to isolate the individual effect of each variable.\n",
        "\n",
        "### Example Interpretation:\n",
        "Suppose you have a regression model predicting house prices based on the number of bedrooms, square footage, and age of the house. If the standard error for the coefficient of square footage is large, it means that the estimated impact of square footage on house prices is uncertain. This might be due to high variability in the data or possible multicollinearity with other predictors like the number of bedrooms.\n",
        "\n"
      ],
      "metadata": {
        "id": "Hy3LaSCz2444"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 20.  What is polynomial regression\n",
        "Polynomial Regression is a form of regression analysis where the relationship between the independent variable \\( X \\) and the dependent variable \\( Y \\) is modeled as an \\( n \\)th-degree polynomial. Unlike Simple Linear Regression, which fits a straight line to the data, Polynomial Regression fits a curve to capture the non-linear relationship between the variables.\n",
        "\n",
        "### Basics\n",
        "The general form of a Polynomial Regression equation is:\n",
        "$$ Y = b_0 + b_1X + b_2X^2 + b_3X^3 + \\ldots + b_nX^n $$\n",
        "\n",
        "Where:\n",
        "- \\( Y \\) is the dependent variable.\n",
        "- \\( b_0 \\) is the intercept.\n",
        "- \\( b_1, b_2, \\ldots, b_n \\) are the coefficients of the polynomial terms.\n",
        "- \\( X \\) is the independent variable.\n",
        "- \\( n \\) is the degree of the polynomial.\n",
        "\n",
        "### Purpose\n",
        "Polynomial Regression is used when the data shows a curvilinear relationship rather than a linear one. By including polynomial terms (e.g., \\( X^2, X^3 \\)), the model can fit more complex patterns in the data.\n",
        "\n",
        "### Example\n",
        "Imagine you're trying to model the growth of a plant over time. If the growth rate changes over time, a linear model might not fit well. Instead, you can use a polynomial model to capture the varying growth rate:\n",
        "$$ \\text{Growth} = b_0 + b_1(\\text{Time}) + b_2(\\text{Time}^2) $$\n",
        "\n",
        "### Advantages\n",
        "- **Flexibility**: Polynomial Regression can fit a wide range of curves, making it more flexible than linear models.\n",
        "- **Better Fit**: It can provide a better fit to data that shows a non-linear trend.\n",
        "\n",
        "### Disadvantages\n",
        "- **Overfitting**: Higher-degree polynomials can lead to overfitting, where the model fits the noise in the data rather than the underlying trend.\n",
        "- **Interpretability**: The coefficients of higher-degree polynomials can be harder to interpret.\n",
        "\n",
        "### Visual Example\n",
        "Imagine you have a dataset with a non-linear relationship. A polynomial regression model can fit a smooth curve through the data points, capturing the underlying pattern more accurately than a straight line."
      ],
      "metadata": {
        "id": "a3TTHTf73Dkk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 21.  When is polynomial regression used\n",
        "Polynomial Regression is used when the relationship between the independent variable \\( X \\) and the dependent variable \\( Y \\) is non-linear, but can be approximated by a polynomial function. Here are some common scenarios where Polynomial Regression is particularly useful:\n",
        "\n",
        "1. **Curvilinear Relationships**: When the data shows a curved trend rather than a straight-line relationship, Polynomial Regression can capture the curvature. For example, if you're modeling the growth of a population over time, where the growth rate accelerates or decelerates, a polynomial model may fit better.\n",
        "\n",
        "2. **Complex Patterns**: When the data exhibits more complex patterns that a straight line cannot capture. For instance, in economics, the relationship between supply and demand may not be linear, and a polynomial model can better capture these dynamics.\n",
        "\n",
        "3. **Higher-Order Trends**: When you need to account for higher-order trends, such as quadratic or cubic relationships. For example, in physics, the trajectory of an object under the influence of gravity is a parabolic curve, which can be modeled using a quadratic polynomial.\n",
        "\n",
        "4. **Smoothing Non-Linear Relationships**: In scenarios where you want to smooth out fluctuations in the data while preserving the overall trend, polynomial regression can provide a smoothed curve that captures the main trend without being overly sensitive to noise.\n",
        "\n",
        "5. **Modeling Interactions**: When interactions between variables create a non-linear effect. For example, the combined effect of temperature and humidity on crop yield might be better captured by a polynomial model.\n",
        "\n",
        "### Example Application\n",
        "Consider a scenario where you're studying the effect of temperature on the yield of a crop. If the relationship is not linear (e.g., yield increases with temperature up to a certain point and then decreases), a linear model might not fit well. Instead, a polynomial model such as \\( Y = b_0 + b_1X + b_2X^2 \\) can capture the non-linear relationship more accurately.\n",
        "\n",
        "### Advantages\n",
        "- **Flexibility**: Can model a wide range of curves.\n",
        "- **Better Fit**: Provides a better fit for data that shows non-linear trends.\n",
        "\n",
        "### Disadvantages\n",
        "- **Overfitting**: Higher-degree polynomials can lead to overfitting, where the model captures noise rather than the true underlying pattern.\n",
        "- **Interpretability**: Coefficients of higher-degree terms can be difficult to interpret.\n"
      ],
      "metadata": {
        "id": "XPLScdR_3Nqc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 22.  How does the intercept in a regression model provide context for the relationship between variables\n",
        "\n",
        "\n",
        "The intercept in a regression model plays a crucial role in providing context for the relationship between the independent and dependent variables. Here’s how it does this:\n",
        "\n",
        "### Baseline Value\n",
        "The intercept represents the predicted value of the dependent variable when all the independent variables are set to zero. In the equation:\n",
        "\n",
        "- **Simple Linear Regression**: \\( Y = mX + c \\)\n",
        "- **Multiple Linear Regression**: \\( Y = b_0 + b_1X_1 + b_2X_2 + \\ldots + b_nX_n \\)\n",
        "\n",
        "In both cases, the intercept (\\( c \\) or \\( b_0 \\)) is the baseline value of \\( Y \\) when the effects of \\( X \\) or \\( X_1, X_2, \\ldots, X_n \\) are not present.\n",
        "\n",
        "### Contextual Insight\n",
        "1. **Starting Point**: The intercept gives a starting point for predictions. For instance, in a model predicting house prices, the intercept might represent the base price of a house with zero square footage (theoretically).\n",
        "2. **Reference Level**: In multiple regression models, the intercept can represent the expected value of the dependent variable for a reference level of the categorical variables when all other variables are zero.\n",
        "\n",
        "### Example\n",
        "Imagine you are using a regression model to predict the salary of employees based on years of experience and education level. The model might look like this:\n",
        "\\[ \\text{Salary} = b_0 + b_1(\\text{Experience}) + b_2(\\text{Education}) \\]\n",
        "\n",
        "Here:\n",
        "- The intercept \\( b_0 \\) represents the predicted salary for an employee with zero years of experience and at the baseline education level.\n",
        "\n",
        "### Importance of Interpretation\n",
        "1. **Realism**: In some cases, interpreting the intercept directly may not be meaningful if a zero value for the independent variables is unrealistic (e.g., zero years of experience).\n",
        "2. **Model Understanding**: It helps in understanding the complete regression equation, providing a foundation upon which the effects of other variables are built."
      ],
      "metadata": {
        "id": "x8DwbqYa3aYw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 23.  How does the intercept in a regression model provide context for the relationship between variables\n",
        "\n",
        "The intercept in a regression model is a fundamental component that provides valuable context for understanding the relationship between the independent and dependent variables. Here's how it does this:\n",
        "\n",
        "### Baseline Value\n",
        "The intercept represents the predicted value of the dependent variable when all the independent variables are set to zero. This value serves as the starting point for the regression equation.\n",
        "\n",
        "### Contextual Insight\n",
        "1. **Starting Point**: The intercept gives the baseline value of the dependent variable in the absence of the independent variables. For example, in a model predicting house prices, the intercept might represent the base price of a house with zero square footage (theoretically).\n",
        "2. **Reference Level**: In models with categorical variables, the intercept represents the expected value of the dependent variable for the reference category when all other variables are zero.\n",
        "\n",
        "### Example\n",
        "Consider a regression model predicting salary based on years of experience and education level:\n",
        "\\[ \\text{Salary} = b_0 + b_1(\\text{Experience}) + b_2(\\text{Education}) \\]\n",
        "Here, \\( b_0 \\) (the intercept) represents the predicted salary for an employee with zero years of experience at the baseline education level.\n",
        "\n",
        "### Importance of Interpretation\n",
        "1. **Realism**: In some cases, a zero value for the independent variables may not be realistic (e.g., zero years of experience), but the intercept still provides a theoretical baseline.\n",
        "2. **Model Understanding**: The intercept helps in understanding the complete regression equation and sets the foundation for interpreting the effects of the independent variables."
      ],
      "metadata": {
        "id": "aXCQr1CS3uZ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 24.  How can heteroscedasticity be identified in residual plots, and why is it important to address it\n",
        "\n",
        "### Identifying Heteroscedasticity in Residual Plots\n",
        "Heteroscedasticity can be identified using residual plots, which plot the residuals (errors) from the regression model against the fitted values or an independent variable. Here's how to spot it:\n",
        "\n",
        "1. **Plot the Residuals**: Create a scatter plot of the residuals on the y-axis versus the fitted values or an independent variable on the x-axis.\n",
        "\n",
        "2. **Look for Patterns**:\n",
        "   - **No Heteroscedasticity**: If the residuals are randomly scattered around zero with a constant spread, it indicates homoscedasticity (constant variance of residuals).\n",
        "   - **Heteroscedasticity**: If the residuals display a pattern, such as a funnel shape (widening or narrowing as the fitted values increase) or any systematic structure, it suggests heteroscedasticity. In other words, the variance of the residuals changes with the level of the independent variable(s).\n",
        "\n",
        "### Why It’s Important to Address Heteroscedasticity\n",
        "1. **Inaccurate Standard Errors**: Heteroscedasticity can lead to biased standard errors, which affect the accuracy of hypothesis tests and confidence intervals. This means that the p-values may be incorrect, leading to potential misinterpretation of the significance of independent variables.\n",
        "\n",
        "2. **Inefficient Estimates**: While the regression coefficients themselves might remain unbiased, they are no longer efficient. This means that the estimated coefficients may have larger variances, making them less reliable.\n",
        "\n",
        "3. **Invalid Inferences**: The presence of heteroscedasticity can invalidate statistical inferences, making it difficult to draw reliable conclusions from the regression analysis.\n",
        "\n",
        "### Addressing Heteroscedasticity\n",
        "- **Transformations**: Applying transformations (e.g., log or square root) to the dependent or independent variables can stabilize the variance.\n",
        "- **Weighted Least Squares (WLS)**: This method gives different weights to observations based on the variance of the residuals, providing more accurate estimates.\n",
        "- **Robust Standard Errors**: Using robust standard errors can mitigate the impact of heteroscedasticity and provide more reliable statistical tests.\n",
        "\n",
        "### Visual Example:\n",
        "A common residual plot might look like this (conceptually):\n",
        "\n",
        "Without Heteroscedasticity:\n",
        "```\n",
        "Residuals\n",
        "   |\n",
        "5  |                   .\n",
        "   |        .                 .\n",
        "0  | .        .       .   .      .\n",
        "   |     .       .             .\n",
        "-5 |                  .\n",
        "   |-------------------------------> Fitted Values\n",
        "```\n",
        "With Heteroscedasticity:\n",
        "```\n",
        "Residuals\n",
        "   |\n",
        "5  |                    .\n",
        "   |         .                  .\n",
        "0  | .         .       .    .       .\n",
        "   |     .       .            .\n",
        "-5 |                   .\n",
        "   |-------------------------------> Fitted Values\n",
        "```\n",
        "\n",
        "Addressing heteroscedasticity is crucial for ensuring the reliability and accuracy of regression analysis results. If you have any specific questions or examples, feel free to share!"
      ],
      "metadata": {
        "id": "1CmRpWYC3zoz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 25.What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²\n",
        "\n",
        "\n",
        "If a Multiple Linear Regression model has a high \\( R^2 \\) but a low Adjusted \\( R^2 \\), it generally indicates that the model may include one or more independent variables that do not significantly contribute to explaining the variance in the dependent variable. Here's a closer look at what this means:\n",
        "\n",
        "### Understanding \\( R^2 \\) and Adjusted \\( R^2 \\):\n",
        "- **\\( R^2 \\)**: Represents the proportion of the variance in the dependent variable that is explained by the independent variables. It ranges from 0 to 1, with higher values indicating a better fit of the model to the data.\n",
        "- **Adjusted \\( R^2 \\)**: Adjusts the \\( R^2 \\) value for the number of independent variables in the model. It accounts for the degrees of freedom and is more accurate in assessing the model's explanatory power, particularly when comparing models with different numbers of predictors.\n",
        "\n",
        "### Interpretation:\n",
        "1. **Overfitting**: A high \\( R^2 \\) but low Adjusted \\( R^2 \\) can be a sign of overfitting. Overfitting occurs when the model includes too many variables, capturing noise in the data rather than the true underlying relationship. This can artificially inflate the \\( R^2 \\) value without genuinely improving the model's predictive power.\n",
        "\n",
        "2. **Redundant Predictors**: The presence of independent variables that do not add meaningful explanatory power to the model can cause the Adjusted \\( R^2 \\) to be lower. Adjusted \\( R^2 \\) penalizes the addition of variables that do not contribute significantly, ensuring that only useful predictors are included.\n",
        "\n",
        "3. **Model Complexity**: As the number of predictors increases, \\( R^2 \\) will never decrease, but Adjusted \\( R^2 \\) can decrease if the added predictors do not improve the model's fit sufficiently. Adjusted \\( R^2 \\) provides a more balanced view of the model's performance by accounting for the number of predictors and avoiding unnecessary complexity.\n",
        "\n",
        "### Example:\n",
        "Imagine you have a model predicting sales revenue based on advertising spend, number of stores, and months since product launch. If the \\( R^2 \\) is high but the Adjusted \\( R^2 \\) is low, it might suggest that one or more of these predictors do not significantly contribute to explaining sales revenue and could be removed to simplify the model.\n",
        "\n",
        "### Actions to Take:\n",
        "- **Variable Selection**: Consider removing independent variables that do not significantly contribute to the model. Techniques such as stepwise regression, forward selection, or backward elimination can help identify the most relevant predictors.\n",
        "- **Model Validation**: Use techniques like cross-validation to assess the model's performance on new data, ensuring that it generalizes well and is not overfitted.\n",
        "- **Evaluate Multicollinearity**: Check for multicollinearity among predictors, as highly correlated variables can inflate the \\( R^2 \\) value without improving model performance.\n"
      ],
      "metadata": {
        "id": "v8DJJe7O4H-3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 26.  Why is it important to scale variables in Multiple Linear Regression\n",
        "\n",
        "\n",
        "Scaling variables in Multiple Linear Regression is crucial for several reasons, particularly when dealing with features that have different units or magnitudes. Here’s why it’s important:\n",
        "\n",
        "### 1. **Improving Model Performance**\n",
        "- **Standardizing Coefficients**: Scaling ensures that all variables contribute equally to the model. Without scaling, variables with larger ranges can disproportionately influence the model's coefficients, leading to misleading results.\n",
        "- **Faster Convergence**: Optimization algorithms used in regression (like gradient descent) converge faster when the data is scaled. This means the model can be trained more efficiently, saving time and computational resources.\n",
        "\n",
        "### 2. **Handling Multicollinearity**\n",
        "- Scaling helps in detecting and managing multicollinearity (high correlation between independent variables) more effectively. When variables are on different scales, it can be challenging to identify multicollinearity, and scaling helps to standardize the data, making it easier to diagnose.\n",
        "\n",
        "### 3. **Interpreting Results**\n",
        "- **Comparability of Coefficients**: When variables are scaled, the regression coefficients can be directly compared. This comparability is essential for understanding the relative importance of each predictor in the model.\n",
        "- **Enhanced Interpretability**: Scaled data provides a clearer understanding of how changes in predictors impact the dependent variable, leading to more interpretable results.\n",
        "\n",
        "### 4. **Stability of the Model**\n",
        "- Scaling helps stabilize the model, especially in the presence of regularization techniques like Ridge (L2) and Lasso (L1) regression. These techniques penalize large coefficients, and unscaled data can lead to unstable and misleading models.\n",
        "\n",
        "### How to Scale:\n",
        "1. **Standardization (Z-score scaling)**: This method transforms the data to have a mean of 0 and a standard deviation of 1.\n",
        "   \\[\n",
        "   X_{\\text{scaled}} = \\frac{X - \\text{mean}(X)}{\\text{std}(X)}\n",
        "   \\]\n",
        "\n",
        "2. **Min-Max Scaling**: This method scales the data to a fixed range, typically [0, 1].\n",
        "   \\[\n",
        "   X_{\\text{scaled}} = \\frac{X - \\text{min}(X)}{\\text{max}(X) - \\text{min}(X)}\n",
        "   \\]\n",
        "\n",
        "3. **Robust Scaling**: This method uses the median and interquartile range, making it more robust to outliers.\n",
        "   \\[\n",
        "   X_{\\text{scaled}} = \\frac{X - \\text{median}(X)}{\\text{IQR}(X)}\n",
        "   \\]\n",
        "\n",
        "### Example:\n",
        "Imagine you’re building a model to predict house prices using features like square footage and number of rooms. Square footage might range from 500 to 5000, while the number of rooms might range from 1 to 10. Without scaling, the model could place undue importance on square footage simply because it has a larger range. Scaling these features ensures they contribute more equally to the model.\n"
      ],
      "metadata": {
        "id": "2lZOq7Ca4X4R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 27.  How does polynomial regression differ from linear regression\n",
        "Polynomial Regression and Linear Regression are both techniques used to model the relationship between a dependent variable and one or more independent variables. However, they differ in the complexity of the relationship they model. Here's a detailed comparison:\n",
        "\n",
        "### Linear Regression:\n",
        "- **Equation**: The general form is \\( Y = mX + c \\) for Simple Linear Regression or \\( Y = b_0 + b_1X_1 + b_2X_2 + \\ldots + b_nX_n \\) for Multiple Linear Regression.\n",
        "- **Relationship**: Assumes a linear relationship between the independent variable(s) and the dependent variable. This means the effect of \\( X \\) on \\( Y \\) is constant.\n",
        "- **Fit**: Fits a straight line to the data.\n",
        "- **Example**: Predicting house prices based on square footage alone, assuming a straight-line increase in price with increasing square footage.\n",
        "\n",
        "### Polynomial Regression:\n",
        "- **Equation**: The general form is \\( Y = b_0 + b_1X + b_2X^2 + b_3X^3 + \\ldots + b_nX^n \\).\n",
        "- **Relationship**: Models a non-linear relationship between the independent variable(s) and the dependent variable by including polynomial terms (e.g., \\( X^2, X^3 \\)).\n",
        "- **Fit**: Fits a curve to the data, which can capture more complex patterns and trends.\n",
        "- **Example**: Predicting house prices based on square footage, where the relationship is not a straight line but a curve, possibly indicating different rates of price increase at different levels of square footage.\n",
        "\n",
        "### Key Differences:\n",
        "1. **Flexibility**: Polynomial Regression is more flexible and can model complex, non-linear relationships, while Linear Regression is limited to linear relationships.\n",
        "2. **Curve Fitting**: Polynomial Regression can fit curves to the data, making it suitable for capturing patterns where the effect of \\( X \\) on \\( Y \\) changes at different levels of \\( X \\).\n",
        "3. **Overfitting Risk**: Polynomial Regression has a higher risk of overfitting, especially with higher-degree polynomials, as it can fit the noise in the data rather than the underlying trend. Linear Regression, being simpler, is less prone to overfitting.\n",
        "4. **Interpretability**: Linear Regression is generally easier to interpret because it models a straight-line relationship. Polynomial Regression, with its higher-degree terms, can be more challenging to interpret.\n",
        "\n",
        "### Visualization:\n",
        "To illustrate, imagine a dataset where the relationship between \\( X \\) and \\( Y \\) is non-linear:\n",
        "\n",
        "- **Linear Regression** might fit a straight line that doesn't capture the curvature.\n",
        "- **Polynomial Regression** can fit a curve that closely follows the data points, providing a better fit for non-linear trends."
      ],
      "metadata": {
        "id": "aueWDrVr4g5w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 28. How does polynomial regression differ from linear regression\n",
        "\n",
        "Polynomial Regression and Linear Regression are both methods for modeling relationships between variables, but they differ in the complexity and flexibility of the relationships they can capture. Here’s a detailed comparison:\n",
        "\n",
        "### Linear Regression\n",
        "- **Equation**: The general form is \\( Y = mX + c \\) for Simple Linear Regression, or \\( Y = b_0 + b_1X_1 + b_2X_2 + \\ldots + b_nX_n \\) for Multiple Linear Regression.\n",
        "- **Relationship**: Assumes a linear relationship between the independent variable(s) and the dependent variable. This means the effect of \\( X \\) on \\( Y \\) is constant.\n",
        "- **Fit**: Fits a straight line to the data.\n",
        "- **Example**: Predicting house prices based on square footage alone, assuming a straight-line increase in price with increasing square footage.\n",
        "\n",
        "### Polynomial Regression\n",
        "- **Equation**: The general form is \\( Y = b_0 + b_1X + b_2X^2 + b_3X^3 + \\ldots + b_nX^n \\).\n",
        "- **Relationship**: Models a non-linear relationship by including polynomial terms (e.g., \\( X^2, X^3 \\)).\n",
        "- **Fit**: Fits a curve to the data, capturing more complex patterns and trends.\n",
        "- **Example**: Predicting house prices based on square footage, where the relationship is a curve, reflecting different rates of price increase at different levels of square footage.\n",
        "\n",
        "### Key Differences\n",
        "1. **Flexibility**: Polynomial Regression is more flexible and can model non-linear relationships, while Linear Regression is limited to linear relationships.\n",
        "2. **Curve Fitting**: Polynomial Regression can fit curves to data, making it suitable for capturing patterns where the effect of \\( X \\) on \\( Y \\) changes at different levels of \\( X \\).\n",
        "3. **Overfitting Risk**: Polynomial Regression has a higher risk of overfitting, especially with higher-degree polynomials, as it can fit the noise in the data rather than the underlying trend. Linear Regression, being simpler, is less prone to overfitting.\n",
        "4. **Interpretability**: Linear Regression is generally easier to interpret because it models a straight-line relationship. Polynomial Regression, with its higher-degree terms, can be more challenging to interpret.\n",
        "\n",
        "### Visualization\n",
        "Imagine a dataset where the relationship between \\( X \\) and \\( Y \\) is non-linear:\n",
        "- **Linear Regression** might fit a straight line that doesn't capture the curvature.\n",
        "- **Polynomial Regression** can fit a curve that closely follows the data points, providing a better fit for non-linear trends.\n",
        ""
      ],
      "metadata": {
        "id": "pSjPU9qb48xD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 29.  What is the general equation for polynomial regression  Can polynomial regression be applied to multiple variables\n",
        "\n",
        "### General Equation for Polynomial Regression\n",
        "The general form of a polynomial regression equation is:\n",
        "\\[ Y = b_0 + b_1X + b_2X^2 + b_3X^3 + \\ldots + b_nX^n \\]\n",
        "\n",
        "Where:\n",
        "- \\( Y \\) is the dependent variable.\n",
        "- \\( b_0 \\) is the intercept.\n",
        "- \\( b_1, b_2, \\ldots, b_n \\) are the coefficients of the polynomial terms.\n",
        "- \\( X \\) is the independent variable.\n",
        "- \\( n \\) is the degree of the polynomial.\n",
        "\n",
        "### Polynomial Regression with Multiple Variables\n",
        "Yes, polynomial regression can be applied to multiple variables. In this case, the equation becomes more complex, as it includes polynomial terms for each of the independent variables as well as their interactions. The general form for a polynomial regression with multiple variables can be expressed as:\n",
        "\n",
        "\\[ Y = b_0 + b_1X_1 + b_2X_2 + b_3X_1^2 + b_4X_2^2 + b_5X_1X_2 + b_6X_1^3 + b_7X_2^3 + \\ldots \\]\n",
        "\n",
        "Where:\n",
        "- \\( Y \\) is the dependent variable.\n",
        "- \\( b_0 \\) is the intercept.\n",
        "- \\( b_1, b_2, \\ldots \\) are the coefficients of the polynomial terms.\n",
        "- \\( X_1, X_2, \\ldots \\) are the independent variables.\n",
        "\n",
        "### Example\n",
        "Imagine you are modeling the yield of a crop based on two factors: amount of fertilizer (\\( X_1 \\)) and amount of water (\\( X_2 \\)). The polynomial regression equation might look like this:\n",
        "\n",
        "\\[ \\text{Yield} = b_0 + b_1(\\text{Fertilizer}) + b_2(\\text{Water}) + b_3(\\text{Fertilizer}^2) + b_4(\\text{Water}^2) + b_5(\\text{Fertilizer} \\times \\text{Water}) + \\ldots \\]\n",
        "\n",
        "This allows the model to capture not only the individual effects of fertilizer and water on yield but also the interaction between them and their non-linear effects.\n",
        "\n",
        "Polynomial regression with multiple variables can capture complex relationships and interactions between predictors, making it a powerful tool for modeling non-linear data.\n",
        ""
      ],
      "metadata": {
        "id": "C9hKv3-_5dY4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 30. What are the limitations of polynomial regression\n",
        "\n",
        "While polynomial regression is a powerful tool for modeling complex, non-linear relationships, it comes with several limitations:\n",
        "\n",
        "### 1. **Overfitting**\n",
        "- Polynomial regression, especially with higher-degree polynomials, can fit the training data very closely. This can lead to overfitting, where the model captures noise and fluctuations in the data rather than the underlying trend. An overfitted model performs well on the training data but poorly on unseen data.\n",
        "\n",
        "### 2. **Extrapolation Issues**\n",
        "- Polynomial models can behave unpredictably outside the range of the data used for training. The higher the degree of the polynomial, the more extreme the predictions can become when extrapolating beyond the observed data range.\n",
        "\n",
        "### 3. **Interpretability**\n",
        "- As the degree of the polynomial increases, the model becomes more complex and harder to interpret. The coefficients of higher-order terms do not have a straightforward interpretation, making it challenging to understand the relationship between the independent and dependent variables.\n",
        "\n",
        "### 4. **Computational Complexity**\n",
        "- Fitting high-degree polynomials can be computationally intensive, especially with large datasets. The complexity of the calculations increases with the degree of the polynomial, which can be a concern for models with many predictors.\n",
        "\n",
        "### 5. **Multicollinearity**\n",
        "- Polynomial regression can introduce multicollinearity, especially when including higher-order terms. Multicollinearity occurs when predictor variables are highly correlated with each other, leading to unstable coefficient estimates and making the model more sensitive to changes in the data.\n",
        "\n",
        "### 6. **Choice of Degree**\n",
        "- Selecting the appropriate degree of the polynomial is crucial but can be challenging. Too low a degree may underfit the data, missing important patterns, while too high a degree may overfit the data. Model selection criteria and cross-validation techniques are often required to find the optimal degree.\n",
        "\n",
        "### 7. **Sensitivity to Outliers**\n",
        "- Polynomial regression models can be highly sensitive to outliers. Outliers can disproportionately influence the fit of the model, leading to misleading predictions and conclusions.\n",
        "\n",
        "### 8. **Risk of Oscillation**\n",
        "- Higher-degree polynomials can exhibit oscillatory behavior, especially at the boundaries of the data. This can lead to large swings in predictions that do not align with the expected trend.\n",
        "\n",
        "### Example\n",
        "Imagine fitting a 10th-degree polynomial to a dataset that exhibits a simple quadratic relationship. While the high-degree polynomial may fit the training data very closely, it is likely to overfit, and its predictions on new data may be erratic and unreliable.\n"
      ],
      "metadata": {
        "id": "PV8rfSrV5tPj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 31.  What methods can be used to evaluate model fit when selecting the degree of a polynomial\n",
        "\n",
        "Selecting the appropriate degree of a polynomial for a regression model is crucial for balancing fit and complexity. Here are some commonly used methods to evaluate model fit when choosing the degree of a polynomial:\n",
        "\n",
        "### 1. **Visual Inspection**\n",
        "- **Residual Plots**: Plotting residuals (errors) against the fitted values helps identify patterns. Ideally, residuals should be randomly scattered without any obvious patterns.\n",
        "- **Fit to Data**: Plotting the polynomial fit against the actual data points can help visually assess how well the polynomial captures the underlying trend.\n",
        "\n",
        "### 2. **Cross-Validation**\n",
        "- **K-Fold Cross-Validation**: This method involves dividing the data into \\( k \\) subsets (folds). The model is trained on \\( k-1 \\) folds and tested on the remaining fold. This process is repeated \\( k \\) times, with each fold used as the test set once. The average performance across all folds helps evaluate model fit and generalizability.\n",
        "- **Leave-One-Out Cross-Validation (LOOCV)**: A special case of k-fold cross-validation where \\( k \\) equals the number of data points. Each data point is used as a test set once, and the model is trained on the remaining points.\n",
        "\n",
        "### 3. **Information Criteria**\n",
        "- **Akaike Information Criterion (AIC)**: Measures the goodness of fit while penalizing model complexity. Lower AIC values indicate better models, balancing fit and simplicity.\n",
        "- **Bayesian Information Criterion (BIC)**: Similar to AIC but with a stronger penalty for model complexity. Lower BIC values indicate better models.\n",
        "\n",
        "### 4. **Adjusted \\( R^2 \\)**\n",
        "- Adjusted \\( R^2 \\) adjusts the \\( R^2 \\) value based on the number of predictors and the sample size. It increases only if the added term improves the model more than would be expected by chance. Higher Adjusted \\( R^2 \\) values indicate better fit while accounting for model complexity.\n",
        "\n",
        "### 5. **Root Mean Squared Error (RMSE)**\n",
        "- RMSE measures the average magnitude of the residuals. Lower RMSE values indicate a better fit. It provides a measure of how well the model predicts the dependent variable.\n",
        "\n",
        "### 6. **Mean Absolute Error (MAE)**\n",
        "- MAE measures the average absolute difference between observed and predicted values. Lower MAE values indicate a better fit and provide an easily interpretable measure of model accuracy.\n",
        "\n",
        "### 7. **Validation Set Approach**\n",
        "- Splitting the data into training and validation sets allows for evaluating the model's performance on unseen data. Comparing the fit on the training and validation sets helps assess overfitting and generalizability.\n",
        "\n",
        "### Example\n",
        "Consider a dataset where you're modeling the growth of a plant over time. You might fit polynomials of different degrees (e.g., 1st, 2nd, 3rd) and use cross-validation to evaluate their performance. You could compare their RMSE, AIC, and Adjusted \\( R^2 \\) values to select the best degree.\n",
        "\n"
      ],
      "metadata": {
        "id": "FcFDJOLh5zUv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 32.   Why is visualization important in polynomial regression\n",
        "\n",
        "Visualization plays a crucial role in polynomial regression for several reasons:\n",
        "\n",
        "### 1. **Understanding the Relationship**\n",
        "- **Visual Representation**: Visualization helps in understanding the nature of the relationship between the independent and dependent variables. It provides an intuitive grasp of how well the polynomial model fits the data and captures the underlying trends.\n",
        "- **Curve Patterns**: By plotting the polynomial regression curve, you can see the patterns and complexities that the model captures, such as peaks, troughs, and inflection points.\n",
        "\n",
        "### 2. **Model Evaluation**\n",
        "- **Residual Plots**: Visualizing residual plots helps in assessing model fit. Patterns in residuals can indicate issues like heteroscedasticity, non-linearity, or outliers. Ideally, residuals should be randomly scattered around zero.\n",
        "- **Fit to Data**: Comparing the polynomial curve to the actual data points allows you to evaluate how well the model fits the data. A good fit should have the curve closely following the data points.\n",
        "\n",
        "### 3. **Detecting Overfitting or Underfitting**\n",
        "- **Overfitting**: Visualization can reveal overfitting, where the model captures noise rather than the underlying trend. Overfitted models have a wiggly curve that fits the training data too closely but may perform poorly on new data.\n",
        "- **Underfitting**: Visualization can also show underfitting, where the model fails to capture important patterns in the data. Underfitted models have a curve that is too simple and misses key trends.\n",
        "\n",
        "### 4. **Communication**\n",
        "- **Explaining Models**: Visualization is a powerful tool for communicating the results and insights of the polynomial regression model to others. It makes it easier for stakeholders to understand the model's behavior and predictions.\n",
        "- **Visual Comparison**: Visualizing multiple polynomial fits of different degrees can help in comparing and selecting the best model. It provides a clear way to demonstrate how increasing the degree affects the fit.\n",
        "\n",
        "### 5. **Identifying Influential Points**\n",
        "- **Outliers and Leverage Points**: Visualization helps in identifying outliers and influential points that can disproportionately affect the model. These points can be analyzed further to understand their impact and decide whether to retain or remove them.\n",
        "\n",
        "### Example\n",
        "Imagine you are modeling the growth of a plant over time with a polynomial regression model. By plotting the growth data and the fitted polynomial curve, you can see how well the model captures the growth pattern. Residual plots can further help assess if the model fits the data appropriately without overfitting or underfitting.\n",
        ""
      ],
      "metadata": {
        "id": "Tdb6Am1e5-8V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 33. How is polynomial regression implemented in Python?\n",
        "\n",
        "Implementing polynomial regression in Python involves using libraries like NumPy, pandas, and scikit-learn. Here’s a step-by-step guide to help you get started:\n",
        ""
      ],
      "metadata": {
        "id": "sUx0RDWV6Hps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Synthetic dataset\n",
        "np.random.seed(0)\n",
        "X = 2 - 3 * np.random.normal(0, 1, 100)\n",
        "Y = X - 2 * (X ** 2) + 0.5 * (X ** 3) + np.random.normal(0, 1, 100)\n",
        "\n",
        "X = X[:, np.newaxis]  # Reshape X to be a column vector\n",
        "\n",
        "# Create polynomial features\n",
        "poly = PolynomialFeatures(degree=3)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "# Train the model\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, Y)\n",
        "\n",
        "# Make predictions\n",
        "Y_pred = model.predict(X_poly)\n",
        "\n",
        "# Calculate metrics\n",
        "mse = mean_squared_error(Y, Y_pred)\n",
        "r2 = r2_score(Y, Y_pred)\n",
        "\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "print(f\"R^2 Score: {r2}\")\n",
        "\n",
        "# Plot results\n",
        "plt.scatter(X, Y, color='blue', label='Original Data')\n",
        "plt.plot(X, Y_pred, color='red', label='Polynomial Regression Fit')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mKT0fwjC5zAD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}