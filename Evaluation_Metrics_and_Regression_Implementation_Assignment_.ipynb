{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vePfchvqu3dl",
        "mH7MjXMvoA9T",
        "z2mxRicVKpPF",
        "0t11UrXLRkhW",
        "CgLAKhdYSZ9M",
        "otELcGvZV-E3",
        "TOPlUrABs6-D",
        "apQH1gdBuZOo",
        "7rNUuUCiiGuX",
        "DxE3LWKol0Mb",
        "btWoquT1np7q",
        "gbmWxpmtnvlN",
        "YOR5-lhHpVei",
        "DFWFGfjsrRud",
        "8AkFEfEnSeLl",
        "tr8SFQnvSkCe",
        "oZkYXE8eWbaC",
        "jrMoEpYcWheh",
        "9Q-7a6aZdZ7X",
        "zA7gUxcKdjbj",
        "GJl_cur1RCav",
        "X0Wh7dshmLfh",
        "FzwjzH93lOS0",
        "BN8SeZ_6lZT6",
        "AJy05PsVlyr6",
        "gLruyF1Hmj_6",
        "Ah0Cw4hNmswx"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Theoretical**"
      ],
      "metadata": {
        "id": "vePfchvqu3dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. What does R-squared represent in a regression model\n",
        "\n",
        "R-squared (R²) is a statistical measure that indicates the proportion of the variance in the dependent variable that is predictable from the independent variable(s). Essentially, it tells you how well the regression model fits the data. Here’s a bit more detail:\n",
        "\n",
        "1. Range: R-squared ranges from 0 to 1.\n",
        "2. Interpretation:\n",
        "   - An R² value of 0 means that the independent variables do not explain any of the variability in the dependent variable.\n",
        "   - An R² value of 1 means that the independent variables explain all the variability in the dependent variable.\n",
        "   - Values between 0 and 1 indicate the proportion of the variance in the dependent variable that can be explained by the model."
      ],
      "metadata": {
        "id": "e1i22mFYoPeg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. What are the assumptions of linear regression\n",
        "\n",
        "Linear regression relies on several key assumptions to ensure the validity of the model and its results. Here are the primary assumptions:\n",
        "\n",
        "1. **Linearity**: The relationship between the independent variables and the dependent variable is linear. This means that the effect of the independent variables on the dependent variable is additive.\n",
        "\n",
        "2. **Independence**: The observations are independent of each other. In other words, the value of the dependent variable for any observation is not influenced by the value of the dependent variable for any other observation.\n",
        "\n",
        "3. **Homoscedasticity**: The residuals (errors) of the regression model have constant variance across all levels of the independent variables. This means that the spread of the residuals is the same for all values of the independent variables.\n",
        "\n",
        "4. **No Perfect Multicollinearity**: The independent variables are not perfectly correlated with each other. If there is perfect multicollinearity, it means that one independent variable can be perfectly predicted from the others, making it difficult to estimate the regression coefficients.\n",
        "\n",
        "5. **Normality of Residuals**: The residuals of the regression model are normally distributed. This assumption is important for making statistical inferences, such as hypothesis testing and constructing confidence intervals."
      ],
      "metadata": {
        "id": "Clqel70FogUO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. What is the difference between R-squared and Adjusted R-squared\n",
        "\n",
        "Great question! Both R-squared (R²) and Adjusted R-squared (Adjusted R²) are metrics used to evaluate the goodness-of-fit of a regression model, but they serve slightly different purposes.\n",
        "\n",
        "**R-squared (R²)**:\n",
        "- **Definition**: R-squared measures the proportion of the variance in the dependent variable that is explained by the independent variables in the model.\n",
        "- **Range**: It ranges from 0 to 1.\n",
        "- **Limitation**: One of the main limitations of R-squared is that it can only increase or stay the same when more independent variables are added to the model, regardless of whether those variables are actually significant.\n",
        "\n",
        "**Adjusted R-squared (Adjusted R²)**:\n",
        "- **Definition**: Adjusted R-squared adjusts the R-squared value for the number of independent variables in the model. It takes into account the complexity of the model.\n",
        "- **Adjustment**: It penalizes the addition of unnecessary variables that do not improve the model. Therefore, it can increase or decrease depending on whether the new variables improve the model.\n",
        "- **Range**: Like R-squared, it also ranges from 0 to 1, but it typically provides a more accurate measure of the model's goodness-of-fit, especially when comparing models with different numbers of independent variables.\n",
        "\n",
        "To put it simply, while R-squared can sometimes be misleading due to its tendency to always increase with additional variables, Adjusted R-squared provides a more balanced view by accounting for the model's complexity and penalizing overfitting.\n",
        "\n",
        "Here's a quick summary in table form for clarity:\n",
        "\n",
        "| Metric           | Description                                                                 | Range | Behavior with More Variables          |\n",
        "|------------------|-----------------------------------------------------------------------------|-------|----------------------------------------|\n",
        "| R-squared        | Proportion of variance explained by the model                               | 0 - 1 | Always increases or stays the same     |\n",
        "| Adjusted R-squared | Adjusts R-squared for the number of predictors, penalizes overfitting      | 0 - 1 | Can increase or decrease based on model improvement |\n"
      ],
      "metadata": {
        "id": "ex89dSiyomDs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Why do we use Mean Squared Error (MSE)\n",
        "\n",
        "Mean Squared Error (MSE) is a widely-used metric for evaluating the performance of regression models. Here are the main reasons why we use MSE:\n",
        "\n",
        "1. **Penalty for Large Errors**: MSE penalizes larger errors more than smaller errors because it squares the differences between the actual and predicted values. This is useful because larger errors are typically more problematic than smaller ones.\n",
        "\n",
        "2. **Differentiability**: MSE is differentiable, making it suitable for optimization algorithms like gradient descent. This property allows us to find the minimum error by adjusting the model's parameters during training.\n",
        "\n",
        "3. **Mathematical Simplicity**: MSE is relatively simple to compute and understand. It's the average of the squared differences between actual and predicted values, providing a straightforward measure of model accuracy.\n",
        "\n",
        "4. **Convexity**: The MSE function is convex, meaning it has a single global minimum. This makes it easier to optimize compared to non-convex functions, which might have multiple local minima.\n",
        "\n",
        "5. **Interpretability**: MSE gives a clear sense of the average magnitude of prediction errors in the same units as the dependent variable (after taking the square root, if desired).\n",
        "\n",
        "To sum up, MSE provides a balanced, mathematically convenient way to assess the accuracy of regression models while placing a higher penalty on larger errors."
      ],
      "metadata": {
        "id": "p3ditBWzoz1z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. What does an Adjusted R-squared value of 0.85 indicate\n",
        "An Adjusted R-squared value of 0.85 indicates that approximately 85% of the variance in the dependent variable is explained by the independent variables in the model, while accounting for the number of predictors. This suggests that the model has a strong explanatory power and fits the data well.\n",
        "\n",
        "In practical terms, this high Adjusted R-squared value implies that the model is likely capturing most of the relevant information and providing accurate predictions. However, it's always important to consider other factors such as the residual plots, potential overfitting, and the theoretical basis of the model."
      ],
      "metadata": {
        "id": "ENvt0RsroBYz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6.  How do we check for normality of residuals in linear regression\n",
        "\n",
        "Checking for the normality of residuals is an important step in validating a linear regression model. Here are some common methods to assess the normality of residuals:\n",
        "\n",
        "1. **Histogram**: Plot a histogram of the residuals. If the residuals are normally distributed, the histogram should resemble a bell curve.\n",
        "\n",
        "2. **Q-Q Plot (Quantile-Quantile Plot)**: A Q-Q plot compares the quantiles of the residuals to the quantiles of a normal distribution. If the residuals are normally distributed, the points should lie approximately along a straight line.\n",
        "\n",
        "3. **Shapiro-Wilk Test**: This is a statistical test that evaluates the normality of residuals. A small p-value (typically less than 0.05) suggests that the residuals are not normally distributed.\n",
        "\n",
        "4. **Kolmogorov-Smirnov Test**: Another statistical test that compares the residuals' distribution to a normal distribution. Like the Shapiro-Wilk test, a small p-value indicates non-normality.\n",
        "\n",
        "5. **Skewness and Kurtosis**: Calculate the skewness and kurtosis of the residuals. Skewness close to 0 and kurtosis close to 3 indicate normality.\n",
        "\n",
        "6. **Normal Probability Plot**: Similar to the Q-Q plot, this plot shows the cumulative probability of the residuals versus a normal distribution. A straight line suggests normality.\n",
        "\n",
        "Here's a quick example of how you might visualize residuals with a histogram and a Q-Q plot using Python (assuming you have a `residuals` array):\n",
        "\n",
        "```python\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Histogram of residuals\n",
        "plt.hist(residuals, bins=30, edgecolor='k')\n",
        "plt.title('Histogram of Residuals')\n",
        "plt.xlabel('Residuals')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "# Q-Q plot of residuals\n",
        "stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
        "plt.title('Q-Q Plot of Residuals')\n",
        "plt.show()\n",
        "```"
      ],
      "metadata": {
        "id": "2OQCAApfpntq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7.  What is multicollinearity, and how does it impact regression\n",
        "\n",
        "**Multicollinearity** refers to a situation in regression analysis where two or more independent variables are highly correlated. This high correlation means that one independent variable can be linearly predicted from the others with a substantial degree of accuracy.\n",
        "\n",
        "### Impact of Multicollinearity on Regression\n",
        "\n",
        "1. **Unstable Coefficients**: Multicollinearity can make the estimates of the regression coefficients unstable and highly sensitive to small changes in the model. This can lead to large standard errors and inflated confidence intervals, making it hard to determine the true effect of each independent variable.\n",
        "\n",
        "2. **Interpretation Difficulty**: It becomes difficult to assess the individual impact of correlated variables on the dependent variable, as their effects are intertwined. This can obscure the true relationship between the predictors and the outcome.\n",
        "\n",
        "3. **Reduced Statistical Power**: The statistical power to detect significant predictors diminishes because of the overlap in the information provided by the correlated variables. This can result in some predictors appearing non-significant when they might actually be important.\n",
        "\n",
        "4. **Reduced Precision**: Multicollinearity can decrease the precision of the estimated coefficients, leading to less reliable predictions and interpretations.\n",
        "\n",
        "### Detecting Multicollinearity\n",
        "\n",
        "1. **Correlation Matrix**: Examining the pairwise correlations between independent variables can give a preliminary idea of potential multicollinearity issues.\n",
        "\n",
        "2. **Variance Inflation Factor (VIF)**: VIF quantifies how much the variance of a regression coefficient is inflated due to multicollinearity. A VIF value greater than 10 is often considered indicative of significant multicollinearity.\n",
        "\n",
        "3. **Tolerance**: Tolerance is the inverse of VIF. A low tolerance value (close to 0) indicates high multicollinearity.\n",
        "\n",
        "4. **Eigenvalues and Condition Index**: The condition index assesses the sensitivity of the estimated coefficients to changes in the data. High values indicate potential multicollinearity problems.\n",
        "\n",
        "### Managing Multicollinearity\n",
        "\n",
        "1. **Remove Highly Correlated Predictors**: If some variables are highly correlated, consider removing one or more to reduce multicollinearity.\n",
        "\n",
        "2. **Combine Predictors**: Creating composite variables or using techniques like principal component analysis (PCA) can help combine correlated predictors into a single index.\n",
        "\n",
        "3. **Regularization Techniques**: Methods like Ridge Regression and Lasso Regression can help mitigate the impact of multicollinearity by introducing penalties to the regression model."
      ],
      "metadata": {
        "id": "Qaf-mnQfpsK2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. What is Mean Absolute Error (MAE)\n",
        "\n",
        "Mean Absolute Error (MAE) is a widely-used metric for evaluating the accuracy of regression models. It measures the average magnitude of the errors between the predicted and actual values, without considering their direction. Here's a bit more detail:\n",
        "\n",
        "### Definition\n",
        "MAE is calculated by taking the average of the absolute differences between the predicted and actual values. Mathematically, it's expressed as:\n",
        "\n",
        "$$ \\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} | y_i - \\hat{y}_i | $$\n",
        "\n",
        "where:\n",
        "- \\( n \\) is the number of observations\n",
        "- \\( y_i \\) is the actual value\n",
        "- \\( \\hat{y}_i \\) is the predicted value\n",
        "\n",
        "### Characteristics\n",
        "- **Interpretability**: MAE is easy to interpret as it provides the average error in the same units as the dependent variable.\n",
        "- **Robustness**: MAE is less sensitive to outliers compared to Mean Squared Error (MSE) because it doesn't square the errors.\n",
        "- **Range**: MAE can range from 0 (perfect prediction) to ∞, with lower values indicating better model performance.\n",
        "\n",
        "### Advantages\n",
        "- **Simplicity**: MAE is simple to understand and calculate.\n",
        "- **Real-world relevance**: Since MAE uses the same units as the dependent variable, it's intuitively meaningful.\n",
        "\n",
        "### Limitations\n",
        "- **Equal weight to all errors**: MAE treats all errors equally, which might not always be desirable in situations where larger errors should be penalized more.\n",
        "\n",
        "Here's a quick summary in a table form for clarity:\n",
        "\n",
        "| Metric | Formula | Sensitivity to Outliers | Units | Interpretation |\n",
        "|--------|---------|-------------------------|-------|----------------|\n",
        "| MAE    | $$ \\frac{1}{n} \\sum | y_i - \\hat{y}_i | $$ | Less sensitive  | Same as dependent variable | Average magnitude of errors |"
      ],
      "metadata": {
        "id": "fNSMOMWSqZKO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. What are the benefits of using an ML pipeline\n",
        "\n",
        "Using a Machine Learning (ML) pipeline offers several benefits, making the process of developing, deploying, and maintaining ML models more efficient and effective. Here are some key advantages:\n",
        "\n",
        "1. **Automation**: ML pipelines automate repetitive tasks such as data preprocessing, feature engineering, model training, and evaluation. This reduces manual effort and speeds up the overall workflow.\n",
        "\n",
        "2. **Consistency**: By defining a standardized sequence of steps, ML pipelines ensure that the same process is followed every time, leading to consistent results. This helps in reducing errors and improving reproducibility.\n",
        "\n",
        "3. **Modularity**: ML pipelines are often built as modular components, allowing different stages (e.g., data preprocessing, model training) to be independently developed, tested, and reused. This makes it easier to update or swap out individual components without affecting the entire pipeline.\n",
        "\n",
        "4. **Scalability**: Pipelines can handle large volumes of data and leverage distributed computing resources to scale the training and evaluation processes. This is particularly important for big data and complex models.\n",
        "\n",
        "5. **Experimentation**: ML pipelines facilitate experimentation by enabling easy modifications and comparisons of different models, parameters, and preprocessing techniques. This accelerates the process of finding the best-performing model.\n",
        "\n",
        "6. **Maintenance and Monitoring**: Pipelines can be integrated with monitoring tools to track the performance of deployed models in real-time. This helps in identifying issues, managing model drift, and ensuring the continued accuracy of predictions.\n",
        "\n",
        "7. **Collaboration**: With a well-defined pipeline, multiple team members (e.g., data scientists, engineers) can collaborate more effectively, as each person can work on different stages of the pipeline without interfering with others.\n",
        "\n",
        "8. **Efficiency**: Automating and streamlining the ML workflow reduces the time and effort required to build and deploy models, leading to faster delivery of insights and solutions.\n",
        "\n",
        "Here’s a quick summary in a table format:\n",
        "\n",
        "| Benefit               | Description                                                          |\n",
        "|-----------------------|----------------------------------------------------------------------|\n",
        "| Automation            | Reduces manual effort and speeds up the workflow                    |\n",
        "| Consistency           | Ensures the same process is followed, leading to reproducible results|\n",
        "| Modularity            | Allows independent development and reuse of pipeline components     |\n",
        "| Scalability           | Handles large volumes of data and leverages distributed computing   |\n",
        "| Experimentation       | Facilitates easy modifications and comparisons for better models    |\n",
        "| Maintenance & Monitoring | Tracks performance and manages model drift in real-time         |\n",
        "| Collaboration         | Enhances teamwork by defining clear stages and responsibilities     |\n",
        "| Efficiency            | Streamlines the workflow, reducing time to deliver insights         |"
      ],
      "metadata": {
        "id": "vF6VQc_Mqez9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10.  Why is RMSE considered more interpretable than MSE\n",
        "\n",
        "Root Mean Squared Error (RMSE) is often considered more interpretable than Mean Squared Error (MSE) because of the following reasons:\n",
        "\n",
        "1. **Units of Measurement**: RMSE is in the same units as the dependent variable, making it more intuitively understandable. MSE, on the other hand, squares the errors, resulting in units that are the square of the dependent variable's units, which can be less intuitive.\n",
        "\n",
        "2. **Magnitude of Errors**: RMSE directly reflects the average magnitude of the errors in the same scale as the original data, making it easier to comprehend the typical prediction error. In contrast, MSE can sometimes be harder to interpret due to the squaring of errors.\n",
        "\n",
        "### Quick Comparison\n",
        "\n",
        "| Metric | Interpretation | Units |\n",
        "|--------|----------------|-------|\n",
        "| MSE    | Average of squared errors | Squared units of the dependent variable |\n",
        "| RMSE   | Square root of MSE, reflecting average error | Same units as the dependent variable |\n",
        "\n",
        "By taking the square root of MSE, RMSE provides a more intuitive measure of model accuracy, which is why it's often preferred for interpreting model performance."
      ],
      "metadata": {
        "id": "beQFqjn6q9OW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 11. What is pickling in Python, and how is it useful in ML\n",
        "\n",
        "Pickling in Python refers to the process of serializing and deserializing objects so they can be saved to a file or transferred over a network and later restored back to their original state. This process is handled by the `pickle` module.\n",
        "\n",
        "### How Pickling Works\n",
        "- **Serialization (Pickling)**: Converts a Python object into a byte stream, which can be written to a file or sent over a network.\n",
        "- **Deserialization (Unpickling)**: Converts the byte stream back into the original Python object.\n",
        "\n",
        "\n",
        "#### Benefits of Pickling in Machine Learning\n",
        "1. **Model Persistence**: You can save trained models to disk and load them later without having to retrain the model, saving time and computational resources.\n",
        "   \n",
        "2. **Data Storage**: Pickling allows you to save complex data structures (like dictionaries or custom objects) that you might use during the data preprocessing or feature engineering stages.\n",
        "\n",
        "3. **Pipeline Portability**: You can pickle entire ML pipelines, including pre-processing steps, models, and post-processing steps, ensuring that the exact same process can be replicated or shared with others.\n",
        "\n",
        "#### Use Cases in ML\n",
        "1. **Model Deployment**: Save trained models and deploy them in production environments for inference without retraining.\n",
        "   \n",
        "2. **Experimentation**: Save and reload models during experimentation to compare results without having to train the model each time.\n",
        "\n",
        "3. **Data Sharing**: Share preprocessed datasets or feature sets with team members or across different environments."
      ],
      "metadata": {
        "id": "w-3ufmiArEaw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 12.  What does a high R-squared value mean\n",
        "A high R-squared (R²) value indicates that a large proportion of the variance in the dependent variable is explained by the independent variables in the regression model. In practical terms, it means the model fits the data well. Here are some key points:\n",
        "\n",
        "1. **Good Fit**: A high R² value, close to 1, suggests that the model is capturing most of the underlying patterns in the data. This implies that the independent variables have a strong explanatory power over the dependent variable.\n",
        "\n",
        "2. **Predictive Accuracy**: A high R² value often correlates with better predictive accuracy. The model's predictions are likely to be closer to the actual values.\n",
        "\n",
        "3. **Reduced Residuals**: With a high R², the residuals (the differences between the actual and predicted values) are typically smaller, indicating that the model's estimates are closer to reality.\n",
        "\n",
        "However, it's important to be cautious:\n",
        "- **Overfitting**: A very high R² might sometimes indicate overfitting, especially if the model is too complex for the data. Overfitting occurs when the model captures noise rather than the underlying relationship.\n",
        "- **Context Matters**: The acceptable R² value can vary depending on the field of study. For example, in social sciences, an R² of 0.4 might be considered good, whereas in physical sciences, a higher threshold might be expected."
      ],
      "metadata": {
        "id": "z1Ht6ULUrRB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 13. What happens if linear regression assumptions are violated\n",
        "\n",
        "Violating the assumptions of linear regression can impact the validity and reliability of your model's results. Here’s a breakdown of potential consequences for each assumption:\n",
        "\n",
        "1. **Linearity**: If the relationship between the dependent and independent variables is not linear, the model may miss important patterns, leading to poor predictions and incorrect conclusions. Non-linear relationships can be addressed by transforming the variables or using non-linear regression techniques.\n",
        "\n",
        "2. **Independence**: If the observations are not independent (e.g., in time series data), the standard errors of the coefficients may be underestimated, leading to overly optimistic p-values and confidence intervals. This can result in incorrect inferences about the significance of predictors. Techniques like time series analysis or mixed-effects models can help address this issue.\n",
        "\n",
        "3. **Homoscedasticity**: If the residuals do not have constant variance (heteroscedasticity), the estimated coefficients may still be unbiased, but the standard errors may be incorrect. This can affect hypothesis tests and confidence intervals. Robust standard errors or weighted least squares can help mitigate this issue.\n",
        "\n",
        "4. **No Perfect Multicollinearity**: Perfect multicollinearity makes it impossible to estimate the unique contribution of each independent variable, leading to large standard errors and unstable coefficient estimates. This can be addressed by removing or combining highly correlated predictors.\n",
        "\n",
        "5. **Normality of Residuals**: If the residuals are not normally distributed, the inference based on the t-tests and F-tests may be invalid, particularly in small samples. However, in large samples, the Central Limit Theorem can mitigate this issue. Transforming the dependent variable or using bootstrapping can help address non-normality.\n",
        "\n",
        "Here’s a summary table for quick reference:\n",
        "\n",
        "| Assumption         | Consequence of Violation            | Potential Solutions                          |\n",
        "|--------------------|-------------------------------------|----------------------------------------------|\n",
        "| Linearity          | Missed patterns, poor predictions   | Transform variables, use non-linear models   |\n",
        "| Independence       | Underestimated standard errors      | Use time series or mixed-effects models      |\n",
        "| Homoscedasticity   | Incorrect standard errors, tests    | Robust standard errors, weighted least squares|\n",
        "| No Multicollinearity | Unstable coefficients, large errors| Remove/combine correlated predictors         |\n",
        "| Normality of Residuals | Invalid inference in small samples| Transform dependent variable, use bootstrapping|\n"
      ],
      "metadata": {
        "id": "BgS0p-ogro36"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 14. How can we address multicollinearity in regression\n",
        "\n",
        "Addressing multicollinearity is crucial for ensuring the reliability and interpretability of a regression model. Here are some strategies to tackle multicollinearity:\n",
        "\n",
        "1. **Remove Highly Correlated Predictors**: Identify and remove one of the highly correlated predictors. This is the simplest approach and can be effective if you can afford to lose one of the variables.\n",
        "\n",
        "2. **Combine Predictors**: Combine the correlated variables into a single predictor through methods like:\n",
        "   - **Principal Component Analysis (PCA)**: Transforms the correlated variables into a smaller set of uncorrelated components.\n",
        "   - **Factor Analysis**: Groups correlated variables into underlying factors.\n",
        "\n",
        "3. **Regularization Techniques**: Use regularization methods that can handle multicollinearity by adding a penalty to the regression model:\n",
        "   - **Ridge Regression**: Adds a penalty term proportional to the square of the coefficients.\n",
        "   - **Lasso Regression**: Adds a penalty term that can shrink some coefficients to zero, effectively performing variable selection.\n",
        "\n",
        "4. **Centering the Variables**: Center the predictors by subtracting the mean value from each predictor. This reduces multicollinearity, especially when dealing with interaction terms.\n",
        "\n",
        "5. **Variance Inflation Factor (VIF) Analysis**: Calculate the VIF for each predictor and remove or combine predictors with high VIF values. A VIF above 10 is often considered indicative of high multicollinearity.\n",
        "\n",
        "6. **Domain Knowledge**: Use your knowledge of the subject matter to prioritize which variables are most important and which ones can be excluded without losing significant information.\n",
        "\n",
        "Here’s a quick summary:\n",
        "\n",
        "| Strategy                   | Description                                                         |\n",
        "|----------------------------|---------------------------------------------------------------------|\n",
        "| Remove Predictors          | Eliminate one of the highly correlated predictors                   |\n",
        "| Combine Predictors         | Use PCA or factor analysis to create composite variables            |\n",
        "| Regularization Techniques  | Apply Ridge or Lasso regression to handle multicollinearity         |\n",
        "| Centering Variables        | Subtract the mean value from each predictor                         |\n",
        "| VIF Analysis               | Calculate and address high VIF values                               |\n",
        "| Domain Knowledge           | Use subject matter expertise to decide on variable importance       |\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "jC63_7MesYNg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 15.  How can feature selection improve model performance in regression analysis\n",
        "Feature selection plays a crucial role in enhancing the performance of regression models. Here are some key benefits:\n",
        "\n",
        "1. **Reduces Overfitting**: By removing irrelevant or redundant features, feature selection helps reduce the risk of overfitting. This ensures that the model generalizes better to new, unseen data.\n",
        "\n",
        "2. **Improves Model Interpretability**: A model with fewer features is easier to interpret and understand. It helps in identifying the most important predictors and their relationship with the dependent variable.\n",
        "\n",
        "3. **Enhances Model Accuracy**: Including only relevant features can improve the predictive accuracy of the model. Irrelevant features can introduce noise and reduce the model's performance.\n",
        "\n",
        "4. **Reduces Complexity**: Feature selection simplifies the model, making it computationally efficient and faster to train and deploy. This is especially important for large datasets with many features.\n",
        "\n",
        "5. **Reduces Multicollinearity**: By removing highly correlated predictors, feature selection helps in addressing multicollinearity, which can lead to more stable and reliable coefficient estimates.\n",
        "\n",
        "6. **Facilitates Better Data Understanding**: By focusing on a smaller set of relevant features, it becomes easier to gain insights into the underlying patterns and relationships in the data.\n",
        "\n",
        "### Common Feature Selection Methods\n",
        "\n",
        "1. **Filter Methods**: Use statistical measures to evaluate the relevance of features independently of the model.\n",
        "   - Examples: Correlation coefficients, Chi-square test, ANOVA.\n",
        "\n",
        "2. **Wrapper Methods**: Evaluate subsets of features by training and testing a specific model.\n",
        "   - Examples: Recursive Feature Elimination (RFE), Forward Selection, Backward Elimination.\n",
        "\n",
        "3. **Embedded Methods**: Perform feature selection during the model training process.\n",
        "   - Examples: Lasso Regression, Ridge Regression, Decision Trees.\n",
        "\n",
        "### Summary Table\n",
        "\n",
        "| Benefit                    | Description                                                        |\n",
        "|----------------------------|--------------------------------------------------------------------|\n",
        "| Reduces Overfitting        | Minimizes the risk of overfitting by eliminating irrelevant features|\n",
        "| Improves Interpretability  | Makes the model easier to understand by focusing on key predictors |\n",
        "| Enhances Accuracy          | Increases predictive performance by reducing noise                |\n",
        "| Reduces Complexity         | Simplifies the model, making it computationally efficient         |\n",
        "| Addresses Multicollinearity| Removes highly correlated predictors for stable estimates         |\n",
        "| Better Data Understanding  | Facilitates insights into underlying data patterns                |\n",
        ""
      ],
      "metadata": {
        "id": "984trUfqsoYd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 16. How is Adjusted R-squared calculated\n",
        "\n",
        "Adjusted R-squared (\\(R^2_{\\text{adj}}\\)) is a modified version of R-squared (\\(R^2\\)) that accounts for the number of predictors in the model. It adjusts for the potential inflation of R-squared when unnecessary predictors are added. Here's the formula to calculate Adjusted R-squared:\n",
        "\n",
        "\\[ R^2_{\\text{adj}} = 1 - \\left( \\frac{(1 - R^2)(n - 1)}{n - k - 1} \\right) \\]\n",
        "\n",
        "where:\n",
        "- \\( R^2 \\) is the R-squared value\n",
        "- \\( n \\) is the number of observations\n",
        "- \\( k \\) is the number of independent variables (predictors)\n",
        "\n",
        "### Key Points\n",
        "- **Penalization**: Adjusted R-squared imposes a penalty for adding more variables to the model, helping to prevent overfitting.\n",
        "- **Interpretation**: While R-squared increases or stays the same as more predictors are added, Adjusted R-squared can decrease if the added predictors do not improve the model."
      ],
      "metadata": {
        "id": "d4qzCJXKs00h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 17.  Why is MSE sensitive to outliers\n",
        "\n",
        "Mean Squared Error (MSE) is sensitive to outliers because it squares the errors (the differences between the actual and predicted values). This squaring operation amplifies larger errors disproportionately compared to smaller errors. Here's why this happens:\n",
        "\n",
        "1. **Squaring Effect**: When you square a number, it grows exponentially. For example, an error of 10 becomes 100 when squared, whereas an error of 1 only becomes 1. This means that a few large errors can dominate the MSE, overshadowing smaller errors and making the metric sensitive to outliers.\n",
        "\n",
        "2. **Influence on Model**: Outliers can significantly impact the calculated MSE, leading to an overestimation of the overall error. This can make the model appear worse than it is for the majority of the data points.\n",
        "\n",
        "3. **Misleading Performance**: Because MSE gives more weight to larger errors, it can be misleading in datasets where outliers are present. It might suggest that the model is performing poorly, even if it fits most of the data well."
      ],
      "metadata": {
        "id": "bZdxGdlWtJRl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 18. What is the role of homoscedasticity in linear regression\n",
        "\n",
        "Homoscedasticity, or constant variance of the residuals, is an important assumption in linear regression. Here’s why it matters:\n",
        "\n",
        "### Role of Homoscedasticity\n",
        "\n",
        "1. **Unbiased Estimates**: When the residuals have constant variance, the estimates of the regression coefficients remain unbiased and efficient. This means the model provides the best linear unbiased estimates (BLUE) of the coefficients.\n",
        "\n",
        "2. **Valid Inferences**: Homoscedasticity ensures that the standard errors of the coefficients are accurately estimated. This is crucial for valid hypothesis testing and constructing reliable confidence intervals. If the residuals have constant variance, the usual t-tests and F-tests remain valid.\n",
        "\n",
        "3. **Predictive Accuracy**: When the assumption of homoscedasticity holds, the model's predictions are more reliable. Homoscedastic residuals indicate that the model's predictive performance is consistent across all levels of the independent variables.\n",
        "\n",
        "### Consequences of Violating Homoscedasticity\n",
        "\n",
        "If the assumption of homoscedasticity is violated (i.e., if the residuals exhibit heteroscedasticity):\n",
        "\n",
        "1. **Biased Standard Errors**: The standard errors of the coefficients may be biased, leading to incorrect inferences. This can result in misleading p-values and confidence intervals, affecting the reliability of hypothesis tests.\n",
        "\n",
        "2. **Inefficient Estimates**: The coefficient estimates remain unbiased but become inefficient, meaning they no longer have the smallest possible variance among all linear unbiased estimators.\n",
        "\n",
        "3. **Loss of Predictive Performance**: The model's predictive performance can be inconsistent, as the variance of the errors changes across different levels of the independent variables.\n",
        "\n",
        "### Addressing Heteroscedasticity\n",
        "\n",
        "Several methods can be used to address heteroscedasticity:\n",
        "\n",
        "1. **Transformations**: Apply a transformation to the dependent variable, such as the logarithm or square root, to stabilize the variance of the residuals.\n",
        "\n",
        "2. **Weighted Least Squares (WLS)**: Use WLS instead of Ordinary Least Squares (OLS) to give less weight to observations with larger residuals.\n",
        "\n",
        "3. **Robust Standard Errors**: Use robust standard errors (also known as heteroscedasticity-consistent standard errors) to adjust for heteroscedasticity without transforming the data.\n",
        "\n",
        "Here's a quick summary:\n",
        "\n",
        "| Role                        | Description                                                    |\n",
        "|-----------------------------|----------------------------------------------------------------|\n",
        "| Unbiased Estimates          | Ensures unbiased and efficient coefficient estimates           |\n",
        "| Valid Inferences            | Maintains accurate standard errors for reliable hypothesis tests|\n",
        "| Predictive Accuracy         | Improves consistency of model predictions                      |\n",
        "| Consequences of Violation   | Biased standard errors, inefficient estimates, inconsistent predictions |\n",
        "| Addressing Heteroscedasticity | Transformations, Weighted Least Squares, Robust Standard Errors|\n",
        "\n",
        "Maintaining homoscedasticity helps ensure that the linear regression model performs accurately and reliably"
      ],
      "metadata": {
        "id": "CQq9jRP0tf75"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 19. What is Root Mean Squared Error (RMSE)\n",
        "\n",
        "Root Mean Squared Error (RMSE) is a commonly used metric to evaluate the accuracy of a regression model. It measures the average magnitude of the errors between the predicted and actual values, giving higher weight to larger errors. Here's a breakdown:\n",
        "\n",
        "### Definition\n",
        "RMSE is the square root of the average of the squared differences between the predicted and actual values. Mathematically, it's expressed as:\n",
        "\n",
        "\\[ \\text{RMSE} = \\sqrt{ \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 } \\]\n",
        "\n",
        "where:\n",
        "- \\( n \\) is the number of observations\n",
        "- \\( y_i \\) is the actual value\n",
        "- \\( \\hat{y}_i \\) is the predicted value\n",
        "\n",
        "### Characteristics\n",
        "- **Units of Measurement**: RMSE is expressed in the same units as the dependent variable, making it intuitively understandable.\n",
        "- **Sensitivity to Outliers**: RMSE is sensitive to outliers because the errors are squared, amplifying the impact of larger errors.\n",
        "- **Interpretability**: RMSE gives a clear sense of the average magnitude of prediction errors in the same units as the dependent variable.\n",
        "\n",
        "### Advantages\n",
        "- **Simplicity**: RMSE is easy to understand and calculate.\n",
        "- **Penalty for Larger Errors**: By squaring the errors, RMSE penalizes larger errors more, highlighting their impact on model performance.\n",
        "\n",
        "### Limitations\n",
        "- **Sensitivity to Outliers**: The squaring of errors makes RMSE highly sensitive to outliers, which can sometimes distort the overall error measure.\n",
        "\n",
        "Here's a quick summary in table form:\n",
        "\n",
        "| Metric | Formula | Sensitivity to Outliers | Units | Interpretation |\n",
        "|--------|---------|-------------------------|-------|----------------|\n",
        "| RMSE   | $$ \\sqrt{ \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 } $$ | Sensitive  | Same as dependent variable | Average magnitude of errors |\n",
        ""
      ],
      "metadata": {
        "id": "XpaLjWnGtipj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 20. Why is pickling considered risky\n",
        "\n",
        "Pickling in Python can be risky due to several reasons:\n",
        "\n",
        "1. **Security Risks**: Pickle can execute arbitrary code during deserialization, which makes it vulnerable to malicious attacks. If untrusted data is unpickled, it could execute harmful code, leading to security breaches.\n",
        "\n",
        "2. **Data Corruption**: Pickled files can become corrupted, leading to data loss. If a pickled file is altered or damaged, it might be impossible to unpickle it successfully.\n",
        "\n",
        "3. **Compatibility Issues**: Pickled objects are not always compatible between different versions of Python or different environments. This can cause issues when sharing pickled data across different systems.\n",
        "\n",
        "4. **Limited Language Support**: Pickling is specific to Python, meaning that pickled data is not easily readable or usable by programs written in other languages.\n",
        "\n",
        "5. **Maintenance Challenges**: Over time, as code evolves, the structure of pickled objects might change, making it difficult to unpickle old data with newer versions of the code.\n",
        "\n",
        "### Summary\n",
        "\n",
        "| Risk                | Description                                                               |\n",
        "|---------------------|---------------------------------------------------------------------------|\n",
        "| Security Risks      | Vulnerable to executing arbitrary code, leading to potential attacks      |\n",
        "| Data Corruption     | Risk of data loss if pickled files become corrupted                       |\n",
        "| Compatibility Issues| Incompatibility between different Python versions or environments         |\n",
        "| Limited Language Support | Specific to Python, not easily usable by other languages              |\n",
        "| Maintenance Challenges  | Difficult to unpickle old data with evolving code structures          |\n",
        "\n",
        "Considering these risks, it's important to use pickling cautiously and only with trusted data sources. Alternatives like JSON or specialized serialization libraries can provide safer options for data serialization."
      ],
      "metadata": {
        "id": "TXnMn5HHt5Kx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 21.  What alternatives exist to pickling for saving ML models\n",
        "\n",
        "There are several alternatives to pickling for saving machine learning models that address some of the risks associated with pickling. Here are a few popular ones:\n",
        "\n",
        "1. **Joblib**: Joblib is a library specifically optimized for the serialization of large NumPy arrays and can efficiently save machine learning models. It's faster and more efficient for large datasets compared to pickle.\n",
        "\n",
        "   ```python\n",
        "   import joblib\n",
        "\n",
        "   # Save model\n",
        "   joblib.dump(model, 'model.joblib')\n",
        "\n",
        "   # Load model\n",
        "   model = joblib.load('model.joblib')\n",
        "   ```\n",
        "\n",
        "2. **JSON**: JSON is a lightweight data-interchange format that is easy to read and write. You can serialize the model parameters and architecture to JSON format. This approach is language-agnostic and can be used across different programming languages.\n",
        "\n",
        "   ```python\n",
        "   import json\n",
        "\n",
        "   # Save model parameters to JSON\n",
        "   with open('model.json', 'w') as f:\n",
        "       json.dump(model_params, f)\n",
        "\n",
        "   # Load model parameters from JSON\n",
        "   with open('model.json', 'r') as f:\n",
        "       model_params = json.load(f)\n",
        "   ```\n",
        "\n",
        "3. **HDF5**: Hierarchical Data Format (HDF5) is a file format designed to store and organize large amounts of data. Libraries like h5py or TensorFlow's `tf.keras` API support saving models in HDF5 format.\n",
        "\n",
        "   ```python\n",
        "   import h5py\n",
        "\n",
        "   # Save model\n",
        "   model.save('model.h5')\n",
        "\n",
        "   # Load model\n",
        "   model = tf.keras.models.load_model('model.h5')\n",
        "   ```\n",
        "\n",
        "4. **ONNX (Open Neural Network Exchange)**: ONNX is an open-source format for representing machine learning models. It allows models to be shared across different frameworks, promoting interoperability.\n",
        "\n",
        "   ```python\n",
        "   import onnx\n",
        "   import onnxruntime as ort\n",
        "\n",
        "   # Save model to ONNX format\n",
        "   onnx.save(model, 'model.onnx')\n",
        "\n",
        "   # Load model from ONNX format\n",
        "   ort_session = ort.InferenceSession('model.onnx')\n",
        "   ```\n",
        "\n",
        "5. **Model-Specific Serialization**: Many machine learning libraries have their own built-in methods for saving and loading models. For example, scikit-learn's `joblib`, TensorFlow's `save_model`, and PyTorch's `torch.save`.\n",
        "\n",
        "   ```python\n",
        "   # TensorFlow example\n",
        "   model.save('model_path')\n",
        "\n",
        "   # PyTorch example\n",
        "   torch.save(model.state_dict(), 'model_path.pth')\n",
        "   model.load_state_dict(torch.load('model_path.pth'))\n",
        "   ```\n",
        "\n",
        "Here's a quick summary in a table format:\n",
        "\n",
        "| Method           | Description                                     | Use Case                                                |\n",
        "|------------------|-------------------------------------------------|---------------------------------------------------------|\n",
        "| Joblib           | Efficient serialization for large NumPy arrays  | Saving large datasets and models                        |\n",
        "| JSON             | Lightweight, language-agnostic data format      | Saving model parameters and architecture                |\n",
        "| HDF5             | File format for large data storage              | Saving complex model structures                         |\n",
        "| ONNX             | Open-source format for model interoperability   | Sharing models across different frameworks              |\n",
        "| Model-Specific   | Library-specific serialization methods          | Using built-in methods for TensorFlow, PyTorch, etc.    |\n"
      ],
      "metadata": {
        "id": "ZWssmVYduJRd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 22.  What is heteroscedasticity, and why is it a problem\n",
        "\n",
        "\n",
        "Heteroscedasticity refers to a condition in regression analysis where the variance of the residuals (errors) is not constant across all levels of the independent variables. Instead, the spread of the residuals changes, leading to an unequal distribution.\n",
        "\n",
        "### Why Heteroscedasticity is a Problem\n",
        "\n",
        "1. **Biased Standard Errors**: Heteroscedasticity can result in biased estimates of the standard errors of the regression coefficients. This affects hypothesis testing, leading to unreliable p-values and confidence intervals. Consequently, you might incorrectly judge the significance of predictors.\n",
        "\n",
        "2. **Inefficient Estimates**: Although the regression coefficients themselves remain unbiased, they become inefficient. This means that the estimates do not have the minimum possible variance, leading to less precise predictions.\n",
        "\n",
        "3. **Invalid Inferences**: The usual t-tests and F-tests assume homoscedasticity. When heteroscedasticity is present, the results of these tests can be misleading, potentially leading to incorrect conclusions about the relationships between variables.\n",
        "\n",
        "4. **Distorted Model Performance**: If the variance of the residuals is not constant, the model's predictive performance can vary across different levels of the independent variables. This means the model may perform well in certain ranges but poorly in others, leading to inconsistent predictions.\n",
        "\n",
        "### Detecting Heteroscedasticity\n",
        "\n",
        "- **Residual Plots**: Plotting the residuals against the fitted values or independent variables can visually indicate heteroscedasticity if there is a pattern or funnel shape.\n",
        "- **Breusch-Pagan Test**: A statistical test specifically designed to detect heteroscedasticity.\n",
        "- **White Test**: Another test used to check for heteroscedasticity by examining whether the variance of the residuals is related to the independent variables.\n",
        "\n",
        "### Addressing Heteroscedasticity\n",
        "\n",
        "- **Transformations**: Apply transformations like the logarithm, square root, or Box-Cox transformation to the dependent variable to stabilize the variance.\n",
        "- **Weighted Least Squares (WLS)**: Use WLS instead of Ordinary Least Squares (OLS) to give different weights to observations, reducing the impact of heteroscedasticity.\n",
        "- **Robust Standard Errors**: Use robust standard errors to adjust for heteroscedasticity without transforming the data.\n",
        "\n",
        "Here’s a quick summary:\n",
        "\n",
        "| Issue                      | Description                                                   |\n",
        "|----------------------------|---------------------------------------------------------------|\n",
        "| Biased Standard Errors     | Leads to unreliable p-values and confidence intervals         |\n",
        "| Inefficient Estimates      | Reduces the precision of regression coefficients              |\n",
        "| Invalid Inferences         | Misleading hypothesis tests                                   |\n",
        "| Distorted Model Performance| Inconsistent predictions across different data ranges         |\n",
        "| Detection Methods          | Residual plots, Breusch-Pagan test, White test                |\n",
        "| Solutions                  | Transformations, Weighted Least Squares, Robust Standard Errors|\n",
        ""
      ],
      "metadata": {
        "id": "1EDXS6a5uZlH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 23.  How can interaction terms enhance a regression model's predictive power?\n",
        "Interaction terms can significantly enhance a regression model's predictive power by capturing the combined effects of two or more independent variables on the dependent variable. Here’s how:\n",
        "\n",
        "### Capturing Combined Effects\n",
        "Interaction terms allow the model to account for situations where the effect of one independent variable depends on the level of another independent variable. This is particularly useful in complex scenarios where variables do not operate independently but interact with each other.\n",
        "\n",
        "### Improving Model Fit\n",
        "By including interaction terms, the model can better fit the data, reducing residual variance and improving overall predictive accuracy. This leads to more accurate predictions and a better understanding of the relationships between variables.\n",
        "\n",
        "### Revealing Insights\n",
        "Interaction terms can reveal hidden relationships that are not apparent when considering variables individually. For example, in a marketing context, the combined effect of price and promotion might significantly influence sales, more than either factor alone.\n",
        "\n",
        "### Addressing Non-Linearity\n",
        "In some cases, interaction terms can help address non-linear relationships between variables, providing a more accurate representation of the underlying data patterns.\n",
        "\n",
        "### Example\n",
        "Consider a regression model with two independent variables, \\(X_1\\) and \\(X_2\\). An interaction term \\(X_1 \\times X_2\\) can be included in the model:\n",
        "\n",
        "\\[ Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 (X_1 \\times X_2) + \\epsilon \\]\n",
        "\n",
        "In this model, \\(\\beta_3\\) represents the interaction effect, showing how the relationship between \\(X_1\\) and \\(Y\\) changes with different levels of \\(X_2\\).\n",
        "\n",
        "### Summary Table\n",
        "\n",
        "| Benefit                  | Description                                                   |\n",
        "|--------------------------|---------------------------------------------------------------|\n",
        "| Capturing Combined Effects | Accounts for the combined influence of variables              |\n",
        "| Improving Model Fit      | Reduces residual variance and enhances predictive accuracy    |\n",
        "| Revealing Insights       | Uncovers hidden relationships between variables               |\n",
        "| Addressing Non-Linearity | Helps model non-linear relationships                          |\n"
      ],
      "metadata": {
        "id": "wsFaaLIfuf5t"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z18buMh1pen_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Practical**"
      ],
      "metadata": {
        "id": "mH7MjXMvoA9T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Write a Python script to visualize the distribution of errors (residuals) for a multiple linear regression model using Seaborn's \"diamonds\" dataset."
      ],
      "metadata": {
        "id": "z2mxRicVKpPF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PCROIEYqKf2M"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sns.get_dataset_names()\n",
        "diamond_data = sns.load_dataset('diamonds')"
      ],
      "metadata": {
        "id": "kUUSax7aK9oZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# diamond_data.info()\n",
        "# #  diamond_data[\"cut\"].value_counts()\n",
        "# diamond_data[\"cut\"] = diamond_data[\"cut\"].map({\"Ideal\":1,\"Premium\":2,\"Very Good\":3,\"Good\":4,\"Fair\":5})\n",
        "\n",
        "# # diamond_data[\"color\"].value_counts()\n",
        "# diamond_data[\"color\"] = diamond_data[\"color\"].map({\"G\":1,\"E\":2,\"F\":3,\"H\":4,\"D\":5,\"I\":6,\"J\":7})\n",
        "\n",
        "# # diamond_data[\"clarity\"].value_counts()\n",
        "# diamond_data[\"clarity\"] = diamond_data[\"clarity\"].map({\"SI1\":1,\"VS2\":2,\"SI2\":3,\"VS1\":4,\"VVS2\":5,\"VVS1\":6,\"IF\":7,\"I1\":8})"
      ],
      "metadata": {
        "id": "GYjZwbCULjma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diamond_data.drop(columns=[\"cut\",\"color\",\"clarity\"],inplace=True)"
      ],
      "metadata": {
        "id": "bYein_LwN_di"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.heatmap(diamond_data.corr(),annot=True)"
      ],
      "metadata": {
        "id": "CHkxbTTiNXuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = diamond_data.drop(columns=[\"price\"])\n",
        "y = diamond_data[\"price\"]"
      ],
      "metadata": {
        "id": "nEWQiMhANwv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.24,random_state=1)\n",
        "\n",
        "print(f\"Shape of X_train = {X_train.shape} \\nShape of X_test = {X_test.shape}\")\n",
        "print(f\"Shape of y_train = {y_train.shape} \\nShape of y_test = {y_test.shape}\")"
      ],
      "metadata": {
        "id": "ZMrkthXfQGJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "model = LinearRegression()\n",
        "model"
      ],
      "metadata": {
        "id": "2icGTnfgQM1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "39SI0EkWQhxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import r2_score,mean_absolute_error,mean_squared_error\n",
        "print(f\"Mean Absolute Error = {mean_absolute_error(y_test,y_pred)}\")\n",
        "print(f\"Mean Squared Error = {mean_squared_error(y_test,y_pred)}\")\n",
        "r2_score(y_test,y_pred)"
      ],
      "metadata": {
        "id": "PgpZabf3Qrnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "residuals = y_test - y_pred\n",
        "sns.scatterplot(residuals)\n",
        "plt.axhline(y=0,color=\"red\",linestyle=\"--\")\n",
        "plt.title(\"Residuals Plot\")"
      ],
      "metadata": {
        "id": "oUiSDcJFREPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Write a Python script to calculate and print Mean Squared Error (MSE), Mean Absolute Error (MAE), and Root Mean Squared Error (RMSE) for a linear regression model"
      ],
      "metadata": {
        "id": "0t11UrXLRkhW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error,mean_absolute_error, root_mean_squared_error\n",
        "print(f\"Mean Squared Error = {round(mean_squared_error(y_test,y_pred),3)}\")\n",
        "print(f\"Mean Absolute Error = {round(mean_absolute_error(y_test,y_pred),3)}\")\n",
        "print(f\"Root Mean Squared Error = {round(root_mean_squared_error(y_test,y_pred),3)}\")"
      ],
      "metadata": {
        "id": "2uvrZLl9Rp4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Write a Python script to check if the assumptions of linear regression are met. Use a scatter plot to check linearity, residuals plot for homoscedasticity, and correlation matrix for multicollinearity."
      ],
      "metadata": {
        "id": "CgLAKhdYSZ9M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assumptions 1 : To Check Linearity of each Features in the dataset\n",
        "# plt.title(\"To Check Linearity\")\n",
        "sns.pairplot(diamond_data)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wrF1s5wdSfsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assumption 02 : NO or Little Multicollinearity\n",
        "\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "{X.columns[i] : variance_inflation_factor(X.values,i) for i in range(1, X.shape[1])}\n",
        "\n",
        "sns.clustermap(diamond_data.corr(),annot=True)"
      ],
      "metadata": {
        "id": "X9yrYCcUToxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  residuals plot for homoscedasticity\n",
        "residuals = y_test - y_pred\n",
        "sns.scatterplot(residuals)\n",
        "plt.axhline(y=0,color=\"red\",linestyle=\"--\")\n",
        "plt.title(\"Residuals Plot\")"
      ],
      "metadata": {
        "id": "vyTuC8TkVZxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4 Write a Python script that creates a machine learning pipeline with feature scaling and evaluates the performance of different regression models"
      ],
      "metadata": {
        "id": "otELcGvZV-E3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Simple Dataset\n",
        "# sns.get_dataset_names()\n",
        "data = sns.load_dataset('mpg')\n",
        "data.head()"
      ],
      "metadata": {
        "id": "YIqzSHIMWB38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preprocessing\n",
        "data.drop(\"name\",axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "Oa11j-X_nsDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "PuFXfosWok1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Featrue Engineering\n",
        "sns.distplot(data[\"horsepower\"].value_counts())\n",
        "plt.show()\n",
        "# horsepower contain outlier so we fill the null value with median"
      ],
      "metadata": {
        "id": "WBabcIdgn9KL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[\"horsepower\"].fillna(data[\"horsepower\"].median(),inplace=True)"
      ],
      "metadata": {
        "id": "wOPI7wr3pi1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[\"origin\"].value_counts()"
      ],
      "metadata": {
        "id": "YDUh2JoVplI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[\"origin\"] = data[\"origin\"].map({\"usa\":1,\"europe\":2,\"japan\":3})"
      ],
      "metadata": {
        "id": "xfssyVtzs2pF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make Independet Feature and Target Feature\n",
        "X = data.drop(\"mpg\",axis=1)\n",
        "y = pd.DataFrame(data[\"mpg\"])"
      ],
      "metadata": {
        "id": "8GaMRhbgq8Zp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.isnull().sum()"
      ],
      "metadata": {
        "id": "opDXaCFAsVW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the Data\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25,random_state=1)\n",
        "print(f\"Shape of X_train = {X_train.shape} \\nShape of X_test = {X_test.shape}\")\n",
        "print(f\"Shape of y_train = {y_train.shape} \\nShape of y_test = {y_test.shape}\")"
      ],
      "metadata": {
        "id": "DKqKLiC2rMJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make The Model\n",
        "from sklearn.linear_model import LinearRegression\n",
        "model = LinearRegression()\n",
        "model"
      ],
      "metadata": {
        "id": "0Np8fv7op9eH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "kNlcEOVIq6ZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Evalualtion\n",
        "from sklearn.metrics import r2_score,mean_absolute_error,mean_squared_error\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(f\"Mean Absolute Error = {mean_absolute_error(y_test,y_pred)}\")\n",
        "print(f\"Mean Squared Error = {mean_squared_error(y_test,y_pred)}\")\n",
        "print(f\"Root Mean Squared Error = {np.sqrt(mean_squared_error(y_test,y_pred))}\")\n",
        "print(f\"R2 Score = {r2_score(y_test,y_pred)}\")"
      ],
      "metadata": {
        "id": "QDDomvtosinu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5.  Implement a simple linear regression model on a dataset and print the model's coefficients, intercept, and R-squared score"
      ],
      "metadata": {
        "id": "TOPlUrABs6-D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data.head()\n",
        "# we build a simple linear regression model on a  mpg dataset we take one feature that is horsepower and target feature is mpg\n",
        "simple_data = pd.DataFrame()\n",
        "simple_data[\"horsepower\"] = data[\"horsepower\"]\n",
        "simple_data[\"mpg\"] = data[\"mpg\"]"
      ],
      "metadata": {
        "id": "WyVuzg0FtBht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# horesepower contian some null value\n",
        "simple_data['horsepower'].fillna(simple_data['horsepower'].median(), inplace=True)"
      ],
      "metadata": {
        "id": "UpbO3SSQtshT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = simple_data.drop(\"mpg\",axis=1)\n",
        "y = simple_data[\"mpg\"]"
      ],
      "metadata": {
        "id": "YCrn_B7FtbWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25,random_state=1)\n",
        "print(f\"Shape of X_train = {X_train.shape} \\nShape of X_test = {X_test.shape}\")\n",
        "print(f\"Shape of y_train = {y_train.shape} \\nShape of y_test = {y_test.shape}\")\n"
      ],
      "metadata": {
        "id": "DgCqH__puFkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make the model\n",
        "from sklearn.linear_model import LinearRegression\n",
        "model = LinearRegression()\n",
        "model"
      ],
      "metadata": {
        "id": "mKJIYn-It8a8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "-gDcUrN-uDac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model evaluation\n",
        "from sklearn.metrics import r2_score,mean_absolute_error,mean_squared_error\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(f\"Mean Absolute Error = {mean_absolute_error(y_test,y_pred)}\")\n",
        "print(f\"Mean Squared Error = {mean_squared_error(y_test,y_pred)}\")\n",
        "print(f\"Root Mean Squared Error = {np.sqrt(mean_squared_error(y_test,y_pred))}\")\n",
        "print(f\"R2 Score = {r2_score(y_test,y_pred)}\")"
      ],
      "metadata": {
        "id": "X33rBV74uONW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Write a Python script that analyzes the relationship between total bill and tip in the 'tips' dataset using simple linear regression and visualizes the results"
      ],
      "metadata": {
        "id": "apQH1gdBuZOo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tips_data = sns.load_dataset('tips')\n",
        "tips_data.head(2)"
      ],
      "metadata": {
        "id": "8iQr8bAKuhHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tips_data.info()"
      ],
      "metadata": {
        "id": "qYpLN-OnhHNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(x=\"total_bill\",y=\"tip\",data=tips_data)\n",
        "plt.title(\"Relation Total Bill And Tip\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BNTFSlXYfrBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = tips_data[\"total_bill\"]\n",
        "y = tips_data[\"tip\"]"
      ],
      "metadata": {
        "id": "Nswb142egQXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25,random_state=1)\n",
        "print(f\"Shape of X_train = {X_train.shape} \\nShape of X_test = {X_test.shape}\")\n",
        "print(f\"Shape of y_train = {y_train.shape} \\nShape of y_test = {y_test.shape}\")"
      ],
      "metadata": {
        "id": "tfnUSliYgz8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "model = LinearRegression()\n",
        "model"
      ],
      "metadata": {
        "id": "EFfgbiwqg9nm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train.values.reshape(-1,1)\n",
        "X_test = X_test.values.reshape(-1,1)"
      ],
      "metadata": {
        "id": "G_2U2QNZhafr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "seScLytbhRKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score,mean_absolute_error,mean_squared_error\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(f\"Mean Absolute Error = {mean_absolute_error(y_test,y_pred)}\")\n",
        "print(f\"Mean Squared Error = {mean_squared_error(y_test,y_pred)}\")\n",
        "print(f\"Root Mean Squared Error = {np.sqrt(mean_squared_error(y_test,y_pred))}\")"
      ],
      "metadata": {
        "id": "KOTpgMTbhw_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Write a Python script that fits a linear regression model to a synthetic dataset with one feature. Use the model to predict new values and plot the data points along with the regression line"
      ],
      "metadata": {
        "id": "7rNUuUCiiGuX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate synthetic dataset\n",
        "np.random.seed(42)\n",
        "X = 2 * np.random.rand(1000, 1)\n",
        "y = 4 + 3 * X + np.random.randn(1000, 1)"
      ],
      "metadata": {
        "id": "dM5Id9_MiL9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make the linear regression model\n",
        "from sklearn.linear_model import LinearRegression\n",
        "model = LinearRegression()\n",
        "model"
      ],
      "metadata": {
        "id": "vsxia07jkMae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25,random_state=1)\n",
        "print(f\"Shape of X_train = {X_train.shape} \\nShape of X_test = {X_test.shape}\")\n",
        "print(f\"Shape of y_train = {y_train.shape} \\nShape of y_test = {y_test.shape}\")"
      ],
      "metadata": {
        "id": "qPU4wTEdkUrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "IOdGsUt0kf7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score,mean_absolute_error,mean_squared_error\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(f\"Mean Absolute Error = {mean_absolute_error(y_test,y_pred)}\")\n",
        "print(f\"Mean Squared Error = {mean_squared_error(y_test,y_pred)}\")\n",
        "print(f\"Root Mean Squared Error = {np.sqrt(mean_squared_error(y_test,y_pred))}\")"
      ],
      "metadata": {
        "id": "Q3yHtpH0kkqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(X_test,y_test,color=\"black\",label=\"Data Points\")\n",
        "plt.plot(X_test,y_pred,color=\"brown\",linewidth=2,label=\"Regression Line\")\n",
        "plt.xlabel(\"Feature\")\n",
        "plt.ylabel(\"Target\")\n",
        "plt.title(\"Linear Regression\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "x42aVNtBkyuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Generate synthetic dataset\n",
        "np.random.seed(42)\n",
        "X = 2 * np.random.rand(100, 1)\n",
        "y = 4 + 3 * X + np.random.randn(100, 1)\n",
        "\n",
        "# Fit a linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Predict new values\n",
        "X_new = np.array([[0], [2]])\n",
        "y_predict = model.predict(X_new)\n",
        "\n",
        "# Plot the data points and the regression line\n",
        "plt.scatter(X, y, color='blue', label='Data Points')\n",
        "plt.plot(X_new, y_predict, color='red', linewidth=2, label='Regression Line')\n",
        "plt.xlabel('Feature')\n",
        "plt.ylabel('Target')\n",
        "plt.title('Linear Regression')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "4xbKiC1ijkCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Write a Python script that pickles a trained linear regression model and saves it to a file."
      ],
      "metadata": {
        "id": "DxE3LWKol0Mb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import pickles\n",
        "import pickle\n",
        "\n",
        "# Serialize Process\n",
        "filename = \"model.pkl\"\n",
        "pickle.dump(model, open(filename, \"wb\"))"
      ],
      "metadata": {
        "id": "BJGs6ZInmecv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unserelize Process\n",
        "pickled_model = pickle.load(open(filename, \"rb\"))\n",
        "y_pred = pickled_model.predict(X_test)"
      ],
      "metadata": {
        "id": "y9voubdPmu51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Write a Python script that fits a polynomial regression model (degree 2) to a dataset and plots the regression curve"
      ],
      "metadata": {
        "id": "btWoquT1np7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Generate sample data\n",
        "np.random.seed(0)\n",
        "X = np.random.rand(100, 1) * 10  # Values between 0 and 10\n",
        "y = 2 * X**2 + 3 * X + 5 + np.random.randn(100, 1) * 10  # Quadratic relationship with noise\n",
        "\n",
        "# Create polynomial features (degree 2)\n",
        "poly_features = PolynomialFeatures(degree=2)\n",
        "X_poly = poly_features.fit_transform(X)\n",
        "\n",
        "# Fit the polynomial regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, y)\n",
        "\n",
        "# Generate points for plotting the curve\n",
        "X_plot = np.linspace(0, 10, 100).reshape(-1, 1)\n",
        "X_plot_poly = poly_features.transform(X_plot)\n",
        "y_plot = model.predict(X_plot_poly)\n",
        "\n",
        "# Plot the data points and the regression curve\n",
        "plt.scatter(X, y, color='blue', label='Data Points')\n",
        "plt.plot(X_plot, y_plot, color='red', linewidth=2, label='Regression Curve (Degree 2)')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.title('Polynomial Regression (Degree 2)')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wWfQh0sgnpbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Generate synthetic data for simple linear regression (use random values for X and y) and fit a linear regression model to the data. Print the model's coefficient and intercept."
      ],
      "metadata": {
        "id": "gbmWxpmtnvlN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate synthetic dataset\n",
        "np.random.seed()\n",
        "X = 2 * np.random.rand(1000, 1)\n",
        "y = 4 - 3 * X + np.random.randn(1000, 1)"
      ],
      "metadata": {
        "id": "brGJBIpin5Ka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(X,y)\n",
        "plt.title(\"Synthetic Data\")\n",
        "plt.xlabel(\"Feature\")\n",
        "plt.ylabel(\"Target\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WsL4ncx0ouJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "model = LinearRegression()\n",
        "model"
      ],
      "metadata": {
        "id": "Fkjj20REpE6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X,y)"
      ],
      "metadata": {
        "id": "Zzo2PLCipISa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Coefficient = {model.coef_}\")\n",
        "print(f\"Intercept = {model.intercept_}\")"
      ],
      "metadata": {
        "id": "pNdDuK6_pNO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 11. Write a Python script that fits polynomial regression models of different degrees to a synthetic dataset and compares their performance"
      ],
      "metadata": {
        "id": "YOR5-lhHpVei"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eCXSnsG-peH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 12.  Write a Python script that fits a simple linear regression model with two features and prints the model's coefficients, intercept, and R-squared score"
      ],
      "metadata": {
        "id": "DFWFGfjsrRud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate synthetic data with two features\n",
        "np.random.seed(0)\n",
        "X = pd.DataFrame(np.random.rand(1000, 2), columns=['feature1', 'feature2'])\n",
        "y = 2 + 3 * X['feature1'] + 4 * X['feature2'] + np.random.randn(1000)"
      ],
      "metadata": {
        "id": "F4u4DPL5rZIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25,random_state=1)"
      ],
      "metadata": {
        "id": "FDYv0NmlrxkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "model = LinearRegression()\n",
        "model"
      ],
      "metadata": {
        "id": "7jkHiKbUsBjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "xipgSu4VsTgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(f\"Coefficients {model.coef_}\")\n",
        "print(f\"intercept {model.intercept_}\")\n",
        "print(f\"R2 Score {r2_score(y_test,y_pred)}\")"
      ],
      "metadata": {
        "id": "RHtt718IsXY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 13. Write a Python script that generates synthetic data, fits a linear regression model, and visualizes the regression line along with the data points"
      ],
      "metadata": {
        "id": "8AkFEfEnSeLl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Generate synthetic data\n",
        "np.random.seed(42)  # Set random seed for reproducibility\n",
        "X = 2 * np.random.rand(100, 1)  # Generate 100 random values for X between 0 and 2\n",
        "y = 4 + 3 * X + np.random.randn(100, 1)  # Generate y values with a linear relationship and some noise\n",
        "\n",
        "# Create and fit the linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Predict values using the model\n",
        "y_pred = model.predict(X)\n",
        "\n",
        "# Plot the data points and the regression line\n",
        "plt.scatter(X, y, color='blue', label='Data Points')\n",
        "plt.plot(X, y_pred, color='red', linewidth=2, label='Regression Line')\n",
        "plt.xlabel('Feature (X)')\n",
        "plt.ylabel('Target (y)')\n",
        "plt.title('Linear Regression with Synthetic Data')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "svFzQm2iShjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 14.  Write a Python script that uses the Variance Inflation Factor (VIF) to check for multicollinearity in a dataset with multiple features."
      ],
      "metadata": {
        "id": "tr8SFQnvSkCe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "diamond = sns.load_dataset(\"diamonds\")\n",
        "# diamond.head(2)"
      ],
      "metadata": {
        "id": "swUuRIsKStVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# diamond[\"cut\"].unique()\n",
        "# diamond[\"clarity\"].unique()\n",
        "# diamond[\"color\"].unique()\n",
        "diamond[\"cut\"] = diamond[\"cut\"].map({\"Ideal\":1,\"Premium\":2,\"Very Good\":3,\"Good\":4,\"Fair\":5})\n",
        "diamond[\"clarity\"]= diamond[\"clarity\"].map({\"SI1\":1,\"VS2\":2,\"SI2\":3,\"VS1\":4,\"VVS2\":5,\"VVS1\":6,\"IF\":7,\"I1\":8})\n",
        "diamond[\"color\"] = diamond[\"color\"].map({\"G\":1,\"E\":2,\"F\":3,\"H\":4,\"D\":5,\"I\":6,\"J\":7})"
      ],
      "metadata": {
        "id": "nNxOgY7KTDVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.clustermap(data=diamond.drop(\"price\",axis = 1).corr(),annot=True)\n",
        "plt.title(\"Correlation Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XL9dZJqtTYol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import import variance_inflation_factor\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor"
      ],
      "metadata": {
        "id": "6tpOD6-SUeDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Variance Inflaction Database for storing data relatied to multicollinearity\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"features\"] = diamond.drop(\"price\",axis=1).columns\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(diamond.values,i) for i in range(len(diamond.drop(\"price\",axis=1).columns))]\n",
        "vif_data"
      ],
      "metadata": {
        "id": "7EtXbcZgVeiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(diamond.values)"
      ],
      "metadata": {
        "id": "olnMSzKTVT9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 15. Write a Python script that generates synthetic data for a polynomial relationship (degree 4), fits a polynomial regression model, and plots the regression curve."
      ],
      "metadata": {
        "id": "oZkYXE8eWbaC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Generate synthetic data\n",
        "np.random.seed(0)\n",
        "X = np.sort(5 * np.random.rand(80, 1), axis=0)\n",
        "y = (X**4 + X**3 + X**2 + X + 1 + np.random.randn(80, 1)*20).ravel() # Introduce some noise\n",
        "\n",
        "# Create polynomial features (degree 4)\n",
        "poly = PolynomialFeatures(degree=4)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "# Fit polynomial regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, y)\n",
        "\n",
        "# Generate points for plotting the curve\n",
        "X_plot = np.linspace(0, 5, 100).reshape(-1, 1)\n",
        "X_plot_poly = poly.transform(X_plot)\n",
        "y_plot = model.predict(X_plot_poly)\n",
        "\n",
        "# Plot the data points and the regression curve\n",
        "plt.scatter(X, y, color='blue', label='Data Points')\n",
        "plt.plot(X_plot, y_plot, color='red', linewidth=2, label='Regression Curve (Degree 4)')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.title('Polynomial Regression (Degree 4)')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7T2iKC6gWg87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 16. Write a Python script that creates a machine learning pipeline with data standardization and a multiple linear regression model, and prints the R-squared score."
      ],
      "metadata": {
        "id": "jrMoEpYcWheh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Dataset from Sklearn Libary\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "housing = fetch_california_housing()"
      ],
      "metadata": {
        "id": "GrNzzfarWwdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make the Dataset\n",
        "housing_data = pd.DataFrame(housing.data,columns=housing.feature_names)\n",
        "housing_data[\"price\"] = housing.target\n",
        "housing_data.head()"
      ],
      "metadata": {
        "id": "ljYJbRypXotW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To check Linarity\n",
        "sns.pairplot(housing_data)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jUlSFEjMYFEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.stats.outliers_influence import variance_inflation_factor"
      ],
      "metadata": {
        "id": "BcUWnQwkYyQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "housing_data.drop(\"price\",axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "JWLpmQ1hbOhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vif_housing_data = pd.DataFrame()\n",
        "vif_housing_data[\"features\"] = housing_data.columns\n",
        "vif_housing_data[\"vif\"] = [variance_inflation_factor(housing_data.values,i) for i in range(len(housing_data.columns))]\n",
        "vif_housing_data"
      ],
      "metadata": {
        "id": "uvjEP0zlZCYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "housing_data.drop(\"Longitude\",axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "MMHLHGINaIX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vif_housing_data[\"features\"] = housing_data.columns\n",
        "vif_housing_data[\"vif\"] = [variance_inflation_factor(housing_data.\n",
        "values,i) for i in range(len(housing_data.columns))]\n",
        "vif_housing_data"
      ],
      "metadata": {
        "id": "ruSBt7U5a3IB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "housing_data.drop(\"AveRooms\",axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "ECjP6TUNbXrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vif_housing_data = pd.DataFrame()\n",
        "vif_housing_data[\"features\"] = housing_data.columns\n",
        "vif_housing_data[\"vif\"] = [variance_inflation_factor(housing_data.\n",
        "values,i) for i in range(len(housing_data.columns))]\n",
        "vif_housing_data"
      ],
      "metadata": {
        "id": "CA3S-ya3bcvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "housing_data.drop(\"Latitude\",axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "mcfIWCy_bn5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vif_housing_data = pd.DataFrame()\n",
        "vif_housing_data[\"features\"] = housing_data.columns\n",
        "vif_housing_data[\"vif\"] = [variance_inflation_factor(housing_data.\n",
        "values,i) for i in range(len(housing_data.columns))]\n",
        "vif_housing_data"
      ],
      "metadata": {
        "id": "6GrYbR1Jb1L6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = housing_data\n",
        "y = pd.DataFrame(housing.target)\n",
        "print(f\"X Shape : {X.shape}\")\n",
        "print(f\"y Shape : {y.shape}\")"
      ],
      "metadata": {
        "id": "sMu2BE_6b3Ce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()"
      ],
      "metadata": {
        "id": "shbwZlVRcOHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25,random_state=1)\n",
        "print(f\"X_train Shape : {X_train.shape}\")\n",
        "print(f\"X_test Shape : {X_test.shape}\")\n",
        "print(f\"y_train Shape : {y_train.shape}\")\n",
        "print(f\"y_test Shape : {y_test.shape}\")"
      ],
      "metadata": {
        "id": "J7HrVPK9cYVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scale the features\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "print(f\"X_train Shape : {X_train.shape}\")\n",
        "print(f\"X_test Shape : {X_test.shape}\")"
      ],
      "metadata": {
        "id": "pl8Kg3NXcrIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make the model\n",
        "from sklearn.linear_model import LinearRegression\n",
        "model = LinearRegression()\n",
        "model"
      ],
      "metadata": {
        "id": "eQLFo-6Kc465"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train The Model\n",
        "model.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "T3ZNyu-Fc-_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test)\n",
        "print(f\"Coefficient = {model.coef_}\")\n",
        "print(f\"Intercept = {model.intercept_}\")\n",
        "\n",
        "from sklearn.metrics import r2_score,mean_absolute_error,mean_squared_error\n",
        "print(f\"Mean Absolute Error = {mean_absolute_error(y_test,y_pred)}\")\n",
        "print(f\"Mean Squared Error = {mean_squared_error(y_test,y_pred)}\")\n",
        "print(f\"Root Mean Squared Error = {np.sqrt(mean_squared_error(y_test,y_pred))}\")\n",
        "print(f\"R2 Score = {r2_score(y_test,y_pred)}\")"
      ],
      "metadata": {
        "id": "-3VigZxQdCn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 17.  Write a Python script that performs polynomial regression (degree 3) on a synthetic dataset and plots the regression curve."
      ],
      "metadata": {
        "id": "9Q-7a6aZdZ7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Generate synthetic data with a non-linear relationship\n",
        "np.random.seed(0)\n",
        "X = np.random.rand(100, 1) * 10  # Feature values between 0 and 10\n",
        "y = 2 * X**3 + 3 * X**2 + 5 + np.random.randn(100, 1) * 10  # Target with cubic relationship and noise\n",
        "X = X.reshape(-1,1)\n",
        "y = y.reshape(-1,1)\n",
        "# Create polynomial features (degree 3)\n",
        "poly_features = PolynomialFeatures(degree=3)\n",
        "X_poly = poly_features.fit_transform(X)\n",
        "\n",
        "# Fit linear regression model to polynomial features\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, y)\n",
        "\n",
        "# Generate points for plotting the curve\n",
        "X_curve = np.linspace(0, 10, 100).reshape(-1, 1)  # Evenly spaced points between 0 and 10\n",
        "X_curve_poly = poly_features.transform(X_curve)  # Transform to polynomial features\n",
        "y_curve = model.predict(X_curve_poly)  # Predict target values for the curve\n",
        "\n",
        "# Plot the data points and the regression curve\n",
        "plt.scatter(X, y, color='blue', label='Data Points')\n",
        "plt.plot(X_curve, y_curve, color='red', linewidth=2, label='Regression Curve (Degree 3)')\n",
        "plt.xlabel('Feature')\n",
        "plt.ylabel('Target')\n",
        "plt.title('Polynomial Regression (Degree 3)')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZbzEK9lPdj2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 18. Write a Python script that performs multiple linear regression on a synthetic dataset with 5 features. Print the R-squared score and model coefficients"
      ],
      "metadata": {
        "id": "zA7gUxcKdjbj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "num_samples = 1000\n",
        "X = np.random.rand(num_samples, 5)  # 5 features\n",
        "coefficients = np.array([2, -3.5, 1, 4.2, -1.5])\n",
        "y = X @ coefficients + np.random.randn(num_samples) * 0.5  # Add some noise"
      ],
      "metadata": {
        "id": "bv_qX1ycOisg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names = [f'feature_{i}' for i in range(1, 6)]\n",
        "data = pd.DataFrame(X,columns=feature_names)\n",
        "data[\"target\"] = y\n",
        "data.head(2)"
      ],
      "metadata": {
        "id": "nUTERbrFPs9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X = data.drop(\"target\",axis=1)\n",
        "y = data[\"target\"]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=1)"
      ],
      "metadata": {
        "id": "Jr79QRzIQU2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "model_18 = LinearRegression()\n",
        "model_18"
      ],
      "metadata": {
        "id": "GA8jOswlQju4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_18.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "SDfYgLwOQmgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model_18.predict(X_test)\n",
        "print(f\"Coefficient = {model.coef_}\")\n",
        "print(f\"Intercept = {model.intercept_}\")\n",
        "\n",
        "from sklearn.metrics import r2_score,mean_absolute_error,mean_squared_error\n",
        "print(f\"Mean Absolute Error = {mean_absolute_error(y_test,y_pred)}\")\n",
        "print(f\"Mean Squared Error = {mean_squared_error(y_test,y_pred)}\")\n",
        "print(f\"Root Mean Squared Error = {np.sqrt(mean_squared_error(y_test,y_pred))}\")\n",
        "print(f\"R2 Score = {r2_score(y_test,y_pred)}\")"
      ],
      "metadata": {
        "id": "cikv45KeQqIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 19. Write a Python script that generates synthetic data for linear regression, fits a model, and visualizes the data points along with the regression line."
      ],
      "metadata": {
        "id": "GJl_cur1RCav"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "num_samples = 1000\n",
        "X = np.random.rand(num_samples)  # 5 features\n",
        "y = X * 2.1 + np.random.randn(num_samples)"
      ],
      "metadata": {
        "id": "JVGWNC4ZRjRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = pd.DataFrame(X,columns=[\"feature_1\"]).head(2)\n",
        "X[\"target\"] = y\n",
        "X.head(2)"
      ],
      "metadata": {
        "id": "P_nZDv-KTM3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(X,y)\n",
        "plt.title(\"Synthetic Data\")\n",
        "plt.xlabel(\"Feature\")\n",
        "plt.ylabel(\"Target\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "F6O6XaOdR7mf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X = data.drop(\"target\",axis=1)\n",
        "y = data[\"target\"]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=1)"
      ],
      "metadata": {
        "id": "YXfjw9ndRLMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "model_19 = LinearRegression()\n",
        "model_19.fit(X_train,y_train)\n",
        "model_19"
      ],
      "metadata": {
        "id": "-zd1wC10SyAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model_19.predict(X_test)\n",
        "print(f\"Coefficient = {model.coef_}\")\n",
        "print(f\"Intercept = {model.intercept_}\")\n",
        "\n",
        "from sklearn.metrics import r2_score,mean_absolute_error,mean_squared_error\n",
        "print(f\"Mean Absolute Error = {mean_absolute_error(y_test,y_pred)}\")\n",
        "print(f\"Mean Squared Error = {mean_squared_error(y_test,y_pred)}\")\n",
        "print(f\"Root Mean Squared Error = {np.sqrt(mean_squared_error(y_test,y_pred))}\")\n",
        "print(f\"R2 Score = {r2_score(y_test,y_pred)}\")"
      ],
      "metadata": {
        "id": "KUOKTwZtS44B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(X_test,y_test,color=\"black\",label=\"Data Points\")"
      ],
      "metadata": {
        "id": "tAfuBwmAS_Q_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 20. Create a synthetic dataset with 3 features and perform multiple linear regression. Print the model's Rsquared score and coefficients"
      ],
      "metadata": {
        "id": "X0Wh7dshmLfh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Generate synthetic data with 3 features\n",
        "np.random.seed(0)\n",
        "num_samples = 1000\n",
        "X = pd.DataFrame(np.random.rand(num_samples, 3), columns=['feature1', 'feature2', 'feature3'])\n",
        "y = 2 + 3 * X['feature1'] + 4 * X['feature2'] + 1.5 * X['feature3'] + np.random.randn(num_samples)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train a Linear Regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model using R-squared score\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Print the results\n",
        "print(\"R-squared score:\", r2)\n",
        "print(\"Coefficients:\", model.coef_)\n",
        "print(\"Intercept:\", model.intercept_)"
      ],
      "metadata": {
        "id": "piwWakRqmfe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 21. Write a Python script that demonstrates how to serialize and deserialize machine learning models using joblib instead of pickling"
      ],
      "metadata": {
        "id": "FzwjzH93lOS0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train a Linear Regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Serialize the model using joblib\n",
        "joblib.dump(model, 'linear_regression_model.joblib')\n",
        "\n",
        "# Deserialize the model using joblib\n",
        "loaded_model = joblib.load('linear_regression_model.joblib')\n",
        "\n",
        "# Make predictions using the loaded model\n",
        "predictions = loaded_model.predict(X_test)\n",
        "\n",
        "# To see the output, run the code."
      ],
      "metadata": {
        "id": "TDegkDtSlY7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 22. Write a Python script to perform linear regression with categorical features using one-hot encoding. Use **the** Seaborn 'tips' dataset"
      ],
      "metadata": {
        "id": "BN8SeZ_6lZT6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Load the 'tips' dataset\n",
        "tips = sns.load_dataset('tips')\n",
        "\n",
        "# Create a OneHotEncoder object\n",
        "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore') # sparse=False for compatibility\n",
        "\n",
        "# Select categorical features for one-hot encoding\n",
        "categorical_features = ['sex', 'smoker', 'day', 'time']\n",
        "\n",
        "# Transform categorical features using one-hot encoding\n",
        "encoded_features = encoder.fit_transform(tips[categorical_features])\n",
        "\n",
        "# Create a DataFrame with encoded features\n",
        "encoded_df = pd.DataFrame(encoded_features, columns=encoder.get_feature_names_out(categorical_features))\n",
        "\n",
        "# Concatenate encoded features with numerical features\n",
        "numerical_features = ['total_bill', 'size']\n",
        "X = pd.concat([tips[numerical_features], encoded_df], axis=1)\n",
        "y = tips['tip']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train a Linear Regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model using R-squared score\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"R-squared score: {r2}\")"
      ],
      "metadata": {
        "id": "yi3tNXz1lua2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 23. Compare Ridge Regression with Linear Regression on a synthetic dataset and print the coefficients and R squared score"
      ],
      "metadata": {
        "id": "AJy05PsVlyr6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression, Ridge\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Generate synthetic data\n",
        "np.random.seed(0)\n",
        "X = pd.DataFrame(np.random.rand(100, 5), columns=['feature1', 'feature2', 'feature3', 'feature4', 'feature5'])\n",
        "y = 2 + 3 * X['feature1'] + 4 * X['feature2'] + np.random.randn(100)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Linear Regression\n",
        "linear_model = LinearRegression()\n",
        "linear_model.fit(X_train, y_train)\n",
        "linear_pred = linear_model.predict(X_test)\n",
        "linear_r2 = r2_score(y_test, linear_pred)\n",
        "\n",
        "# Ridge Regression\n",
        "ridge_model = Ridge(alpha=1.0)  # Alpha is the regularization parameter\n",
        "ridge_model.fit(X_train, y_train)\n",
        "ridge_pred = ridge_model.predict(X_test)\n",
        "ridge_r2 = r2_score(y_test, ridge_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Linear Regression:\")\n",
        "print(\"Coefficients:\", linear_model.coef_)\n",
        "print(\"R-squared:\", linear_r2)\n",
        "print(\"\\nRidge Regression:\")\n",
        "print(\"Coefficients:\", ridge_model.coef_)\n",
        "print(\"R-squared:\", ridge_r2)"
      ],
      "metadata": {
        "id": "bbq5-wryl4rO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 24. Write a Python script that uses cross-validation to evaluate a Linear Regression model on a synthetic dataset."
      ],
      "metadata": {
        "id": "gLruyF1Hmj_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import make_scorer, r2_score\n",
        "\n",
        "# Generate synthetic data\n",
        "np.random.seed(0)\n",
        "X = pd.DataFrame(np.random.rand(100, 5), columns=['feature1', 'feature2', 'feature3', 'feature4', 'feature5'])\n",
        "y = 2 + 3 * X['feature1'] + 4 * X['feature2'] + np.random.randn(100)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Linear Regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Define the scoring metric (R-squared)\n",
        "scoring = make_scorer(r2_score)\n",
        "\n",
        "# Perform cross-validation\n",
        "scores = cross_val_score(model, X_train, y_train, cv=5, scoring=scoring)  # 5-fold cross-validation\n",
        "\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", scores)\n",
        "print(\"Average R-squared:\", scores.mean())"
      ],
      "metadata": {
        "id": "kgoUJJ6CmrMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 25.  Write a Python script that compares polynomial regression models of different degrees and prints the R squared score for each"
      ],
      "metadata": {
        "id": "Ah0Cw4hNmswx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Generate synthetic data with a non-linear relationship\n",
        "np.random.seed(0)\n",
        "X = np.random.rand(100, 1) * 10  # Feature values between 0 and 10\n",
        "y = 2 * X**2 + 3 * X + 5 + np.random.randn(100, 1) * 10  # Target with quadratic relationship and noise\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define degrees of polynomial to test\n",
        "degrees = [1, 2, 3, 4, 5]\n",
        "\n",
        "# Iterate over degrees and fit polynomial regression models\n",
        "for degree in degrees:\n",
        "    # Create polynomial features\n",
        "    poly_features = PolynomialFeatures(degree=degree)\n",
        "    X_train_poly = poly_features.fit_transform(X_train)\n",
        "    X_test_poly = poly_features.transform(X_test)\n",
        "\n",
        "    # Fit linear regression model to polynomial features\n",
        "    model = LinearRegression()\n",
        "    model.fit(X_train_poly, y_train)\n",
        "\n",
        "    # Make predictions on the testing set\n",
        "    y_pred = model.predict(X_test_poly)\n",
        "\n",
        "    # Calculate and print R-squared score\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    print(f\"Degree {degree}: R-squared = {r2}\")"
      ],
      "metadata": {
        "id": "bdMJKIjDmybx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}